# kuIotBigdataclass
C/C++ database, arduino, rasberrypi, Iot class.... AVR
데이터베이스 수업 리드미 작성 시작 
-----------------------------------------------------------------------------------------------------------------------------------
## 2024-02-22
-----------------------------------------------------------------------------------------------------------------------------------
- Instruction.
- google slide share link.
- github sign up.
- github create repository.
- ubuntu hangul setting.
- VsCode install.
- git install.
```shell

- sql workbench install - windows
- MySQL DBMS install - windows
- sql workbench install - ubuntu
- MySQL DBMS install - ubuntu
- vscode mysql extension install
- Chapter01 데이터베이스 개론
- Chapter03 SQL 기초
    - 데이터베이스 만들기 create database;
    - 데이터베이스에 테이블 만들기3개
        - bookid
        - bookname
        - publisher
        - price
    - 데이터베이스에 데이터 넣기 insert into ... values (...)
    - 유저 만들어서 권한 설정하기;
        - grant all privileges on *.* to 'root'@localhost' identified by '1234';
    - windows cli에서 testDB 생성하기
sudo apt-get install git
git clone https://github.com/mattsum/kulotBigdataclass.git
```
-...
-----------------------------------------------------------------------------------------------------------------------------------
## 2024-02-23
-----------------------------------------------------------------------------------------------------------------------------------
    - my ubuntu 문제 해결
    - 유저를 새로만들어서 권한을 주어 봤지만 해결되지 않음.
    - 권한설정 및 user create 시
        'ALTER USER 'root@localhost; Identified with mysql_native_password By '0000';, 명령어
    - 추가한파일
        - create_database.sql
        - create_data.sql
        - create_user.sql
        - insert_data.sql
        - select1.sql
        - 파일을 한번에 저장했었는데, 제대로 저장 된건지 잘 모르겠음.
        - 어제 한 데이터에서 책 133-159 마지막 'DESC' 까지 실습을 함.
    - 3자 내용 추가
        - 셀렉트 쓰는 방법
    - 2장 데이터베이스 모델 진행
        - 릴레이션의 개념 스키마&인스턴스
            -릴래이션용어 CREATE TABLE, INSERT, DELETE, UPDATE, SELECT
        - 무결성 제약조건 (키)
            - 슈퍼키 후보키 기본키 대리키 대체키 외래키
        - 관계대수 - 연산자 관계대수식 
        - 셀렉션과 프로젝션
        - 집합연산
            - 합집합, 교집합, 차집합, 카티션프로덕트
        - 조인 (세타조인과 동등조인)
            - 자연조인 외부조인과 세미조인
        - 디비전
    - 3장 SQL 기초 _ group by 165p까지 진행
    - 윈도우 DMBS를 우분투에서 연결
        - 윈도우 DBMS에 외부 IP; 접근 가능한 user 생성
        - 윈도우 방화벽에 3306 포트열기
        - 윈도우 DBMS에 임의의 데이터를 넣고 우분투에서 확인
    - 우분투 DBMS를 윈도우에 연결
        - 우분투 DBMS에 외부IP 접근가능한 user 생성
        - 우분투 방화벽에 전체포트열기
        - 우분투 DBMS에 임의이 데이터 넣고 윈도우에서 확인
            - vertualbox 프로그램에서는 windows로 가는 Ping이 진행되지않음 (학생들이 직접확인 하지 못함)
            - VMware 프로그램에서는 windows로 가는 ping 이 잘됨 (프로젝트 화면으로만 보여만 줌)
------------------------------------------------------------------------------------------------------------------------------
## 2024-02-29
------------------------------------------------------------------------------------------------------------------------------
        - 빌드 과정 설명
            - make cmake for linux
            - 전처리 어셈블리 바이너리 링크
        - VS Code Make 파일 하는 방법. 
            - 기본 문법
            - 여러개 명령어 등록
            - 연속 실행
            - 생략가능 명령어
            - 변수 사용
        - 3장 SQL 기초 전체 진행 [너무 어렵다]
            - 데이터 조작어 검색
                - SELECT 문
                - 집계함수와 GROUP BY
                - 두 개 이상의 테이블을 이용한 sql질의
            - 데이터 정의어
                - CREATE ALTER DROP
            - 데이터 조작어
                - INSERT UPDATE DELETE
        
        
        - 4장 SQL 고급 진행
            -SQL 내장함수
-------------------------------------------------------------------------------------------------------------------------
## 2024-03-07
-------------------------------------------------------------------------------------------------------------------------
            -null if null
            - 외래키 확인 및 삭제 방법
            
                - 부족질의
                    - select 부속질의 (스칼라 부속질의)
                    - from 부속질의( 인라인 뷰 )
                    - where 부속질의
                        - 단일 - 비교 ( =, >, <, >=, <=, !=, <>, is null, is not null) [비교]
                        - 다수의 열, 단일 행 ( all, some, any) [한정]
                        - 다수의 행, 다수의 열 (in, not in, exists, not exists) [집합,존재]
                - 뷰 view
                    - 뷰생성 - create view 뷰 이름 as select ...
                    - 뷰삭제 - drop view 뷰 이름
                    - 뷰수정 - create or replace view 뷰이름 as select...
                - 인덱스
                    - 인덱스 생성 - create index 인덱스이름 on 테이블이름(열이름)
                    - 인덱스 삭제 - drop index 인덱스이름 on 테이블이름
                    - 인덱스 재구성 - analyze table 테이블이름
                -숫자함수
                -문자열함수
                -날짜함수
                -format 형식 %Y%m%d 등등 새롭게 표현하는식
        - 갑작스러운? [Quick Test 2024.03.08. 있을예정]
            - 시험범위 1~4장 + 7장 정보화
            - 정보처리기사 올려주신 것 40번~61번 까지 한번  풀어 볼 것.
                - 비전공자들은 정보처리기사 자격증을 취득하는게 기본 - 추천하심
                - 정보처리기사(IT분야에서 전문지식을 검증하는 중요한 자격증)
                    - 2024 정보처리기사 2회 필기시험 원서접수 	2024.04.16.(화) ~ 2024.04.19.(금)
                    - 2024 정보처리기사 3회 필기시험 원서접수 	2024.06.18.(화) ~ 2024.06.21.(금) 
        - 5장 데이터베이스 프로그래밍 시작
            - 1. 데이터베이스 프로그래밍의 개념
                - SQL전용언어를 사용하는 방법
                - 일반프로그래밍언어에 SQL을 삽입하여 사용하는 방법
                - 웹 프로그래밍 언어에 SQL을 삽입하여 사용하는 방법
                - 4GL 4th Generation Language
            - 2. 저장 프로그램의 개념
                - 저장프로그램
                    - 저장루틴은프로시져procedure와 함수function으로 나뉜다.
                        - create procedure (begin - end)로 구성된다.
                            - 삽입작업을하는 프로시져, 제어문을 사용하는 프로시져.
                            - 결과를반환하는 프로시져, 커서를 사용하는 프로시져.
                        - 트리거 (Trigger) - 데이터의변경문이 실행될떄 자동으로 같이 실행되는 프로시져를 말함.
                        - 사용자 정의 함수 (function) - 변수를 쓸때 규칙대로 쓰면 디버깅을 아낄수 있다.
                - 저장프로그램의 문법요약
                    - 데이터정의어 (create table, create procedure, create function, create trigger, drop)
                    - 데이터조작어 (select, insert, delete, update)
                    - 데이터타입 (integer, carchar(n) date)
                    - 변수 (declare문으로 선언, 치환(set,=사용))
                    - 연산자 (산술연산자[+,-,*,/] 비교연산자[=,<,>,>=,<=,<>] 문자열연산자[||] 논리연산자[Not,and,or])
                    - 주석 (--, /* */)
                    - 내장함수 (숫자함수[abs,ceil,floor,power1] 집계함수[avg,count,max,min,sum] 날짜함수[sysdate,date,datname] 문자함수[char,left,lower,substr])
                    - 제어문 (begin-end, if-then-else, while,loop)
                    - 데이터 제어어 (grant, revoke)
            - 마지막 파이썬설치 및 파이썬 make [사실 정신이 없어서 뭐한건지 아직 정확히 모르겠다]
----------------------------------------------------------------------------------------------------------------------------------------------------------------
## 2024-03-08
----------------------------------------------------------------------------------------------------------------------------------------------------------------
            - make 파일 booklist.c make파일만들기 
                - booklist make 
                - C Make 파일 작성후 올바르게 실행되는지 확인
                    - C Make 파일 작성법
                - C Make 파일 작성 후 실행과정에서 sudo apt install cmake.p
                    - C Make 파일 작성후 실행
                - webpage 만들기
                    - webpage pg.309 교과서
        - 6장 데이터 모델링 시작
            - 데이터모델링의 개념
            - ER(ENTITY RELATIONSHIP) - Diagram 표현
                    - 강한개체타입은 직사각형,   약한개체타입은 이중사각형
                    - 단순속성과 복합속성 [묶어서 표현가능]
                    - ISA 관계 
                    - IE 표기법법
            - ER모델을 관계데이터 모델로 사상(mapping)하기
            - Workbench 모델링 실습
                - 도메인 정의하기 실습, 마당서점ERD 만들기 (완성 못 함)
            - Madang 모델링 연습 [시간이 없어서 아주 빠르게 지나감]
        - 7장 정규화 시작
            - 이상현상 Anomoly 테이블설계를 못할시에 발생
                    - 삽입 삭제 수정
            - 함수종속성(FD)
                    - 속성사이에는 의존성이 존재함
                        - 함수종속성 다이어그램, 함수종속성 규칙, 함수 종속성기본키, 이상현상과 결정자, 함수 종속성 예제
            - 정규화 (NORMALIZATION)
                - 제1정규형, 제2정규형, 제3규형, BCNF형 [시간이없어서 코딩없이 교과서위주로만 배움]
                    - 무손실분해
                        - JOIN써서 다시 돌아 올수 있으면 무손실분해, 다시 돌아오지못하면 손실분해
                    - 정규화정리 + 정규화연습[시간이없어서 해보질 못 함]
        - 8장 트랜잭션, 동시성제어, 회복 시작
            - 트랜잭션 - commit문이 트랜잭션의 종료를 알리는 SQL문
            - 트랜잭션의 성질
                -원자성(ATOMICITY), 일관성(CONSISTENCY), 고립성(ISOLATION), 지속성(DURABILITY)
            - 동시성제어
                - 두개를 동시에 쓰게되면 반드시 충돌사고가 일어남.
            - 갱신손실문제
            - 락(LOCK) 
                    - 공유락(LS - SHARED LOCK) 베타락(EXCLUSIVE LOCK)
                    - 2단계 락킹(2 phase locking)
                    - 데드락(중요) - 교착상태 (deadlock)
                        -두 개 이상의 트랜잭션이 각각 자신의 데이터에 대하여 락을 획득하고 
                            상대방 데이터에 대하여 락을 요청하면 무한대기 상태에 빠질수 있음.
            - 트랜잭션 고립수준
-------------------------------------------------------------------------------------------
## 2024-05-03 [금]
-------------------------------------------------------------------------------------------
             디버깅없이 실행을 터미널에서 하는 법

 : mkdir build
 : cmake ..
 : make
 : ./(파일이름)

-------------------------------------------------------------------------------------------

- OpenCV 설치
    sudo apt install libopencv-dev python3-opencv
    cmake 설정 _ mkdir build + cmake ..+ make + ./실행파일
    - glob  file 적용하기
    - 영상처리 영상인식 기본개념
    -기본클래스
        -Point class
        -Size class
        -Rect class
        -RotatedRect class
        -String class
        -Mat class일
            얕은복사 - 대입, 연산
            깊은복사 - clone, copyTo
        -Vec class
        -Scalar class
        -InputArray class
        -OutputArray class

    -영상입출력
        -imread
        -imshow
        -imWrite

    -동영상입출력
        -VideoCapture
        -VideoWriter

    -직선그리기
        -line함수
        -움직이는함수 그리기
-------------------------------------------------------------------------------------------
## 2024-05-07 [화]
-------------------------------------------------------------------------------------------
    -drawing
        -circle
        -rectangle
        -ellipse
    -text
        -putText
        -freeStyle 한글폰트기능
    -mouse
        -mouse callback 함수 사용
    -keyboard 
        -키보드 입력받기
    -tickmeter
        -시간측정
        -frame 일정하게 만들기 최적화
    -trackbar
        -trackbar Callback 함수 사용

    -4장 마지막실습
        -마우스 따라다니는 사각형, BGR정보읽기 + 파일저장
            실습 문제
                1.레나 이미지를 윈도우에 표시합니다.
                2.마우스가 클릭된 위치를 기준으로 사각형을 그립니다.
                3.마우스 클릭시 클릭된 위치의 BGR 값을 출력합니다.
                4.BGR 값을 저장하고 마지막으로 최적화를 위해 시간을 측정합니다.
                5.ch04까지 배운거 다 종합적으로 해 실습하기
    -5장 시작
        -그레이스케일
            - Brightness
            - Saturated
                - brightness.cpp 실습
                - saturated.cpp 실습
        -히스토그램 분석
            - pg 200. 까지 진도 나감

-------------------------------------------------------------------------------------------
## 2024-05-08 [수]
-------------------------------------------------------------------------------------------
    -5장 이어서 하기

        - 수업시작전 간단한 4장5장 리뷰
        - brightTrackBar
        영상밝기조절 - 트랙바
        영상대비조절 - 단순곱하기, 중간영역확장
        - 히스토그램, 히스토그램스트래치 바, 히스토그램 이쿠얼리제이션
        - 히스토그램 평활화

    - 6장 시작 [ 영상의 산술 및 논리 연산 ]
        - bitwise_and 함수
        - bitwise_or 함수
        - bitwise_xor 함수
        - bitwise_not 함수
    
    - 7장 시작 [ 필터링 ]
    convolution
        - 필터링연산방법
            가장자리 픽셀 처리 방법
            - BORDER_CONSTANT
            - BORDER_REPLICATE
            - BORDER_REFLECT
            - BORDER_REFLECT_101
            - BORDER_REFLECT101
            - BORDER_DEFAULT
        - EMBOSSING FILTERING [2D]
        - BLURRING [영상부드럽게하기]
        - 가우시안필터 - 독일의수학자 이름에서 딴 것
        - 샤프닝 [ 영상날카롭게하기 ]
            -unsharp_mask
        - 잡음 제거 필터링
            - 영상과잡음모델 [noise] - 잡는 미디언 필터
            - 양방향 필터   [bilateralFilter]
            - 미디언 필터   [medianFilter]

    - 8장 시작 [ 영상의 기하하적 변환 ]
        - 어파인 변환(Affine Transform) 3x2
            - getAffine Transform(), warpAffine() 함수.
            - 이동 변환
            - 전단 변환
            - 크기 변환
            - 회전 변환
            - 대칭 변환
        - 투시 변환 (perspective tramsform) 3x3, 마지막 숫자는 1이기에  dot이 8개가 필요하다.
            - getPerspectiveTransform(), warpPerspectiveTransform()
    - 9장 시작 [ 엣지 검출과 응용 ] 
        - 미분과 그래디언트
            - 엣지 - 픽셀부분 중 가장 많이 변하는 끝부분
            - 엣지 - 라인 - 코너 - 꼭지점을바탕으로 - 키포인트 - 객체인식 기반 
            - 엣지, 미분, 영상의 미분구하기[전진, 후진, 중앙]
            - 소벨마스크, 샤르마스크
                -magnitude()함수, phase()함수
            - 마스크 기반 엣지 검출
                - 캐니 엣지 검출기 canny()함수
                    가우시안필터링 -> 그래디언트계산 -> 비최대억제 -> 히스테리시스 엣지 트래킹
            
-------------------------------------------------------------------------------------------
## 2024-05-09 [목]
-------------------------------------------------------------------------------------------

- 9장 이어서 시작 9.2 직선 검출과 원 검출
    - 허프 변환 직선 검출 
        - 극좌표계의 좌표점을 검출해 직선, 원 검출한다.
        - 직교좌표로는 검출이 힘들고 극좌표계로 추출해야한다.
        - 축적배열  - 직선이 지나가는 행렬의 값을 1 씩 증가 시키는 것
                   - 이 작업을 반복하면 최종적으로 한점에서 값이 높아지게 되는 것이다.
            - 허프 변환 직선, 원검출 
                1.허프 변환 좌표계에서 행렬구성
                2.영상 내 모든 화소의 직선여부 검사
                3.직선 인지 좌표에 대한 허프 변환누적행렬구성
                4.허프누적행렬의 지역최대값 선정
                5.임계값 이상인 누적값(직선) 선별
                6.직선( )을 누적값 기준으로 내림차순 정렬
            - HoughLines(), HoughLinesP(),HoughCircles()
            - HouchLinesTrackbar 설치하기 실습

- 10장 시작 [ 컬러 영상 처리 ]
    - 컬러 영상다루기
        - imread의 3채널 컬러영상은 보통 Vec3b 타입을 사용함
        - Vec3b는 uchar가 3개인 정확히 3byte
        - 컬러영상의 픽셀 값 반전
    - 색 공간 변환
        - cvtColor(), split(), merge() 색상 채널 나누기
    - 컬러 히스토그램 평활화
        - 영상에서 색에 의한 구분을 지을때는 HSV로 영역을 지정하는게 좋다.
            -inRange()함수
        - 히스토그램 역투영 [실습x]
            -calcBackProject()함수

- 11장 시작 [ 이진화와 모폴로지 ]
    - 영상의 이진화 Binarization
        - 영상의 픽셀값을 0 또는 255로 만드는 연산
        - 이진화를 하는 이유? 
            1. 배경과 객체를 구분
            2. 관심영역과 비관심 영역 구분
            -threshold()함수
        - 적응형 이진화 - 픽셀마다 서로 다른 임계값을 사용
            -adptiveThreshold()함수
        - 모폴로지 Morphology
            - 영상을 형태학적인 측면으로 접근하는 것
            - 구조요소는 모폴로지 연산의 결과를 결정하는 커널, 마스크. 윈도우
        - 이진 영상의 침식 연산 erosion - 객체 외각을 깎아내는 연산 + 작은크기의 객체잡은제거 효과
            - erode()함수
        - 이진 영상의 팽창 연산 dilation - 객체 외곽을 확대시키는 연산
            - dilate()함수
        - 이진영상의 열기 닫기
            - 열기 - 침식을 적용하고 팽창을 적용하는 기법
            - 닫기 - 팽창을 적용하고 침식을 적용하는 기법
            - morphologyEx()함수

- 12장 시작 [레이블링과 외곽선 검출]
    - 레이블링 Labeling - 객체구역을 영역단위로 분석, 객체 픽셀에 고유한 번호를 지정하는 작업(레이블맵), 이진영상에서 수행
    - 객체단위 분석 - 객체를 분할아여 특징을 분석, 객체위치 및 크기정보, ROI추출, 모양분석
        -4이웃 연결관계, 8이웃 연결관계
        -connectedComponents(),connectedComponentsWithStats();
    -외곽선 검출 - 객체의 외곽선 좌표를 모두 추출하는 작업, 외각선의 계층구조도 표현
        -findcontours(), drawContours()
    -외곽선처리 함수
        -boundingRect(), minAreaRect(),minEnclosingCircle(), arcLength(), contourArea, approxPolyDp()

- 13장 시작 [객체 검출]
    - 템플릿 매칭 - template matching
    - 캐스케이드 분류기와 얼굴 검출
    - HOG 알고리즘과 보행자 검출
        - SVM
    - QR 코드 검출
    ppt파일 없어서 간단하게 하고 넘어감

- 14장 시작 [ 지역특징점 검출과 매칭 ]
    - 코너 검출
        - 코너점들이 영상에서 고유한 특징을 갖고있어서, 변별력있게 잘 검출
        - 지역 특징을 갖고 있어 변별성이 있고 고유성이 있다.
            cornerHarris()함수
        - FAST() 함수 - 속도가 빠름 [features from accelerated segment test]
        - GFTT() 함수 - 좀 더 디테일함 (Good Features to Track)
    - 크기 불변 특징점 검출과 기술
        - 특징점 검출 클래스
        알고리즘
        - KeyPoint 클래스 (pt, size, angle, response, octave, class_id)
            - ORB (Oriented Fast and Rotated BRIEF 알고리즘)
            - AKAZE (Accelerated_KAZE 알고리즘)
            - SIFT (Scale - Invariant Feature Transform 알고리즘)
                수행과정 
                    입력영상 -> 특징검출기 -> 디스크립터 -> 매칭
            - ORB::create(), ORB::detect(), ORB::compute(), drawKeypoints
    - 특징점 매칭 - Feature point matching
        - Dmatch 클래스  - queryldx, trainldx, imgldx, distance
        - DescritorMatcher 클래스
            - match 가장 비슷한거 1개를 매칭
        - BFMatcher 클래스 (Brute-Force Matcher) 클래스
            - querDescriptors와 trainDescriptors를 하나하나 확인해 매칭되는지 판단하는 알고리즘 - 특징점이 많을수록 연산이 늘어남
        - FLANN(Fast Library for Approximate Nearest Neighbors Matching) 클래스
            - 모든 기술자를 전수 조사하기 보다 이웃하는 기술자끼리 비교
        - BFMatcher::create 메소드
        - FlannBaseMatcher::create 메소드
        - DescriptorMatcher::match 메소드
            - drawMatches() 함수
        -호모그래피 (Homography)
            - 두 평면 사이의 투시변환을 의미
            - 호모그래피는 투시변환과 유사
                -findHomography함수, RANSAC 알고리즘즘
            - find_homography() 예제 
            - Stitcher 클래스 
                - 영상을 이어붙일수 있게 제공
                - create로 객체생성
                - stitch로 vector<Mat>를 붙여서 dst로 반환

---------------------------------------------------------------------------------------------------------------------------
## 2024-05-10 [금]
---------------------------------------------------------------------------------------------------------------------------
[PROJECT_II] - BEGIN
[PROJECT_II] - END _ 5.20 월

## 2024-05-21 [화]--------------------------------------------------
python 시작
가상환경 설정 설명

## 목차
1. [파이썬이란?](#파이썬이란)
2. [파이썬 설치](#파이썬-설치)
3. [기본 문법](#기본-문법)
    - [Hello, World!](#hello-world)
    - [변수와 데이터 타입](#변수와-데이터-타입)
    - [기본 연산](#기본-연산)
4. [첫 번째 파이썬 프로그램](#첫-번째-파이썬-프로그램)

## 파이썬이란?

파이썬(Python)은 해석되고, 인터프리터 방식이며, 객체 지향적이고, 고급 프로그래밍 언어입니다. 1991년 귀도 반 로섬(Guido van Rossum)에 의해 처음 만들어졌습니다. 파이썬은 코드 가독성이 높고, 생산성이 뛰어나며, 다양한 라이브러리와 프레임워크를 지원하여 웹 개발, 데이터 분석, 인공지능 등 다양한 분야에서 사용됩니다.

### Linux
터미널을 열고 다음 명령어를 입력하여 파이썬을 설치합니다:
    ```sh
    sudo apt-get update
    sudo apt-get install python3
    ```
## 기본 문법

### Hello, World!
print("Hello, World!")
파이썬설치
    -  linux시스템에서는 기본적으로 설치가 되어있다
        -python3

3장 - 리버스 인덱스가 적용된 이유?

del을 객체를 삭제한다
remove 는 리스트안에 있는 

## 2024-05-22 [수]------------------------------------------------------
아침 모닝 리뷰
Chap 4장 이어서 수업 진행
챕터 4장의 학습 목표
    01 조건문의 기본 개념 이해
    02 다양한 형태의 조건문 사용법 익히기
    03 반복문의 기본 개념 이해
    04 다양한 반복문 사용법 익히기
    05 조건문과 반복문을 활용한 프로그램 작성

* 주요 내용

    - 반복문
        - for ~ in ~:
        - for ~ in range():
        - while ~ :
        - break, continue
            - 구구단 프로그램
            - 진수 변환 프로그램
            - 숫자 찾기 프로그램
            - 평균 구하기 프로그램
    - 함수
        - def 함수명(매개변수)
        - 매개변수 종류
            - 위치 매개변수
            - 키워드 매개변수
            - 디폴트 매개변수
            - 가변 매개변수 - 튜플의 형태로 넘어 간다.
            - 키워드 가변 매개변수 - 딕션너리의 형태로 넘어 간다.
        - 함수의 리턴 값 : 여러개를 리턴 하면 튜플로 전달
        - 함수 호출 방식
            - 기본적으로 모두 call by value 이다.
            - list 를 쓰면 call by referece 처럼 사용 가능하다.
        - 변수의 사용 범위
            - 지역 변수
            - 전역 변수 global 키워드
        - 람다 함수
            - lambda 매개변수 : 리턴값
            - map, filter, reduce
            - 함수의 인자로 함수를 넘길때 사용
        - 제너레이터
            - yield 키워드 사용
            - next 함수 사용
            - for ~ in ~ 사용
            - 제너레이터 표현식
        - 데코레이터
            - 함수를 감싸는 함수
            - 함수의 시작과 끝을 감싸는 함수
            - 함수의 인자를 검사하는 함수
            - 함수의 리턴값을 검사하는 함수
            - 함수의 실행시간을 측정하는 함수
    - 문자열
        - 파이썬의 문자열은 클래스의 객체로서 시퀀스 자료형이다.
        - 문자열의 인덱싱과 슬라이싱
        - 문자열의 메소드
            - upper, lower, title, capitalize, swapcase
            - strip, lstrip, rstrip
            - split, join
            - replace, find, index, count
            - startswith, endswith
            - isalpha, isdigit, isalnum, isspace
        - 문자열 포맷팅
            - % 연산자
            - format 함수
            - f-string
            - 패딩 처리
        - 단어카운팅 프로그램
    - 코딩 테스트 연습(프로그래머스-파이썬3)
        - 꼬리 문자열
        - 정수 찾기
        - 부분 문자열인지 확인하기
        - 부분 문자열
        - 0 떼기

1. 조건문
if 문
    기본 구조: 조건이 참일 때 특정 코드를 실행.
    python
        if 조건:
            실행할_코드

    예제:
    python
        x = 10
        if x > 5:
            print("x는 5보다 큽니다.")

if-else 문
    구조: 조건이 참일 때와 거짓일 때 각각 다른 코드를 실행.
    python
        if 조건:
            실행할_코드_참
        else:
            실행할_코드_거짓

    예제:
    python
        x = 3
        if x > 5:
            print("x는 5보다 큽니다.")
        else:
            print("x는 5보다 작거나 같습니다.")

if-elif-else 문
    구조: 여러 조건을 순차적으로 검사하여 실행.

        python
            if 조건1:
                실행할_코드1
            elif 조건2:
                실행할_코드2
            else:
                실행할_코드3
    예제:
    python
        x = 7
        if x > 10:
            print("x는 10보다 큽니다.")
        elif x > 5:
            print("x는 5보다 큽니다.")
        else:
            print("x는 5보다 작거나 같습니다.")
2. 반복문
    for 문
        기본 구조: 시퀀스의 각 요소를 순회하며 실행.
    python
        for 변수 in 시퀀스:
            실행할_코드
    예제:
    python
        for i in range(5):
            print(i)

    while 문
        기본 구조: 조건이 참일 동안 반복하여 실행.

    python

        while 조건:
            실행할_코드
    python
        x = 0
        while x < 5:
            print(x)
            x += 1

3. 반복 제어문
break 문

    기능: 반복문을 즉시 종료.

    python

for i in range(10):
    if i == 5:
        break
    print(i)

예제:

python

    i = 0
    while True:
        if i == 3:
            break
        print(i)
        i += 1

continue 문

    기능: 현재 반복을 건너뛰고 다음 반복으로 진행.

    python

    for i in range(5):
        if i == 2:
            continue
        print(i)

4. 중첩 조건문과 반복문

    구조: 조건문과 반복문을 서로 포함하여 복잡한 흐름 제어.

    python

    for i in range(3):
        for j in range(3):
            if i == j:
                print(f'i와 j가 같습니다: {i}')


결론

챕터 4장에서는 파이썬에서 조건문과 반복문을 사용하여 프로그램의 흐름을 제어하는 방법을 학습하였습니다. 조건문을 통해 조건에 따라 다른 코드를 실행하고, 반복문을 통해 코드의 특정 부분을 여러 번 반복 실행할 수 있다. 
이러한 제어 구조를 활용하여 보다 복잡하고 유연한 프로그램을 작성할 수 있다.
- 조건문
- 반복문

* Chap 5장 시작
    01 함수기초
    02 함수심화
    03 함수의 인수
    04 좋은코드를 작성하는 방법

1. 함수의 기본 개념

    함수란?: 특정 작업을 수행하는 코드의 묶음. 코드의 재사용성을 높이고, 프로그램을 더 구조적으로 작성할 수 있게 해줌.
    함수의 장점: 코드의 중복을 줄이고, 가독성을 높이며, 유지보수를 용이하게 함.

2. 함수 정의 및 호출

    함수 정의: def 키워드를 사용하여 함수를 정의. 함수 이름과 괄호 안에 매개변수를 포함.

    python
        def 함수이름(매개변수1, 매개변수2):
            실행할_코드
            return 반환값

    함수 호출: 함수 이름과 괄호 안에 인수(argument)를 넣어 호출.

    python
        함수이름(인수1, 인수2)

3. 매개변수와 반환 값

    위치 매개변수와 키워드 매개변수: 위치 매개변수는 순서대로 인수를 전달하고, 키워드 매개변수는 이름을 명시하여 인수를 전달.
    기본 매개변수 값: 매개변수에 기본값을 설정하여 함수 호출 시 해당 매개변수를 생략 가능.

    python

        def 함수이름(매개변수1, 매개변수2=기본값):
            실행할_코드

    반환 값: return 문을 사용하여 함수가 값을 반환. 반환 값이 없을 경우 None을 반환.

4. 람다 함수와 고차 함수

    람다 함수: 익명 함수로, 짧은 코드를 간결하게 작성할 때 사용.

    python

    lambda 매개변수1, 매개변수2: 표현식

    고차 함수: 함수를 인수로 받거나 함수를 반환하는 함수. 대표적인 예로 map, filter, reduce가 있음.
        map(함수, 시퀀스): 시퀀스의 각 요소에 함수를 적용하여 새로운 시퀀스를 반환.
        filter(함수, 시퀀스): 시퀀스의 각 요소에 함수를 적용하여 참인 요소들로 새로운 시퀀스를 반환.
        reduce(함수, 시퀀스): 시퀀스의 각 요소에 함수를 차례대로 적용하여 단일 값을 반환 (이용 시 functools 모듈 필요).

5. 모듈화와 코드 재사용

    모듈화: 관련 함수들을 모아 모듈로 관리. 모듈은 파일 단위로 존재하며, 다른 파이썬 파일에서 import하여 사용.

    python

    import 모듈이름
    모듈이름.함수이름()

    코드 재사용성: 함수와 모듈을 이용하여 반복적인 코드 작성을 피하고, 효율적으로 코드를 재사용.

예제
함수 정의와 호출 예제

python

def add(a, b):
    return a + b

result = add(3, 4)
print(result)  # 출력: 7

람다 함수와 고차 함수 예제

python

# 람다 함수 예제
multiply = lambda x, y: x * y
print(multiply(3, 4))  # 출력: 12

# map 함수 예제
numbers = [1, 2, 3, 4, 5]
squared = list(map(lambda x: x**2, numbers))
print(squared)  # 출력: [1, 4, 9, 16, 25]

결론

챕터 5장에서는 파이썬 프로그래밍에서 함수의 중요성과 활용 방법에 대해 학습하였습니다. 함수를 사용하여 코드의 재사용성을 높이고, 보다 효율적이고 가독성 좋은 코드를 작성할 수 있습니다. 다양한 함수 작성 방법과 고차 함수의 사용법을 익히면 데이터 과학 작업에서도 유용하게 활용할 수 있습니다.

*책에 없는 내용
    lambda()
    yield()
    List() = li()
    decorator()

* Chap6 문자열 BEGIN
    문자열의 이해
    Lab : 단어 카운팅
    문자열 서식 지정

## 2024-05-23 [목]--------------------------------------------------------

* chap7 - 자료구조 시작
    01 자료구조의 이해
        - 자료구조는 데이터를 효율적으로 저장하고 관리하는 방법을 의미한다. 좋은 자료구조는 프로그램의 성능을 향상시키고 메모리 사용을 최적화할 수 있다. 파이썬에서는 다양한 내장 자료구조와 모듈을 제공하여 데이터 관리를 쉽게 할 수 있다.
    02 스택과 큐
        - 스택 (Stack)
            - 스택은 LIFO(Last In First Out) 구조로, 마지막에 삽입된 요소가 가장 먼저 제거됩니다. 주요 연산은 push (삽입)와 pop (제거)입니다.
        - 큐 (Queue)
            - 큐는 FIFO(First In First Out) 구조로, 먼저 삽입된 요소가 가장 먼저 제거됩니다. 주요 연산은 enqueue (삽입)와 dequeue (제거)입니다.
    03 튜플과 세트
        - 튜플 (Tuple)
            -튜플은 불변(immutable)한 순서가 있는 자료구조로, 한 번 생성되면 수정이 불가능합니다.
        -세트 (Set)
            -세트는 중복을 허용하지 않는 자료구조로, 순서가 없습니다. 집합 연산을 수행할 수 있습니다.
    04 딕셔너리
        - 딕셔너리는 키-값 쌍으로 데이터를 저장하는 자료구조로, 키를 통해 값을 빠르게 조회할 수 있습니다.
    05 collections 모듈
        - 'collections' 모듈은 파이썬의 내장 자료구조를 보완하는 유용한 자료구조들을 제공합니다.
        - 'deque' 양방향 큐로, 양 끝에서 빠른 추가와 제거가 가능합니다.
        - 'defaultdict' 기본 값을 제공하는 딕셔너리입니다.    
    06 Lab:텍스트 마이닝 프로그램

    # 딕셔너리 - 딕셔너리는 순서가 없으니 어떻게 출력해야겠다라는것을 잊어라.


* chap8 - 파이썬 스타일 코드 I
    01 파이썬스타일 코드의 이해
        * 파이썬 스타일 코드의 이해

            - 파이썬 스타일 코드는 파이썬 커뮤니티에서 정한 규칙을 따라 깨끗하고 읽기 쉬운 코드를 작성하는 것을 의미합니다. 가장 널리 인정된 가이드라인은 PEP 8입니다. PEP 8을 따르면 코드의 일관성이 보장되고 유지 보수성이 향상된다.
        * PEP 8의 주요 내용:
            - 들여쓰기: 들여쓰기 수준당 4칸의 공백을 사용합니다.
            - 행 길이: 각 행은 79자를 넘지 않도록 합니다.
            - 빈 줄: 함수와 클래스, 그리고 함수 내부의 큰 코드 블록을 구분할 때 빈 줄을 사용합니다.
            - 임포트 순서: 표준 라이브러리 임포트, 서드파티 라이브러리 임포트, 로컬 어플리케이션/라이브러리 임포트 순으로 그룹화합니다. 각 그룹 사이에는 빈 줄을 넣습니다.
            - 이름 규칙: 함수와 변수 이름에는 snake_case, 상수에는 UPPER_CASE, 클래스 이름에는 CamelCase를 사용합니다.
            - 공백: 표현식과 문장에서 불필요한 공백을 피합니다.

    02 문자열의 분리 및 결합
        - split(), join()
    03 리스트 컴프리헨션
        - 리스트 컴프리헨션은 리스트를 생성하는 간결한 방법이다. 구문은 [표현식 for 항목 in 반복가능객체 if 조건] 이다.
    04 다양한방식의 리스트값 출력
        - 기본루프(), join(), 리스트 컴프리헨션(), '*'연산자().


* chap9 - 파이썬 스타일 코드 II
    01 람다함수
        - 람다 함수는 lambda 키워드를 사용하여 정의되는 작은 익명 함수이다. 인자는 여러 개일 수 있지만 표현식은 하나만 가질 수 있다. 
            - 예 add = lambda x, y: x + y
                 print(add(2, 3))  # 출력: 5
    02 맵리듀스
        - map()
            입력 리스트의 모든 항목에 함수를 적용합니다.
        - filter()
            리스트에서 조건을 만족하는 항목만을 걸러냅니다.
        - reduce()
            리스트의 항목들을 순차적으로 누적 적용하여 하나의 값으로 반환합니다. functools 모듈이 필요합니다.        
    03 별표의활용
        - 반복 가능한 객체의 언패킹, 가변 길이 인자, 키워드 인자
    04 선형대수학
        - 파이썬에서는 NumPy와 같은 강력한 라이브러리를 사용하여 선형대수학을 다룰 수 있습니다.

chap 10 객체지향프로그래밍
    - 객체지향 프로그래밍(Object-Oriented Programming, OOP)은 소프트웨어 개발 패러다임의 하나로, 데이터를 객체로 다루며 이 객체들을 통해 프로그램을 구조화하는 방법론이다

    01 객체지향 프로그래밍의 이해
        - 객체지향 프로그래밍(OOP)은 프로그래밍 패러다임 중 하나로, 데이터를 객체(object)로 표현하고 이 객체들을 조작하며 프로그램을 개발하는 방식입니다. 주요 개념으로는 클래스(class), 객체(object), 상속(inheritance), 캡슐화(encapsulation), 다형성(polymorphism), 추상화(abstraction) 등이 있습니다.

            * 클래스(Class): 객체를 생성하기 위한 청사진 또는 템플릿.
            * 객체(Object): 클래스의 인스턴스(instance)로, 실제로 메모리에 할당된 데이터.
            * 상속(Inheritance): 새로운 클래스가 기존 클래스의 특성과 메서드를 물려받는 것.
            * 캡슐화(Encapsulation): 데이터와 메서드를 하나의 단위로 묶고, 데이터의 직접적인 접근을 제한하는 것.
            * 다형성(Polymorphism): 동일한 메서드나 연산자가 다른 클래스에서 다른 의미로 동작하는 것.
            * 추상화(Abstraction): 불필요한 세부사항을 숨기고, 필요한 기능만 노출하는 것.

    02 파이썬의 객체지향프로그래밍
        - 클래스 구현하기
            - 속성의 선언
            - 함수의 선언
            - 3_의 쓰임
        - 인스턴스 사용하기
        - 클래스를 사용하는 이유

    03 Lab : 노트북 프로그램 만들기
        - 노트북(Notebook) 클래스를 생성한다.
        - 각 노트북 객체는 제목(title), 내용(content), 작성자(author)를 속성으로 가집니다.
        - 노트북에 메모를 추가하고, 메모 목록을 확인할 수 있는 기능을 구현합니다.

    04 객체지향 프로그래밍의 특징
        * 상속, 다형성, 가시성!

        - 상속(Inheritance): 기존 클래스의 속성과 메서드를 새로운 클래스에 상속하여 코드의 중복을 줄이고, 계층적인 구조를 만들고, 
        - 다형성(Polymorphism): 인터페이스 - 같은 인터페이스를 통해 다른 방식으로 동작하도록 하여, 코드의 유연성과 확장성을 높이고, 다른 사람으 ㅣ코드를 쉽게 재사용하기 위해 사용한다.
        - 가시성(Visibility) - 캡슐화 + 은닉 - 가시성은 데이터 보호와 코드의 명확한 인터페이스 제공을 목적으로 하며, 주로 캡슐화(encapsulation) 원칙을 구현하는 데 사용됩니다.

        - 파이썬에는 기본적으로 제공되는 데코레이터 외에도, 사용자 정의 데코레이터를 통해 원하는 기능을 추가할 수 있습니다. 가장 일반적으로 사용되는 표준 라이브러리 데코레이터는 다음과 같다. 
        - 데코레이터는 함수와 클래스 이외에는 붙일수가 없다.
            -@property
            -@classmethod
            -@staticmethod
            -@functools.wraps
            -@functools.lru_cache
            -@dataclass
            -@contextmanager

        -pygame 실습

## 2024-05-24 [금]--------------------------------------------------------
-pygame 스네이크 게임 실습

Chapter11 모듈과패키지 시작

    01 모듈과 패키지의이해
    02 모듈 만들기
    03 패키지 만들기
    04 가상환경 사용하기

Chapter12 예외처리와 파일 다루기

    01 예외처리
    02 파일다루기

Chapter13 CSV와 로그관리

    01 CSV
    02 로그관리 [ LOGGING : 매우중요!!!]
    03 설정저장
    04 Lab:로깅 프로그램

Chapter14 웹스크래핑

    01 웹의이해
    02 HTML 데이터다루기
    03 정규 표현식
    04 Lab: 웹 스크래핑실습

Chapter15 XML과 JSON

    01 XML의 이해
    02 Lab:XML파싱
    03 JSON이해
        -환경변수를 저장하는 형태
    04 Lab:JSON 데이터 분석

Python 끝

--------------------------------------------------------------------------------------
## 2024-05.27 [월]
--------------------------------------------------------------------------------------
Practical Statistics for Data scientist 시작
데이터 과학을 위한 통계 = 데이터분석에서 머신러닝까지 파이썬과 R로 살펴보는 50가지 핵심개념
탐색적 데이터 분석[exploratory data analysis (EDA)]

그럼 통계를 왜배우는가? - 빅데이터를 배우기 위해
                        - 표본에 대한 평균을 구하고 그 모집단을 확실시 하게 하는법
                        대표값 [ 평균,중앙평균, 가중평균, 절사평균]
                               [ 분산(n-1m 루트를 씌움 - 표준편차) - 편차. 절대편산 ]
                        - 데이터분포 탐색
                        수치데이터 - 히스토그램 x-y, boxblot상자그림
                        이산데이터 [범주]- 히스토그램 
                        상관관계 corr, -1 SNS -> heatmap
                        PANDAS 모르면 안되어서 NUMPY와 PANDAS를 딥러닝 책에서 먼저 배움

    PANDAS
    NUMPY - []약점 C-type array. 선언 -> one zeros empty
            [], 단위, eye, diag.

    메소드 - reshape주의사항(원소를 낮춰야한다.)
           - 사칙연산 +,-,*,//
           - dot - 브로드캐스팅 까지... 

Chap1. 탐색적 데이터 분석
1.1 정형화된 데이터의 요소
1.2 테이블 데이터
    - 데이터 프레임과 인덱스
    - 테이블 형식이 아닌 데이터 구조
1.3 위치 추정
    - 평균
    - 중간값과 로버스트 추정 [ 중간값, 가중중간값, 특잇값]
    - 인구에 따른 살인 비율의 위치 추정
1.4 변이 추정
    - 변이, 산포도
        용어정리
        - 편차, 분산, 표준편차, 평균절대편차, 중간값의 중위절대편차, 범위, 순서통계량, 백분위수, 사분위범위(IQR)
    - 표준편차와 관련 추정값들, 백분위수에 기초한 추정, 주별인구의 변이추정
1.5 데이터 분포 탐색하기
    - 백분위수와 상자그림, 도수분포표와 히스토그램, 밀도 그림과 추정
1.6 이진 데이터와 범주 데이터 탐색하기
    - 최빈값, 기대값, 확률
1.7 상관관계
    -산점도, 상관계수는 표준화된 측정지표라고 할 수 있다.

넘파이 numpy
넘파이? = 과학계산을 위한 라이브러리, 주로 배열처리에 특화됨.
        [다차원리스트처리, 데이터의크기, 처리시간 이 3가지 문제들을 해결하기 위해 가장많이사용하는 도구]

---------------------------------------------------------------------------------------------------
## 2024-05.28 [화]
---------------------------------------------------------------------------------------------------
NUMPY, PANDAS
# NumPy and Pandas Overview

## NumPy
### 정의
NumPy는 파이썬에서 과학 계산을 위한 필수적인 라이브러리로, 주로 고성능의 다차원 배열 객체를 제공하고 다양한 수치 계산을 지원합니다. 수학, 통계, 선형 대수, 푸리에 변환 등의 기능을 통해 효율적인 배열 연산을 가능하게 합니다.

### 특징
- **N차원 배열 (ndarray)**
- **고성능**: 대규모 수치 계산에 최적화
- **수학 함수, 선형 대수, 푸리에 변환 지원**
- **브로드캐스팅**: 배열 간의 연산을 자동으로 확장
- **벡터화 연산**: 루프 없이 배열 전체에 대한 연산 수행
- **메모리 효율성**: 연속된 메모리 블록에 데이터 저장
- **확장성**: C/C++와의 통합 가능

### 장점
- **성능**: 매우 빠름, 대규모 수치 계산에 최적화
- **브로드캐스팅 및 벡터화 연산 지원**: 복잡한 수치 연산을 간편하게 수행
- **메모리 사용 효율성**: 대규모 데이터를 효율적으로 처리

### 단점
- **복잡한 데이터 조작 어려움**: 고급 데이터 분석 및 조작 기능 부족
- **파일 입출력 기능 부족**: 데이터 파일 읽기/쓰기가 직접적으로 지원되지 않음
- **다차원 배열 조작 복잡**: 다차원 배열을 다루는 코드가 복잡할 수 있음

## Pandas

### 정의
Pandas는 파이썬에서 데이터 조작과 분석을 위한 고성능 데이터 처리 라이브러리입니다. 데이터 프레임과 시리즈라는 두 가지 주요 데이터 구조를 통해 데이터를 효율적으로 처리하고 분석할 수 있게 도와줍니다.

### 특징
- **1차원 시리즈 (Series), 2차원 데이터프레임 (DataFrame)**
- **데이터 분석 및 조작에 최적화**
- **다양한 파일 형식 입출력 지원 (CSV, Excel, SQL, JSON 등)**
- **데이터 정리, 필터링, 그룹화, 집계, 결측값 처리 기능 제공**
- **시계열 데이터 처리**: 시간 인덱스를 가진 데이터 처리
- **직관적인 데이터 프레임 구조**

### 장점
- **사용 편의성**: 직관적인 데이터 프레임 구조와 다양한 기능 제공
- **다양한 데이터 처리 기능**: 데이터 정리, 변환, 필터링, 그룹화 등 용이
- **다양한 파일 형식 지원**: 데이터 입출력이 간편
- **데이터 분석 및 조작 최적화**: 강력한 데이터 조작 및 분석 기능 제공

### 단점
- **성능**: 대규모 데이터 처리 시 NumPy보다 약간 느림
- **메모리 사용 비효율성**: 큰 데이터 프레임 처리 시 메모리 사용량이 많을 수 있음
- **고급 수치 계산 기능 부족**: 복잡한 수치 계산에는 적합하지 않음

### 조인(Join) 유형

Pandas는 SQL과 유사한 조인 연산을 통해 여러 데이터프레임을 결합할 수 있습니다. 주요 조인 유형은 다음과 같습니다:

- **이너 조인 (Inner Join)**: 두 데이터프레임에서 공통적으로 존재하는 키를 기준으로 결합합니다.
- **레프트 조인 (Left Join)**: 왼쪽 데이터프레임의 모든 데이터를 유지하면서, 오른쪽 데이터프레임의 공통 키를 기준으로 결합합니다.
- **라이트 조인 (Right Join)**: 오른쪽 데이터프레임의 모든 데이터를 유지하면서, 왼쪽 데이터프레임의 공통 키를 기준으로 결합합니다.
- **아우터 조인 (Outer Join)**: 두 데이터프레임의 모든 데이터를 결합하며, 공통 키가 없는 경우 NaN으로 표시합니다.

#### 조인 예시

```python 예
import pandas as pd

# 데이터 프레임 생성
df1 = pd.DataFrame({
    'key': ['A', 'B', 'C'],
    'value1': [1, 2, 3]
})

df2 = pd.DataFrame({
    'key': ['B', 'C', 'D'],
    'value2': [4, 5, 6]
})

# 이너 조인
inner_join = pd.merge(df1, df2, on='key', how='inner')
print('Inner Join:\n', inner_join)

# 레프트 조인
left_join = pd.merge(df1, df2, on='key', how='left')
print('Left Join:\n', left_join)

# 라이트 조인
right_join = pd.merge(df1, df2, on='key', how='right')
print('Right Join:\n', right_join)

# 아우터 조인
outer_join = pd.merge(df1, df2, on='key', how='outer')
print('Outer Join:\n', outer_join)

집계 함수 (Aggregation Functions)

집계 함수는 데이터의 여러 값들을 하나의 요약된 값으로 변환하는 함수입니다. 주로 데이터 요약 및 통계 분석에 사용됩니다.
주요 집계 함수

    sum(): 합계를 계산합니다.
    mean(): 평균을 계산합니다.
    median(): 중앙값을 계산합니다.
    min(): 최소값을 계산합니다.
    max(): 최대값을 계산합니다.
    count(): 데이터의 개수를 계산합니다.
    std(): 표준 편차를 계산합니다.
    var(): 분산을 계산합니다.

변환 함수 (Transformation Functions)

변환 함수는 데이터의 각 요소를 변환하는 데 사용됩니다. 주로 데이터 전처리 및 정규화 작업에 사용됩니다.
주요 변환 함수

    apply(): 특정 함수를 데이터프레임의 행 또는 열에 적용합니다.
    map(): 시리즈의 각 요소에 특정 함수를 적용합니다.
    replace(): 특정 값을 다른 값으로 대체합니다.
    astype(): 데이터 타입을 변환합니다.

Pandas의 주요 함수 요약
집계 함수 요약

    sum(): 열 또는 행의 합계를 계산
    mean(): 열 또는 행의 평균을 계산
    median(): 열 또는 행의 중앙값을 계산
    min(): 열 또는 행의 최소값을 계산
    max(): 열 또는 행의 최대값을 계산
    count(): 열 또는 행의 데이터 개수를 계산
    std(): 열 또는 행의 표준 편차를 계산
    var(): 열 또는 행의 분산을 계산

변환 함수 요약

    apply(): 특정 함수를 데이터프레임의 행 또는 열에 적용
    map(): 시리즈의 각 요소에 특정 함수를 적용
    replace(): 특정 값을 다른 값으로 대체
    astype(): 데이터 타입 변환

필터 함수 요약

    query(): 쿼리 문자열을 사용하여 데이터 필터링
    loc[]: 라벨을 사용하여 데이터 선택 및 필터링
    iloc[]: 위치 인덱스를 사용하여 데이터 선택 및 필터링
    drop(): 특정 행 또는 열 삭제
    isin(): 특정 값이 포함된 행 필터링

병합 함수 (Merge Functions)

병합 함수는 SQL의 JOIN 연산과 유사하게 두 데이터프레임을 특정 키를 기준으로 병합합니다. 병합 유형에는 이너 조인, 레프트 조인, 라이트 조인, 아우터 조인이 포함됩니다.
주요 병합 함수

    pd.merge(): 두 데이터프레임을 키를 기준으로 병합합니다.

병합 유형

    이너 조인 (Inner Join): 두 데이터프레임에 공통적으로 존재하는 키를 기준으로 병합합니다.
    레프트 조인 (Left Join): 왼쪽 데이터프레임의 모든 데이터를 유지하며, 오른쪽 데이터프레임의 공통 키를 기준으로 병합합니다.
    라이트 조인 (Right Join): 오른쪽 데이터프레임의 모든 데이터를 유지하며, 왼쪽 데이터프레임의 공통 키를 기준으로 병합합니다.
    아우터 조인 (Outer Join): 두 데이터프레임의 모든 데이터를 포함하여 병합하며, 공통 키가 없는 경우 NaN으로 표시합니다.

연결 함수 (Concatenate Functions)

연결 함수는 여러 데이터프레임을 행 또는 열 방향으로 연결합니다. 주로 데이터프레임을 쌓거나 붙이는 데 사용됩니다.
주요 연결 함수

    pd.concat(): 여러 데이터프레임을 행 또는 열 방향으로 연결합니다.
    df.append(): 두 데이터프레임을 행 방향으로 연결합니다.

병합 함수 요약

    pd.merge(): 두 데이터프레임을 키를 기준으로 병합합니다.
        how 파라미터를 사용하여 조인 유형 선택 ('inner', 'left', 'right', 'outer').

연결 함수 요약

    pd.concat(): 여러 데이터프레임을 행 또는 열 방향으로 연결합니다.
        axis 파라미터를 사용하여 방향 선택 (axis=0은 행 방향, axis=1은 열 방향).
    df.append(): 두 데이터프레임을 행 방향으로 연결합니다.

----------------------------------------------------------------------------------------------------------------------------------
## 2024.05.29 [수]         matplotlib 시각화 리뷰
----------------------------------------------------------------------------------------------------------------------------------
데이터 과학을 위한 파이썬머신러닝 책에서 4가지만 먼저 학습

01.데이터시각화(data visualization)
    ** 정의 ** : 데이터분석결과를 쉽게 이해할수 있도록 시작걱즈올 ㅍ현하고 전달하는 과정을 의미한다.
    ** 도구 ** : 맷플롯립(matplotlib), 시본(seaborn), 플롯리(plotly)

02.맷플롤립(Matplotlib)
    **   모듈 호출   ** : import matplotlib.pyplot as plt
    **   기본 구조   ** : 맷플롯립을 이요할때 가장 기본이 되는 구조 파이플롯(pyplot)위에 그림 (figure)객체를 올리고 그 위에 그래프를 그리기 위한 축을 올리는 형식
    ** 그래프 꾸미기 ** : 맷플롯립에서는 색상(color), 선의 형태(linestyle), 제목(title), 범례(legend)등을 변형하여 다양한 형태로 그래프를 꾸밀수있다.
    ** 그래프의 종류 ** : 산점도, 막대그래프, 누적막대그래프, 히스토그램, 상자그림

03.시본(seaborn)
    **   모듈 호출   ** : import seaborn as sns
    **사용하는 그래프** : 희귀 그래프, 산점도, 비교 그래프, 막대 그래프
    **사전정의된그래프** : 바이올린 플롯, 스웜 플롯, 패싯그리드

04.플롯리(plotly)
    **   모듈 구조   ** : import plotly.graph_objects as go
    **     정의      ** : 비즈니스 인텔리전스 대시보드로의 역할을 하기위해 개발된 도구로, 인터랙션 그래프를 지원한다.



평균과 분산에 대해서 
위치추정 
로버스트한 값을 갖기위해서 중간값을 
그래프로 그려봤을때 범주형데이터를 어떻게 처리하는지?
범주된 수치된 데이터는 boxplat or violin plot
상관관계 두 변수가 얼마나 연관이 있는지? 따라 움직이는지? 값




Chap2. 데이터와 표분분포
    2.1 임의표본추출과 표본편향
    임의표본추출은 대상이 되는 모집단내의 선택가능한 원소들을 무작위로 추출하는 과정을 말한다.
    그 결과 얻은 샘플을 단순임의표본이라 한다.
    복원추출 | 비복원추출 | 대표성

    **용어정리**
        * 표본[sample] - 더 큰 데이터 집합으로부터 얻은 부분집합
        * 모집단[population] - 어떤데이터집합을 구성하는 전체대상 혹은 전체집합
        * N[n] - 모집단의 크기
        * 임의표본추출[random sampling] - 무작위로 표본을 추출하는것
        * 층화표본추출[stratified sampling] - 모집단을 층으로 나눈뒤, 각 층에서 무작위로 표본을 추출하는것
        * 계층[stratum] - 공통된 특징을 가진 모집단의 동종 하위 그룹(복수형은 strata로 쓴다)
        * 단순임의표본[simple random sample] - 모집단 층화없이 임의표본추출로 얻은 표본
        * 편향[bias] - 계통상의 오류
        * 표본편향[sample bias] - 모집단을 잘못 대표하는 표본
        
        2.1.1 편향
        2.1.2 임의선택
        2.1.3 크기와 품질:크기는 언제중요해지나?
        2.1.4 표본평균과 모평균
    
    2.2 선택편향
        - 데이터를 의식적이든 무의식적이든 선택적으로 고르는 관행을 의미한다.
        
        **용어정리**
            * 선택편향[selection bias] - 관측데이터를 선택하는 방식떄문에 생기는 편향
            * 데이터 스누핑[data snooping] - 뭔가 흥미로운 것을 찾아 광범위하게 데이터를 살피는것
            * 방대한검색효과[vast search effect] - 중복데이터 모델링이나 너무 많은예측변수를 고려하는 모델링에서 비롯되는편향 혹은 비재현성
            *목표값섞기[target shuffling] - 본질적으로는 순열검정이라는것을 추천함.

        2.2.1 평균으로의 회귀(regression to the mean) - 주어진 어떤 변수를 연속적으로 측정했을때 나타나는 현상.

    2.3 통계학에서의 표본분포
        - 하나의 동일한 모집단에서 얻은 여러샘플에 대한 표본통계량의 분포를 나타냄

        **용어정리**
            * 표본통계량[sample statistic] : 더 큰 모집단에서 추출된표본 데이터들로부터 얻은 측정지표
            * 데이터 분포[data distribution] : 어떤데이터집합에서의 각 개별값의 도수분포
            * 표본분포[sampling distribution] : 여러 표본들 혹은 재표본들로부터 얻은 표본통계량의 도수분포
            * 중심극한증리[central limit theorem] : 표본크기가 커질수록 표본분포가 정규분포를 따르는 경향
            * 표준오차[standard error] : 여러표본들로부터 얻은 표본통계량의 변량[개별데이터 값들의 변량을 뜻하는 표준편차와 혼동하지 말 것]

            표본의 변동성[sampling variablilty]
            2.3.1 중심극한정리
            2.3.2 표준오차

    2.4 부트스트랩[bootstrap]
        - 현재있는 표본에서 추가적으로 표본을 복원추출하고 각 표본에 대한 통계량과 모델을 다시 계산하는 것

        2.4.1 재표본추출 대 부트스트래핑
            - 재표본추출은 여러 표본이 결합되어 비복원추출을 수행할수있는순열과정을 포함함.
    
    2.5 신뢰구간
        -도수분포표, 히스토그램, 상자그림, 표준오차는 모두 표본추정에서 잠재적인 오차를 이해하는 방법

        **용어정리**
            *신뢰수준[confidence level : 같은모집단으로부터 같은방식으로 얻은, 관심 통계량을 포함할 것으로 예상되는, 신뢰구간의 백분율]
            *구간끝점[interal endpoint : 신뢰구간의 최상위, 최하위 끝점]

    2.6 정규분포[normal distribution]
        -전통적인 통계의 상징 - 표본통계량 분포가 보통 어떤 일정한 모양이 있다는 사실은 이 분포를 근사화하는 수학공식을 개발하는데 강력한 도구가 됨.


        **용어정리**
            *신뢰수준[confidence level : 같은모집단으로부터 같은방식으로 얻은, 관심 통계량을 포함할 것으로 예상되는, 신뢰구간의 백분율]
            *구간끝점[interal endpoint : 신뢰구간의 최상위, 최하위 끝점]

    2.6 정규분포[normal distribution]
        -전통적인 통계의 상징 - 표본통계량 분포가 보통 어떤 일정한 모양이 있다는 사실은 이 분포를 근사화하는 수학공식을 개발하는데 강력한 도구가 됨.

        **용어정리**
            *오차[error] : 데이터포인트와 예측값 혹은 평균사이의 차이
        **용어정리**
            *오차[error] : 데이터포인트와 예측값 혹은 평균사이의 차이
            *표준화(정규화)하다[standardize] : 평균을 빼고 표준편차로 나눈다.
            *z 점수[z-score] : 개별 데이터 포인트를 정규화한결과
            *표준정규분포[standard normal distribution] : 평균=0, 표준편차=1 인 정규분포
            *QQ그림[QQ-plot] : 표본분포가 특정 분포(예:정규분포)에 얼마나 가까운지를 보여주는 그림
        
        2.6.1 표준정규분포와 QQ 그림
            - 표준정규분포는 X축의 단위가 평균의 표준편차로 표현되는 정규분포를 말한다.
            - 데이터를 표준정규분포와 비교하려면 데이터에서 평균을 뺀 다음 표준편차로 나누면 됨.[정규화 또는 표준화라고 함] 정규분포 = z 분포
            - QQ그림은 표본이 특정분포에 얼마나 가까운지 시각적으로 판별하는데 사용됨.

    2.7 긴 꼬리 분포[long tail distribution]
        - 긴꼬리 분포 (Long Tail Distribution)

            긴꼬리 분포는 통계학에서 빈도가 낮지만 종류가 많은 항목들이 전체에서 상당한 비중을 차지하는 분포를 말합니다. 이 개념은 주로 다음과 같은 특성을 가집니다:

            1. 빈도 분포의 꼬리 부분이 길다: 많은 항목들이 매우 적은 빈도로 발생하며, 이러한 항목들이 분포의 꼬리 부분을 형성합니다.
            2. 누적 빈도가 크다: 이러한 저빈도 항목들의 총합이 무시할 수 없을 정도로 큰 비중을 차지합니다.
            3.일반적인 사례: 인터넷 콘텐츠, 영화, 음악 등에서 히트작 외에도 다양한 니치(niche) 상품들이 전체 매출이나 사용량에서 큰 부분을 차지하는 경우입니다.

        - 흑고니 이론 (black Swan Theory)

            흑고니 이론은 나심 니콜라스 탈레브(Nassim Nicholas Taleb)가 제안한 이론으로, 다음과 같은 특성을 가진 예측 불가능한 사건을 설명합니다:

            1.극도로 드물고 예측 불가능함: 흑고니 사건은 과거 데이터로부터 예측할 수 없는 사건으로, 발생 확률이 매우 낮습니다.
            2.엄청난 충격을 준다: 발생하면 사회, 경제, 정치 등 다양한 분야에 엄청난 영향을 미칩니다.
            3.사후에 예측 가능한 것처럼 보인다: 사건 발생 후에는 사람들이 그 사건을 예측할 수 있었다고 착각하는 경향이 있습니다.

        **용어 정리**
            *꼬리 [tail] : 적은 수의 극단값이주로 존재하는, 도수분포의 길고 좁은 부분
            *왜도 [skewness] : 분포의 한쪽 꼬리가 반대쪽 다른 꼬리보다 긴 정도

    2.8 T분포
        - T분포[t-distribution]는 T 분포는 정규 분포와 비슷하지만, 꼬리가 더 두꺼운 형태를 가지며, 이는 표본 크기가 작을 때 표본 평균의 변동성이 더 크다는 것을 반영한다.
    
    2.9 이항분포
        -이항분포 (binomial distribution)는 통계학에서 이산 확률 분포 중 하나로, 고정된 수의 독립적인 시행에서 각 시행이 성공 또는 실패의 두 가지 결과만을 가지는 실험에서 성공의 횟수를 모델링하는 데 사용된다.

        **용어 정리**
        시행 : 독립된결과르 가져오는 하나의 사건
        성공 : 시행에 대한 관심의 결과
        이항식 : 두가지 결과를 갖는다
        이하시행 : 두가지 결과를 가져오는 시행
        이항분포 : n번 시행에서 성공한 횟수에 대한 분포

    2.10 카이제곱분포
        -카이제곱 분포(Chi-Square Distribution)는 통계학에서 많이 사용되는 분포 중 하나로, 주로 표본 분산의 분포를 분석하거나 독립성 검정, 적합성 검정 등의 다양한 통계적 가설 검정에 사용됩니다.

        - 카이제곱 분포의 특징

            1. 자유도(df): 카이제곱 분포는 자유도(degrees of freedom)에 따라 모양이 달라지며, 자유도는 보통 표본의 크기나 변수의 수에 따라 결정된다.
            2. 비대칭성: 카이제곱 분포는 오른쪽으로 꼬리가 긴 비대칭 분포입니다. 자유도가 증가할수록 분포는 정규 분포에 가까워진다.
            3. 양수 값: 카이제곱 분포는 0 이상의 값을 가지며, 음수 값은 가지지 않는다.

    2.11 F분포
        -F분포는 측정된 데ㅣ터와 관련한 실험 및 선형 모델에 사용된다.
        -F통계량은 관심요인으로 인한 변동성과 전체변동성을 비교한다.

    2.12 푸아송 분포 그 외 관련된 분포
        - 푸아송분포, 지수분포, 베이불 분포

        ** 용어정리 **
        람다[lambda] : 단위 시간이나 단위 면적당 사건이 발생하는 비율
        푸아송분포[Poisson distribution] : 표집된 단위 시간 혹은 단위 공간에서 발생한 사건의 도수분포
        지수분포[exponential distribution] : 한 사건에서 그 다음 사건까지의 시간이나 거리에 대한 도수분포
        베이불 분포[Weibull distrtibution] : 사건 발생률이 시간에 따라 변화하는, 지수분포의 일반화된 버전

    -----------------------------------총 정리 ----------------------------------------
    오늘 뭐 했는가? 
        
        1.샘플을 할떄 임의추출을 하는것.
        2.분산이나 평균을 표준화를 해야지 비교하기가 쉽다.
        3.다양한 분포들에 대해 공부함


--------------------------------------------------------------------------------------
## 2024-05.30 [목]
--------------------------------------------------------------------------------------
* 오렌지 실습 
* 머신러닝 : 지도학습[Target] ---> 각종이름 ->학습결과->모델 
                     O , X

머신러닝 (Machine Learning)

머신러닝은 컴퓨터가 명시적으로 프로그래밍되지 않고도 데이터를 통해 학습하고 예측할 수 있도록 하는 인공지능(AI)의 한 분야입니다. 즉, 컴퓨터가 데이터에서 패턴을 인식하고, 그 패턴을 기반으로 새로운 데이터에 대해 결정을 내리거나 예측을 하는 기술입니다. 머신러닝은 크게 세 가지 주요 유형으로 나뉩니다: 지도학습, 비지도학습, 강화학습.

1. 지도학습 (Supervised Learning)

지도학습은 입력 데이터와 해당 데이터에 대한 정답(라벨)이 주어질 때, 이 데이터를 바탕으로 새로운 입력 데이터에 대한 예측을 수행하는 학습 방식입니다. 대표적인 지도학습 알고리즘은 다음과 같습니다:

    * 선형 회귀 (Linear Regression)
    * 로지스틱 회귀 (Logistic Regression)
    * 서포트 벡터 머신 (Support Vector Machine, SVM)
    * 결정 트리 (Decision Tree)
    * 랜덤 포레스트 (Random Forest)
    * k-최근접 이웃 (k-Nearest Neighbors, k-NN)

지도학습은 분류(Classification)와 회귀(Regression) 문제로 나뉩니다. 분류는 입력 데이터를 특정 카테고리로 분류하는 것이며, 회귀는 연속적인 값을 예측하는 것입니다.

2. 비지도학습 (Unsupervised Learning)

비지도학습은 입력 데이터만 주어지고 정답(라벨)은 주어지지 않을 때, 데이터의 구조를 학습하는 방식입니다. 대표적인 비지도학습 알고리즘은 다음과 같습니다:

    * 군집화 (Clustering): K-평균 (K-Means), 계층적 군집화 (Hierarchical Clustering)
    * 차원 축소 (Dimensionality Reduction): 주성분 분석 (Principal Component Analysis, PCA), t-SNE

비지도학습의 목표는 데이터의 숨겨진 패턴이나 구조를 발견하는 것입니다.

3. 강화학습 (Reinforcement Learning)

강화학습은 에이전트(Agent)가 환경과 상호작용하면서 보상(Reward)을 최대화하는 행동을 학습하는 방식입니다. 에이전트는 일련의 행동을 통해 환경을 탐색하며, 각 행동의 결과로부터 보상을 받아 정책(Policy)을 개선해 나갑니다. 대표적인 강화학습 알고리즘은 다음과 같습니다:

    Q-러닝 (Q-Learning)
    심층 Q-네트워크 (Deep Q-Network, DQN)
    정책 경사 (Policy Gradient) 방법

머신러닝의 주요 과정

    1. 데이터 수집 (Data Collection): 문제를 해결하기 위한 데이터를 수집합니다.
    2. 데이터 전처리 (Data Preprocessing): 결측치 처리, 데이터 정규화, 범주형 데이터 인코딩 등 데이터를 정리합니다.
    3. 특징 선택 (Feature Selection): 모델 학습에 유용한 특징을 선택하거나 생성합니다.
    4. 모델 선택 (Model Selection): 문제에 맞는 머신러닝 알고리즘을 선택합니다.
    5. 모델 학습 (Model Training): 훈련 데이터를 사용해 모델을 학습시킵니다.
    6. 모델 평가 (Model Evaluation): 검증 데이터를 사용해 모델의 성능을 평가합니다.
    7. 하이퍼파라미터 튜닝 (Hyperparameter Tuning): 모델의 성능을 최적화하기 위해 하이퍼파라미터를 조정합니다.
    8. 모델 테스트 (Model Testing): 테스트 데이터를 사용해 최종 모델의 성능을 평가합니다.
    9. 배포 및 모니터링 (Deployment and Monitoring): 모델을 실제 환경에 배포하고 성능을 지속적으로 모니터링합니다.

지도학습 [Supervised Learning]
    * 지도학습은 머신러닝의 한 종류로, 입력 데이터와 해당 데이터에 대한 정답(라벨)이 함께 주어질 때, 이 데이터를 바탕으로 새로운 입력 데이터에 대한 예측을 수행하는 학습 방식입니다. 지도학습의 주요 목표는 주어진 입력 데이터와 출력 데이터 간의 관계를 학습하는 모델을 만드는 것입니다. 지도학습은 크게 두 가지로 나뉩니다:

    1. 분류 (Classification): 입력 데이터를 특정 카테고리(클래스)로 분류하는 것. 예를 들어, 이메일이 스팸인지 아닌지를 분류하거나, 이미지가 고양이인지 개인지 판별하는 작업이 이에 해당합니다.
    2. 회귀 (Regression): 연속적인 값을 예측하는 것. 예를 들어, 주택 가격을 예측하거나 주식 시장 가격을 예측하는 작업이 이에 해당합니다.

지도학습의 예시:

    분류:
        스팸 필터: 이메일 데이터를 입력으로 받아서 스팸인지 아닌지를 예측.
        이미지 분류: 이미지 데이터를 입력으로 받아서 이미지의 카테고리를 예측.
        의료 진단: 환자의 의료 데이터를 입력으로 받아서 특정 질병의 유무를 예측.

    회귀:
        주택 가격 예측: 주택의 크기, 위치, 방 수 등의 데이터를 입력으로 받아서 주택의 가격을 예측.
        주식 가격 예측: 과거 주식 가격 데이터를 입력으로 받아서 미래 주식 가격을 예측.
        기온 예측: 기상 데이터를 입력으로 받아서 미래의 기온을 예측.

지도학습 과정

    1. 데이터 수집: 입력 데이터(X)와 해당 데이터에 대한 정답(라벨, Y)을 수집합니다.
    2. 데이터 전처리: 결측치 처리, 정규화, 데이터 변환 등 데이터를 모델에 맞게 전처리합니다.
    3. 모델 선택: 문제에 적합한 알고리즘을 선택합니다 (예: 로지스틱 회귀, SVM, 결정 트리 등).
    4. 모델 학습: 훈련 데이터를 사용하여 모델을 학습시킵니다.
    5. 모델 평가: 검증 데이터를 사용하여 모델의 성능을 평가합니다 (예: 정확도, MSE 등).
    6. 하이퍼파라미터 튜닝: 모델의 성능을 최적화하기 위해 하이퍼파라미터를 조정합니다.
    7. 모델 테스트: 테스트 데이터를 사용하여 최종 모델의 성능을 평가합니다.
    8. 배포: 모델을 실제 환경에 배포하여 새로운 데이터를 예측합니다.

아이리스 데이터셋의 세 가지 품종

    1.Setosa
    2.Versicolor
    3.Virginica

Python을 사용하여 지도학습의 간단한 예시. 아이리스 데이터셋을 사용하여 꽃의 품종을 분류하는 로지스틱 회귀 모델을 학습하는 코드입니다.

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 데이터 로드
iris = load_iris()
X = iris.data
y = iris.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 데이터 표준화
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 로지스틱 회귀 모델 훈련
model = LogisticRegression()
model.fit(X_train, y_train)

# 예측
y_pred = model.predict(X_test)

# 정확도 평가
accuracy = accuracy_score(y_test, y_pred)
print(f'모델 정확도: {accuracy * 100:.2f}%')

이 코드는 아이리스 데이터셋을 로드하고, 데이터를 훈련 및 테스트 세트로 분할한 후, 로지스틱 회귀 모델을 훈련시키고, 테스트 데이터에 대한 예측 정확도를 계산한다.

-----다시 데이터과학을 위한 통계 시작 -------------------------------3장-----------

chap3 통계적 실험과 유의성검정 BEGIN

- 유의성 검정 (Significance Testing)

    유의성 검정은 통계학에서 특정 가설을 검증하기 위해 사용되는 방법입니다. 이 검정은 실험이나 관측 데이터를 분석하여, 관찰된 효과가 우연에 의한 것인지 아니면 실제로 유의미한 것인지 판단하는 데 사용됩니다. 유의성 검정은 두 가지 가설, 즉 귀무가설 (null hypothesis)과 대립가설 (alternative hypothesis)로 시작합니다.

** 귀무가설 (Null Hypothesis, H0H0​)

    귀무가설은 보통 "효과가 없다" 또는 "두 집단 간 차이가 없다"라는 진술을 포함합니다. 예를 들어, 신약의 효과가 기존 약과 차이가 없다는 가설이 귀무가설이 될 수 있습니다.

** 대립가설 (Alternative Hypothesis, H1H1​)

    대립가설은 "효과가 있다" 또는 "두 집단 간 차이가 있다"라는 진술을 포함합니다. 신약의 효과가 기존 약보다 우수하다는 가설이 대립가설이 될 수 있습니다.

** 유의수준 (Significance Level, αα)

    유의수준은 귀무가설을 기각할 기준이 되는 확률값입니다. 일반적으로 α=0.05α=0.05를 많이 사용하며, 이는 5%의 확률로 귀무가설이 참임에도 불구하고 기각하는 것을 허용합니다.

** 검정통계량 (Test Statistic)

    검정통계량은 관찰된 데이터로부터 계산되는 값으로, 귀무가설이 참일 때의 분포를 따릅니다. 검정통계량의 예로는 t-값, z-값, chi-제곱 값 등이 있습니다.

** p-값 (p-value)

    p-값은 관찰된 데이터가 귀무가설 하에서 얼마나 일어날 가능성이 있는지를 나타내는 확률입니다. p-값이 유의수준 αα보다 작으면 귀무가설을 기각하고, 대립가설을 채택합니다.

** 유의성 검정의 단계

    1. 가설 설정: 귀무가설과 대립가설을 설정합니다.
    2. 유의수준 설정: 일반적으로 α=0.05α=0.05로 설정합니다.
    3. 검정통계량 계산: 데이터를 기반으로 검정통계량을 계산합니다.
    4. p-값 계산: 검정통계량에 해당하는 p-값을 계산합니다.
    5. 결론 도출: p-값이 유의수준 αα보다 작으면 귀무가설을 기각합니다.


    - 3.1 A/B 검정 (A/B Testing)
 
        - A/B 검정은 두 가지 버전(A와 B)의 변형을 비교하여 어느 것이 더 나은 성과를 내는지 평가하는 실험 방법입니다. A/B 검정은 마케팅, 웹사이트 최적화, 사용자 경험 연구 등 다양한 분야에서 널리 사용됩니다. 이 방법은 과학적이고 통계적인 접근을 통해 어떤 변화가 성과에 영향을 미치는지 확인한다.

        **용어 정리**
        -처리 (treatement) : 어떤 대상에 주어지는 특별한 환경이나 조건
        -처리군(처리그룹)[treatement group] :특정 처리에 노출된 대상들의 집단
        -대조군(대조그룹)[control group] : 어떤처리도 하지않은 대상들의 집단
        -임의화(랜덤화)[randomization] : 처리를 적용할 대상을 임의로 결정하는과정
        -대상[subject] : 처리를 적용할 개체대상(유의어:피실험자)
        -검정통계량[test statistic] : 처리 효과를 측정하기 위한 지표

        3.1.1 대조군 (Control Group)이 필요한 이유

            - 대조군은 실험에서 비교 기준을 제공하는 그룹입니다. 실험군과 비교하여 변화나 처치의 효과를 평가하기 위해 사용됩니다. 대조군이 필요한 이유는 다음과 같습니다:

                1.비교 기준 제공:
                    대조군은 실험군과 비교할 수 있는 기준을 제공합니다. 이를 통해 실험군에서 관찰된 변화가 실제로 처치나 변화에 의한 것인지, 아니면 다른 요인에 의한 것인지를 판단할 수 있습니다.
                2. 변수 통제:
                    대조군을 사용함으로써 실험 조건 외의 다른 변수들이 실험 결과에 미치는 영향을 최소화할 수 있습니다. 이는 결과의 신뢰성을 높여줍니다.
                3. 효과 측정:
                    대조군과 실험군 간의 차이를 통해 특정 처치나 변화의 효과를 정량적으로 측정할 수 있습니다.

                예를 들어, 새로운 약물의 효과를 테스트할 때 대조군은 기존 약물 또는 플라시보를 받는 그룹이 됩니다. 이를 통해 새로운 약물이 실제로 더 나은 효과를 발휘하는지 비교할 수 있다.

        3.1.2 왜 A/B 테스트인가? (C/D 테스트가 아닌 이유)

            - A/B 테스트라는 용어는 단순히 두 가지 변형(A와 B)을 비교하는 것을 의미합니다. 이 용어가 널리 사용되는 이유는 다음과 같습니다:

                1.단순성:
                    A/B 테스트는 가장 기본적이고 단순한 형태의 비교 실험입니다. 두 가지 변형만을 비교하기 때문에 실험 설계와 분석이 간단합니다.
                2.명확한 결과:
                    두 가지 선택지(A와 B)만을 비교하기 때문에 결과 해석이 명확합니다. 이는 빠르고 확실한 결정을 내리는 데 유리합니다.
                3.역사적 이유:
                    A/B 테스트라는 용어는 오랜 기간 동안 마케팅, 웹사이트 최적화, 사용자 경험 연구 등 다양한 분야에서 사용되어 왔습니다. 이 용어는 이미 널리 이해되고 받아들여지고 있습니다.

        요약

            ** 대조군: 비교 기준을 제공하여 실험 결과의 신뢰성을 높입니다.
            ** A/B 테스트: 두 가지 변형을 비교하는 기본적인 방법으로, 단순성과 명확성을 제공합니다.
            ** 여러 변형 비교: A/B/n 테스트를 통해 세 개 이상의 변형을 비교할 수 있으며, 이는 더 많은 데이터를 수집하고 다양한 옵션을 평가하는 데 유용합니다.

    3.2 가설검정 (Hypothesis Testing)

        가설검정은 주어진 데이터에 기초하여 특정 주장(가설)을 검증하는 통계적 방법입니다. 이는 연구나 실험에서 관찰된 현상이 우연에 의한 것인지, 아니면 실제로 의미 있는 것인지를 판단하는 데 사용됩니다. 가설검정은 두 가지 기본 가설, 즉 귀무가설 (null hypothesis, H0H0​)과 대립가설 (alternative hypothesis, H1H1​)로 시작됩니다.

        1. 기본 개념
            귀무가설 (H0H0​):
                일반적으로 "효과가 없다"거나 "차이가 없다"는 가설을 말합니다.
                예를 들어, 새로운 치료법이 기존 치료법과 효과가 동일하다는 가설.

            대립가설 (H1H1​):
                "효과가 있다"거나 "차이가 있다"는 가설을 말합니다.
                예를 들어, 새로운 치료법이 기존 치료법보다 효과적이라는 가설.

        2. 유의수준 (Significance Level, αα)

            유의수준은 가설검정에서 귀무가설을 기각할 기준이 되는 확률값입니다.
            일반적으로 α=0.05α=0.05를 많이 사용하며, 이는 5%의 확률로 귀무가설을 기각하는 오류(제1종 오류)를 허용한다는 의미입니다.

        3. 검정통계량 (Test Statistic)

            검정통계량은 관찰된 데이터로부터 계산된 값으로, 귀무가설이 참일 때의 분포를 따릅니다.
            (예를 들어, t-값, z-값, chi-제곱 값 등이 있습니다.)

        4. p-값 (p-value)

        p-값은 관찰된 데이터가 귀무가설 하에서 발생할 확률입니다.
        p-값이 유의수준 αα보다 작으면, 귀무가설을 기각하고 대립가설을 채택합니다.

    **용어정리**
        -귀무가설(null hypothesis): 우연 때문이라는 가설(유의어 : 영가설)
        -대립가설(alternative hypothesis) : 귀무가설과의 대조(증명하고자하는 가설)
        -일원검정(one-way test): 한방향으로만 우연히 일어날 확률을 계산하는 가설검정
        -이원검정(two-way test): 양방향으로 우연히 일어날 확률을 계산하는 가설검정
    
    **가설검정의 절차**
        1. 가설 설정:
            귀무가설 (H0H0​)과 대립가설 (H1H1​)을 명확히 설정합니다.
        2. 유의수준 설정:
            일반적으로 α=0.05α=0.05를 설정합니다.
        3. 데이터 수집:
            검정을 수행하기 위해 필요한 데이터를 수집합니다.
        4. 검정통계량 계산:
            수집된 데이터를 바탕으로 검정통계량을 계산합니다.
        5. p-값 계산:
            검정통계량에 해당하는 p-값을 계산합니다.
        6. 결론 도출:
            p-값이 유의수준 αα보다 작으면 귀무가설을 기각하고 대립가설을 채택합니다.
    
    3.2.1 귀무가설
        - 귀무가설은 가설검정에서의 중요한 개념으로, 대립가설과 반대되는 가설을 나타냅니다. 주로 "효과가 없다" 또는 "차이가 없다"는 가설로 설정되며, 연구나 실험을 통해 주장하고자 하는 가설에 대립하는 명제입니다.

        귀무가설은 H0H0​로 표기되며, 일반적으로 연구자가 주장하고자 하는 가설인 대립가설과 반대되는 내용을 가지고 있습니다. 가설검정의 목적은 귀무가설의 참/거짓을 결정하는 것입니다. 만약 귀무가설이 기각된다면, 대립가설을 받아들일 수 있습니다.   
    
    3.2.2 대립가설 (Alternative Hypothesis)

        - 대립가설은 가설검정에서 검정하고자 하는 주장이나 이론을 나타냅니다. 보통은 연구자가 주장하고자 하는 가설로, 특정 변수 간에 관계나 차이가 있다는 것을 나타냅니다.
        예를 들어, 어떤 신약이 기존 약보다 더 효과적이라는 주장은 대립가설로 나타낼 수 있습니다. 이 대립가설은 통계적 검정을 통해 데이터를 통해 지지되거나 기각될 수 있습니다.

    3.2.3 일원가설검정 (One-Tailed Test)

        -일원가설검정은 가설검정의 유형 중 하나로, 대립가설이 양측(양쪽) 방향으로만 주장되는 경우를 나타냅니다. 즉, 대립가설은 변수 간에 방향성이 있는 차이를 주장하며, 이 차이가 양측으로 나타날 수 있습니다.
        일원가설검정에서는 귀무가설을 기각하는 기준이 특정 방향에만 존재합니다. 따라서 가설검정 결과가 양측으로 비대칭적인 경우에 사용됩니다.
        예를 들어, 어떤 신약이 기존 약보다 더 높은 효과를 보인다는 주장을 한다면, 대립가설은 "신약의 효과는 기존 약보다 크다"로 설정되고, 이는 일원가설검정으로 검증될 수 있습니다.

        **이원가설검정 (Two-Tailed Test)

        - 이원가설검정은 대립가설이 양측 방향으로 주장될 수 있는 경우에 사용됩니다. 즉, 대립가설은 변수 간에 차이가 있는지만을 주장하며, 이 차이가 양측으로 나타날 수 있습니다.
        이원가설검정에서는 귀무가설을 기각하는 기준이 양쪽 모두에 존재하며, 양측으로의 비대칭적인 결과에 사용됩니다.
        예를 들어, 어떤 신약이 기존 약과 다른 효과를 보인다는 주장을 한다면, 대립가설은 "신약의 효과는 기존 약과 다르다"로 설정되고, 이는 이원가설검정으로 검증될 수 있습니다.

    3.3 재표본추출(Resampling)

        - 재표본추출은 통계적 추론을 위해 주어진 데이터에서 반복적으로 표본을 추출하는 과정을 말합니다. 이를 통해 주어진 데이터셋으로부터 여러 개의 표본을 생성하고, 이를 활용하여 통계량이나 추정치의 분포를 조사하거나 가설검정을 수행할 수 있습니다.

        주요 재표본추출 방법으로는 부트스트래핑(bootstrapping)과 재표본추출(permuation)이 있습니다. 부트스트래핑은 복원추출 방식으로 표본을 추출하는 반면, 재표본추출은 복원추출 없이 주어진 데이터를 순열하여 새로운 표본을 생성합니다.

    **용어정리**
        -순열검정(permutation test) : 두개 이상의 표본을 함께결합하여관측값들을 무작위로 재표본으로 추출하는 과정을 말한다.[유의어:임의화검정, 임의순열검정, 정확검정]
        -재표본추출(resampling) : 관측 데이터로부터 반복해서 표본추출하는 과정.
        -복원/비복원(with or without replacement) : 표본을 추출할 때, 이미 한번 뽑은 데이터를 다음번 추출을 위해 다시 제자리에 돌려놓거나 다음 추출에서 제외하는 표본추출 방법.

            3.3.1 순열검정 (Permutation Test)

                - 순열검정은 통계적 가설검정의 한 방법으로, 표본 데이터를 재표본추출을 통해 생성된 표본들로 재표본추출된 통계량을 비교함으로써 가설을 검정합니다.
                순열검정은 가설검정을 위해 임의의 표본을 생성하는 과정을 거치기 때문에, 표본분포가 표본크기가 충분히 클 때 중심극한정리에 의해 정규분포를 따르지 않아도 되는 장점이 있습니다.

            3.3.2 웹 점착성(Web Stickiness)

                -웹 점착성은 웹사이트나 앱의 이용자가 그 서비스에 얼마나 오래 머무르는지를 나타내는 지표입니다. 이는 방문자가 웹사이트에서 얼마나 많은 시간을 보내는지, 페이지를 얼마나 자주 방문하는지, 사용자들이 얼마나 자주 상호 작용하는지 등을 포함합니다. 높은 웹 점착성은 사용자들이 서비스에 충성하고 활발하게 참여한다는 것을 나타내며, 기업이나 기관에게 중요한 지표 중 하나.

            3.3.3 전체 및 부트스트랩 순열검정 

                - 순열검정은 통계적 가설검정 방법 중 하나로, 관측된 데이터의 순서를 무작위로 바꾸어 가설을 검정하는 방법입니다. 이를 통해 표본의 크기가 작거나 분포에 대한 가정이 충족되지 않을 때도 유효한 검정을 수행할 수 있습니다.

                * 전체 순열검정 (Exact Permutation Test):
                    -모든 가능한 표본 순열을 고려하여 검정하는 방법입니다. 데이터셋이 작거나 가능한 표본 공간이 비교적 작을 때 사용됩니다. 가능한 모든 조합을 고려하기 때문에 정확한 p-값을 얻을 수 있지만, 계산 비용이 매우 높을 수 있습니다.

                * 부트스트랩 순열검정 (Bootstrap Permutation Test):
                    -부트스트랩 순열검정은 전체 순열검정의 계산 비용을 줄이기 위해 사용됩니다. 관측된 데이터로부터 복원추출을 통해 여러 재표본을 생성하고, 각 재표본에 대해 순열검정을 수행합니다. 이를 통해 근사적인 p-값을 얻을 수 있으며, 계산 비용이 낮아 전체 순열검정에 비해 효율적입니다.

    3.4 통계적 유의성과 p값

        -통계적 유의성 (Statistical Significance)

            통계적 유의성은 주어진 데이터에서 관찰된 차이나 관계가 우연에 의한 것이 아니라 실제로 의미 있는 것인지를 나타내는 지표입니다. 즉, 연구나 실험 결과가 우연에 의한 것이 아니라 진짜로 의미 있는지를 판단하는 것입니다.
            통계적 유의성은 주로 p-값과 유의수준을 통해 결정됩니다. 유의수준은 연구자가 미리 설정하는 임계값으로, 일반적으로 0.05 또는 0.01이 많이 사용됩니다. p-값이 유의수준보다 작으면, 우연히 나타날 확률이 매우 낮기 때문에 해당 결과가 통계적으로 유의하다고 판단됩니다.
        
        **용어정리**
            - p값[p-value] : 귀마가설을 구체화한 기회모델이 주어졌을떄 관측된 결과와 같이 특이하거나극단적인결과를 얻을 확률
            - 알파[alpha] : 실제결과가 통계적으로 의미있는것으로 간주되기 위해, 우연에 의한 결과가 능가해야하는 '비정상적인' 가능성의 임계확률
            - 제1종 오류[type 1 error] : 우연에 의한 효과를 실제효과라고 잘못 결론내리는 것
            - 제2종 오류 [type 2 error] : 실제효과를 우연에 의한 효과라고 잘못 결론 내리는 것

        3.4.1 p값
            - p-값 (p-value)

                p-값은 귀무가설이 참일 때, 관측된 데이터나 더 극단적인 데이터가 관측될 확률을 나타냅니다. 즉, p-값은 귀무가설을 기각하는데 필요한 증거의 강도를 나타내는 지표입니다. 작은 p-값은 귀무가설을 기각하는 강력한 증거를 제공하며, 이는 관찰된 데이터가 우연에 의한 것이 아니라는 것을 나타냅니다.
                일반적으로, p-값이 유의수준보다 작을수록 (예: 0.05), 해당 결과가 통계적으로 유의하다고 판단됩니다. 따라서 작은 p-값은 우연에 의한 것이 아니라는 증거를 제공하여 통계적으로 유의한 결과를 나타냅니다.

        3.4.2 유의수준 (Significance Level)

            -유의수준은 통계적 가설검정에서 사용되는 중요한 개념으로, 귀무가설이 참인데도 불구하고 해당 가설을 잘못 기각할 확률을 나타냅니다. 일반적으로 알파(α)로 표기되며, 가설을 검정하는 데 사용되는 임계값입니다.
            가장 흔히 사용되는 유의수준은 0.05와 0.01입니다. 유의수준이 0.05인 경우, 귀무가설이 참일 때 해당 가설을 잘못 기각할 확률이 5% 이하라는 것을 의미합니다.
        
        3.4.3 제1종 오류와 제2종 오류 (Type I and Type II Errors)

            - 제1종 오류 (Type I Error): 귀무가설이 참인데도 불구하고 해당 가설을 잘못 기각하는 오류입니다. 유의수준과 관련이 있으며, 유의수준을 낮추면 제1종 오류의 발생 확률이 줄어듭니다.
            - 제2종 오류 (Type II Error): 귀무가설이 거짓인데도 불구하고 해당 가설을 기각하지 않는 오류입니다. 제2종 오류의 확률은 표본 크기, 효과의 크기 등 여러 요소에 의해 영향을 받습니다.

        3.4.4 데이터 과학과 p값

            -데이터 과학에서의 p-값은 통계적 가설검정에서 주로 사용되며, 가설을 검정하고 결과를 해석하는 데 중요한 역할을 합니다. p-값은 귀무가설을 참으로 가정했을 때, 관찰된 데이터 또는 더 극단적인 데이터가 나타날 확률을 나타냅니다.
            데이터 과학에서는 p-값을 사용하여 가설을 검정하고, 결과를 통계적으로 유의한지를 판단합니다. 일반적으로, 유의수준과 p-값을 비교하여 해당 결과가 통계적으로 유의한지를 결정합니다. p-값이 유의수준보다 작을수록 해당 결과가 계적으로 유의하다고 판단됩니다.
            예를 들어, 어떤 실험에서의 p-값이 0.03이고 유의수준이 0.05라면, 해당 결과는 통계적으로 유의하다고 할 수 있습니다. 따라서 귀무가설을 기각하고, 대립가설을 받아들일 수 있습니다.
            요약하자면, 유의수준은 가설검정에서 사용되는 임계값으로, 제1종 오류의 발생 확률을 나타내며, p-값은 가설을 검정하고 결과를 해석하는 데 사용되는 지표로, 유의수준과 비교하여 해당 결과가 통계적으로 유의한지를 판단하는 데 중요한 역할을 합니다.

    3.5 t 검정

        - t-검정은 두 집단 간의 평균 차이를 비교하는 통계적 방법 중 하나입니다. 주로 두 집단의 평균이 동일한지 여부를 확인하기 위해 사용됩니다.
        t-검정은 표본 평균 간의 차이를 표준 오차로 조정하여 검정 통계량(t-값)을 계산합니다. 이 t-값을 통해 두 집단 간의 평균 차이가 우연에 의한 것인지를 판단하며, 이를 통해 귀무가설을 기각하거나 채택합니다.

    일반적으로 t-검정은 다음과 같은 단계로 진행됩니다:

        1. 가설 설정: 귀무가설과 대립가설을 설정합니다. 귀무가설은 두 집단의 평균이 같다는 것을 나타내며, 대립가설은 두 집단의 평균이 다르다는 것을 나타냅니다.
        2. 표본 추출: 두 집단에서 표본을 추출합니다.
        3. 평균 및 표준 편차 계산: 각 집단의 표본 평균과 표준 편차를 계산합니다.
        4. t-값 계산: 두 집단 간의 평균 차이를 표준 오차로 조정하여 t-값을 계산합니다.
        5. 유의수준과 비교: 계산된 t-값을 유의수준과 비교하여 귀무가설을 기각하거나 채택합니다.
        6. 결과 해석: 귀무가설을 기각하면, 두 집단의 평균이 다르다고 결론을 내릴 수 있습니다.

        t-검정은 표본의 크기가 작거나 모집단의 표준 편차를 모를 때 주로 사용됩니다. 특히, 두 집단의 표본 크기가 비슷하고 정규분포를 따를 때 효과적으로 사용됩니다. t-검정은 학계와 실무에서 폭넓게 사용되며, 두 집단 간의 평균 차이를 비교하는 많은 분야에서 활용됩니다.

    3.6 다중검정 (Multiple Testing)

        다중검정은 여러 개의 가설을 동시에 검정하는 과정을 말합니다. 하나의 실험에서 여러 개의 가설을 검정하게 되면, 가설검정 과정에서 제1종 오류(가설이 참일 때 거짓으로 기각되는 오류)의 위험이 증가할 수 있습니다. 다중검정을 통해 이러한 위험을 줄이고 신뢰할 수 있는 검정 결과를 얻을 수 있습니다.

        다중검정에는 여러 가지 방법이 있으며, 대표적으로는 Bonferroni 수정, Holm-Bonferroni 방법, False Discovery Rate (FDR) 등이 있습니다. 이러한 방법들은 제1종 오류의 위험을 조절하거나 p-값을 조정하여 다중검정을 수행합니다.

        **용어 정리**
            -제1종 오류[type 1 error] : 어떤 효과가 통계적으로 유의미하다고 잘못되 결론을 내린다.
            -거짓발견비율[FDR : false discovery rate] : 다중검저에서 1종오류가 발생하는 비율
            -알파 인플레이션[alpha inflation] : 1종오류를 만들 확률인 알파가 더 많은 테스트를 수행할수록 증가하는 다중검정현상.
            -p값 조정[adjustment of p-value] : 동일한 데이터에 대해 다중검정을 수행하는 경우에 필요하다.
            -과대적립(오버피팅)[overfitting] : 잡음까지 피팅

        - 투기의 HSD(Honestly Significant Difference)는 다중비교에서 사용되는 통계적 기법 중 하나입니다. 이 방법은 ANOVA(분산분석)를 통해 그룹 간 평균의 차이가 통계적으로 유의한 경우, 어떤 그룹들이 서로 유의하게 다른지를 식별하는 데 사용됩니다.

        투기의 HSD는 각 그룹의 평균값들 사이의 최소 유의한 차이를 계산하여 비교합니다.  이 값은 각 그룹의 평균 차이가 두 개 이상의 그룹에서 특정한 기준값 이상인 경우에만 유의하다고 판단됩니다. 따라서, 그룹 간의 모든 가능한 조합에 대해 최소한의 유의한 차이를 계산하여 어떤 그룹들이 통계적으로 다른지를 판별하는 데 도움이 됩니다.

        투기의 HSD는 다중비교에 많이 사용되는 방법 중 하나로, 특히 ANOVA로 차이가 있는 그룹들을 식별하는 경우 유용합니다. 이 방법을 사용하면 다중비교에서 제1종 오류의 위험을 제어하면서 각 그룹 간의 평균 차이를 식별할 수 있습니다.

    3.7 자유도 (Degrees of Freedom)

        자유도는 통계 모형에서 모수를 추정할 때 사용되는 독립적인 정보의 수를 나타냅니다. 자유도는 모집단의 크기와 샘플의 크기 간의 관계에 따라 결정됩니다. 보통, 자유도는 샘플의 크기에서 추정된 모수의 수를 뺀 값입니다.

        예를 들어, t-검정에서는 자유도가 샘플의 크기에서 1을 뺀 값이 됩니다. 자유도가 높을수록 데이터에서 추정된 통계량의 분포가 모집단의 분포에 더 가깝습니다.

        **용어 정리**
        -표본크기 n : 해당 데이터에서 관측값의 개수(행 혹은 기록값의 개수와 같은 의미)
        - d.f.[degrees of freedom] : 자유도

    3.8 분산분석 (Analysis of Variance, ANOVA)

        분산분석은 세 개 이상의 집단 간의 평균 차이를 비교하는 통계적 방법입니다. ANOVA는 각 집단의 평균값이 동일한 모집단에서 추출된 것인지를 검정합니다.

        ANOVA는 전체 변동을 그룹 간 변동과 그룹 내 변동으로 분해하여 F-통계량을 계산하고 이를 통해 가설을 검정합니다. 만약 그룹 간 변동이 크다면 (즉, 그룹 간 평균 차이가 크다면), F-통계량이 유의하게 됩니다.

        분산분석은 일반적으로 하나의 요인을 가지고 있는 일원배치 분산분석과 두 개 이상의 요인을 가지고 있는 이원배치 분산분석으로 나뉩니다. 이를 통해 여러 집단 간의 평균 차이를 비교하고 효과적으로 검정할 수 있습니다.

        **용어 정리**
        *쌍별 비교[pairwise comparison] : 여러 그룹 중 두 그룹간의 가설검정
        *총괄 검정[omnibus test] : 여러 그룹 평균들의 전체 분산에 관한 단일 가설검정
        *분산분해[decomposition of variance] : 구성 요소 분리, 예를 들면 전체평균, 처리 평균, 잔차 오차로 부터 개별 값들에 대한 기여를 뜻함.
        *F통계량[F-statistic]: 그룹 평균간의 차이가 랜덤모델에서 예상되는 것에서 벗어나는 정도를 측정하는 표준화된 통계량
        *SS[sum of squares] : 어떤 평균으로부터의 편차들의 제곱합.

        3.8.1 F통계량

            -F 통계량은 분산 분석(ANOVA)에서 사용되는 통계량입니다. 주로 두 개 이상의 그룹 간의 평균 차이를 비교하는 데 사용됩니다.

            F 통계량은 그룹 간의 평균 제곱 (Mean Square Between, MSB)을 그룹 내의 평균 제곱 (Mean Square Within, MSW)으로 나눈 값입니다. 이는 그룹 간의 변동과 그룹 내의 변동을 비교하여 그룹 간의 차이가 우연에 의한 것인지를 판단하는 데 사용됩니다.

            * 그룹 간의 평균 제곱 (MSB): 그룹 간의 변동을 자유도로 나눈 값입니다. 즉, 그룹 간의 차이가 얼마나 큰지를 측정합니다.
            * 그룹 내의 평균 제곱 (MSW): 그룹 내의 변동을 자유도로 나눈 값입니다. 즉, 그룹 내의 차이가 얼마나 큰지를 측정합니다.

            F 통계량은 그룹 간의 평균 차이가 우연에 의한 것인지를 판단하는 데 사용됩니다. F 값이 클수록 그룹 간의 차이가 통계적으로 유의하다는 것을 나타냅니다. 따라서, F 통계량을 사용하여 ANOVA에서 귀무가설을 검정하고, 그룹 간의 평균 차이를 평가할 수 있습니다.

        3.8.2 이원 분산분석

            - 이원 분산 분석(ANOVA)은 두 개 이상의 독립 변수(요인)가 종속 변수에 미치는 영향을 평가하는 통계적 기법입니다. 이 분석은 실험 설계에서 두 가지 이상의 요인이 종속 변수에 미치는 영향을 동시에 평가하는 데 사용됩니다.

            예를 들어, 약의 효과를 평가하는 실험에서 약물의 종류와 운동량이 두 개의 요인으로 작용할 수 있습니다. 이원 분산 분석을 사용하여 약물의 종류와 운동량이 혈압에 미치는 영향을 동시에 평가할 수 있습니다.

        이원 분산 분석은 다음과 같은 가정을 기반으로 합니다:

            1. 독립 변수들은 서로 독립적이어야 합니다.
            2. 종속 변수는 정규 분포를 따라야 합니다.
            3. 그룹 간의 오차는 등분산성을 가져야 합니다.

        이원 분산 분석은 다음과 같은 세 가지 주요 요소를 평가합니다:

            1. 각 요인의 주효과(Main Effects): 각 요인이 종속 변수에 미치는 영향을 평가합니다.
            2. 상호 작용 효과(Interaction Effects): 두 요인 간의 상호 작용이 종속 변수에 미치는 영향을 평가합니다.
            3. 오차(Residuals): 모형에 포함되지 않은 다른 요인이나 랜덤한 요인으로 인한 오차를 평가합니다.

        이원 분산 분석은 실험 설계에 따라 두 요인이 종속 변수에 미치는 영향을 정확하게 이해하고 해석하는 데 도움이 됩니다.

    3.9 카이제곱검정

        -카이제곱 검정(Chi-square test)은 범주형 자료의 독립성을 검정하는 통계적 방법입니다. 주로 두 개 이상의 범주형 변수 간의 관계가 있는지를 확인하는 데 사용됩니다.

        주요 사용 사례는 다음과 같다:

            1. 카이제곱 독립성 검정 (Chi-square Test for Independence): 두 개 이상의 범주형 변수 간의 독립성을 검정합니다. 예를 들어, 성별과 선호하는 음료수 간의 관계를 조사할 때 사용될 수 있습니다. 이 검정은 두 변수 간의 관계가 우연에 의한 것인지를 확인합니다.
            2. 카이제곱 적합도 검정 (Chi-square Goodness of Fit Test): 하나의 범주형 변수가 기대되는 분포와 일치하는지를 확인합니다. 예를 들어, 주사위를 던져서 나오는 눈의 분포가 균일한지를 확인할 때 사용될 수 있습니다.

        카이제곱 검정은 다음과 같은 가정에 기초한다:

            1. 한변수가 범주형이어야 합니다.
            2. 각 셀(카운트)의 기대도수가 충분히 큰 경우(보통 5 이상)에 적용됩니다.
            3. 셀 간의 관측된 빈도가 독립적이어야 합니다.

        카이제곱 검정은 관찰된 빈도와 예상되는 빈도 간의 차이를 측정하여 검정 통계량을 계산합니다. 이 검정 통계량은 자유도를 가진 카이제곱 분포를 따르며, 이를 통해 유의수준에서의 임계값을 비교하여 귀무가설을 검정합니다. 유의수준에서의 임계값보다 큰 검정 통계량은 귀무가설을 기각하고, 변수 간의 관계가 통계적으로 유의미하다는 것을 의미합니다.

        **용어정리**
            -카이제곱통계량[chi-square statistic] : 기대값으로부터 어떤 관찰값까지의 거리를 나타내는 측정치
            -기댓값[expectation(expected)] : 어떤가정(보통 귀무가설)으로부터 데이터가 발생할때, 그에 대해 기대하는 정도
            -d.f[degree of freedom] : 자유도

        3.9.1 카이제곱검정:재표본추출방법

            - 카이제곱 검정에서 재표본 추출은 통계적 추론을 수행하는 데 사용되는 일반적인 방법 중 하나입니다. 이 방법은 표본의 크기를 증가시키거나 표본을 다시 추출함으로써 통계적 특성을 추정하고 가설을 검정하는 데 사용됩니다.

            재표본추출 방법을 사용하여 카이제곱 검정을 수행하는 과정은 다음과 같습니다:

                1. 원래 표본 추출: 먼저 원래 데이터에서 표본을 추출합니다.
                2. 재표본 추출: 원래 표본에서 복원 추출 또는 비복원 추출을 사용하여 여러 번의 재표본을 생성합니다. 이 과정에서는 표본의 크기를 유지하거나 증가시키는 것이 일반적입니다.
                3. 카이제곱 통계량 계산: 각 재표본에 대해 카이제곱 통계량을 계산합니다. 이를 통해 각 표본에서 관측된 데이터와 예상되는 기대도수 간의 차이를 측정합니다.
                4. 재표본 분포 생성: 재표본에서 얻은 카이제곱 통계량을 사용하여 재표본 분포를 생성합니다. 이를 통해 표본 크기의 영향을 고려하여 카이제곱 통계량의 분포를 추정할 수 있습니다.
                5. 유의확률 계산: 재표본 분포를 사용하여 유의확률(재표본 p-값)을 계산합니다. 이는 귀무가설을 기각하는 데 필요한 통계적 증거를 제공합니다.
                6. 결과 해석: 재표본 p-값을 유의수준과 비교하여 귀무가설을 기각하거나 채택합니다. 만약 재표본 p-값이 유의수준보다 작다면, 귀무가설을 기각하고 대립가설을 채택할 수 있습니다.

            재표본추출 방법은 표본 크기가 작거나 표본이 비정규 분포를 따르는 경우에도 유용하게 사용될 수 있습니다. 또한 이 방법은 표본의 무작위성을 보장하고, 다양한 표본 크기에 대해 일관된 결과를 얻을 수 있는 장점이 있습니다.

        3.9.2 카이제곱검정: 통계적이론

            -카이제곱 검정은 관찰된 빈도와 기대 빈도 사이의 차이를 사용하여 범주형 변수 간의 관계를 평가하는 데 사용되는 통계적 방법입니다. 이론적으로 카이제곱 검정은 다음과 같은 단계로 진행됩니다:

                1. 귀무가설과 대립가설 설정: 카이제곱 검정의 첫 번째 단계는 귀무가설(H0)과 대립가설(H1)을 설정하는 것입니다. 귀무가설은 두 범주형 변수 사이에 관계가 없다는 것을 의미하며, 대립가설은 두 변수 사이에 관계가 있다는 것을 의미합니다.
                2. 예상 빈도 계산: 다음으로, 각 범주의 관찰된 빈도와 전체 데이터에서 예상되는 비율을 사용하여 예상 빈도를 계산합니다. 예상 빈도는 범주별로 관찰된 빈도가 어떤 값일 것으로 예상되는지를 나타냅니다.
                3. 카이제곱 통계량 계산: 실제 관찰된 빈도와 예상 빈도 사이의 차이를 계산하여 카이제곱 통계량을 얻습니다. 이 값은 두 변수 간의 관계의 적합도를 측정하는 데 사용됩니다.
                4. 유의확률 계산: 카이제곱 통계량을 자유도와 함께 사용하여 유의확률을 계산합니다. 이는 귀무가설을 기각하는 데 필요한 통계적 증거를 제공합니다.
                5. 결과 해석: 유의확률을 유의수준과 비교하여 귀무가설을 기각하거나 채택합니다. 만약 유의확률이 유의수준보다 작다면, 귀무가설을 기각하고 대립가설을 채택합니다. 따라서, 두 변수 간에는 통계적으로 유의한 관계가 있다고 결론을 내릴 수 있습니다.

            이러한 통계적 이론은 카이제곱 검정을 사용하여 범주형 변수 간의 관계를 평가하는 데 사용됩니다. 이를 통해 데이터의 분포가 기대되는 분포와 얼마나 일치하는지를 확인하고, 변수 간의 관계를 이해할 수 있습니다.

        3.9.3 피셔의 정확검정
            -피셔의 정확검정(Fisher's Exact Test)은 카이제곱 검정과 유사한 목적을 가지고 있지만, 표본 크기가 작거나 데이터가 특별한 구조를 가질 때 사용되는 검정 방법입니다. 주로 2x2 크로스테이블(교차표)을 분석하는 데 사용됩니다.

            피셔의 정확검정은 범주형 자료를 처리할 때 발생할 수 있는 한계점을 극복하기 위해 개발되었습니다. 이 검정은 직접적으로 관찰된 데이터의 확률과 같은 모집단 하에서 해당 결과가 나타날 확률을 계산하여 가설을 평가합니다.

            피셔의 정확검정은 다음과 같은 상황에서 주로 사용됩니다:

                1.표본 크기가 작을 때: 표본 크기가 작거나 예상되는 빈도가 작을 때 카이제곱 검정의 근사치가 부정확할 수 있습니다. 이러한 경우에 피셔의 정확검정을 사용하여 정확한 p-값을 계산할 수 있습니다.
                2.표본이 비균형적일 때: 카이제곱 검정은 표본이 균형적이거나 각 셀에 충분한 기대 빈도가 있는 경우에 적합합니다. 그러나 표본이 불균형적이거나 기대 빈도가 작은 경우에는 피셔의 정확검정이 더 적합할 수 있습니다.
                3.정확한 p-값이 필요한 경우: 피셔의 정확검정은 정확한 p-값을 계산하기 때문에 결과의 신뢰도를 높일 수 있습니다. 피셔의 정확검정은 카이제곱 검정보다 계산량이 많고 계산이 더 복잡할 수 있습니다. 그러나 표본이 작거나 비균형적인 경우에는 더 신뢰할 수 있는 결과를 제공할 수 있습니다.

        한문장요약 ! : 피셔의 정확검정은 작거나 불균형한 데이터에서 카이제곱 검정의 한계를 극복하여 정확한 p-값을 계산하는 검정 방법이다.

        3.9.4 데이터 과학과의 관련성

            - 데이터 과학은 데이터를 수집, 분석, 해석하여 통찰력을 얻고 의사 결정을 내리는 과정을 다루는 학문 분야입니다. 데이터 과학은 통계학, 컴퓨터 과학, 정보 기술 등 다양한 분야의 원리와 기술을 활용하여 데이터를 이해하고 활용합니다.

            데이터 과학은 데이터를 분석하여 정보와 지식을 추출하는 과정을 포함하며, 이를 통해 비즈니스 의사 결정이나 문제 해결에 도움이 되는 인사이트를 제공합니다. 이를 통해 기업은 시장 동향을 예측하고 비즈니스 전략을 개발할 수 있으며, 과학 연구에서는 새로운 발견을 이루고 문제를 해결하는 데 활용됩니다.

            데이터 과학은 데이터를 효과적으로 수집, 저장, 처리, 분석하는 기술과 방법론을 개발하고 적용하는 것을 목표로 합니다. 따라서 데이터 과학은 통계학과 밀접한 관련이 있으며, 데이터 분석을 위한 다양한 통계적 기법을 사용합니다. 또한 데이터 과학은 컴퓨터 과학과도 관련이 있으며, 데이터 처리 및 분석을 위한 알고리즘과 소프트웨어 개발에 대한 지식이 필요합니다.

            데이터 과학은 현대 사회에서 매우 중요한 역할을 하고 있으며, 기업, 학계, 정부 등 다양한 분야에서 활발하게 연구 및 적용되고 있습니다. 데이터 과학의 발전은 새로운 기술과 서비스의 발전을 촉진하고, 사회와 경제의 변화를 이끌어내는 중요한 요소 중 하나입니다.

        한문장 요약 ! "
            - 데이터 과학은 데이터를 수집, 분석, 해석하여 통찰력을 얻고 의사 결정을 내리는 학문 분야로, 통계학과 컴퓨터 과학의 원리와 기술을 활용하여 데이터를 이해하고 활용합니다. 데이터 과학은 비즈니스 의사 결정이나 과학 연구에서 인사이트를 추출하여 문제 해결과 비즈니스 전략 개발에 기여합니다.

    3.10 멀티암드 밴딧 알고리즘
        - 멀티암드 밴딧(Multi-Armed Bandit) 알고리즘은 선택지(슬롯 머신) 중에서 최적의 선택지를 찾기 위해 사용되는 기계 학습 문제입니다. 이 알고리즘은 탐색(exploration)과 활용(exploitation) 사이의 균형을 유지하면서 각 선택지의 기대 보상을 추정하고, 최적의 선택지를 선택하는 것을 목표로 합니다.

        멀티암드 밴딧 문제는 다음과 같은 상황을 가정합니다:
            여러 개의 슬롯 머신(팔)이 있고, 각 팔은 서로 다른 확률 분포를 가진 보상을 제공합니다.
            목표는 제한된 시도 횟수 내에서 최대의 총 보상을 얻기 위해 어느 팔을 당겨야 할지를 결정하는 것입니다.

        멀티암드 밴딧 알고리즘의 주요 방법은 다음과 같습니다:
            ε-탐욕적(ε-greedy) 알고리즘:
                일정 확률 ε로 임의의 팔을 탐색하고, 나머지 확률(1-ε)로 현재까지의 보상이 가장 높은 팔을 선택합니다.
            UCB(Upper Confidence Bound) 알고리즘:
                각 팔의 평균 보상과 그 팔의 탐색 횟수를 기반으로 신뢰 구간을 계산하여, 신뢰 구간의 상한이 가장 높은 팔을 선택합니다.
            톰슨 샘플링(Thompson Sampling):
                각 팔의 보상 분포를 베이지안 방식으로 모델링하여, 각 시도마다 확률적으로 팔을 선택합니다.

        멀티암드 밴딧 알고리즘은 광고 배치, 임상 시험, 추천 시스템 등 다양한 분야에서 활용됩니다. 이 알고리즘은 탐색과 활용의 균형을 유지하여 최적의 선택지를 효율적으로 찾을 수 있도록 돕습니다.

        **용어정리** 
            - 멀티암드 배딧(MAB)multi-armed bandit: 고객이 선택 할수 있는 손잡이가 여러 개인가상의 슬롯머신을 말하며, 각 손잡이는 각기 다른 수익을 가져다 준다. 다중처리실험에 대한 비유라고 생각할 수 있다.
            -손잡이[arm] - 실험에서 어떤 하나의 처리를 말한다(예를 들면 '웹 테스트에서 헤드라인 A')
            -상급 수익 - 슬롯머신으로 딴 상금에 대한 실험적 비유유(예를 들면 '고객들의 링크 클릭 수)

    3.11 검정력과 표본크기
        1. 검정력 (Power):
            * 정의: 검정력은 실제로 대립가설이 참일 때, 귀무가설을 기각하는 확률입니다. 즉, 실제 효과가 존재할 때 이를 검출해내는 능력을 나타냅니다.
            * 계산: 검정력은 1 - β (제2종 오류 확률)로 계산됩니다. 제2종 오류는 실제로 대립가설이 참임에도 불구하고 귀무가설을 기각하지 못하는 오류입니다.
            * 의의: 검정력이 높을수록 실제 효과를 놓치지 않고 발견할 가능성이 커집니다. 일반적으로 검정력은 0.8 (80%) 이상이 권장됩니다.

        2. 표본 크기 (Sample Size):
            *정의: 표본 크기는 연구에서 데이터를 수집한 개체 수를 의미합니다.
            *영향: 표본 크기가 크면, 통계적 검정의 검정력이 증가합니다. 이는 표본이 클수록 모집단의 특성을 더 정확하게 반영할 수 있기 때문입니다.
            *계산: 표본 크기를 결정할 때 고려해야 할 요소는 검정력, 유의수준(α), 효과 크기, 모집단의 변동성 등입니다.

        검정력과 표본 크기의 관계:

            1. 표본 크기 증가: 표본 크기를 늘리면 표본의 변동성이 줄어들고, 효과를 더 명확하게 감지할 수 있어 검정력이 증가합니다.
            2. 검정력 증가: 높은 검정력을 얻기 위해 필요한 표본 크기를 계산할 수 있습니다. 이는 연구 설계 단계에서 중요합니다.

        **용어정리**
            -효과크기[effect size] - '클릭률의 20%향상'과 같이 통계검정을통해 판단할 수 있는 효과의 최소크기
            -검정력[power] - 주어진 표본크기로 주어진 효과크기를 알아낼 확률
            -유의수준[significance lever] - 검증시 사용할 통계 유의수준

        3.11.1 표본크기

            -표본 크기는 통계적 분석에서 데이터 수집의 단위 수를 나타내며, 연구의 정확성, 신뢰성, 검정력 등에 중요한 영향을 미칩니다. 적절한 표본 크기를 설정하는 것은 연구 결과의 유의성과 신뢰성을 확보하기 위해 필수적입니다. 
            다음은 표본 크기와 관련된 주요 개념들입니다:

            * 표본 크기의 중요성
                1. 정확성 증가:
                    표본 크기가 크면 모집단의 특성을 더 잘 반영할 수 있습니다.
                    평균, 비율 등의 추정치의 오차가 줄어듭니다.
                2. 검정력 향상:
                    검정력은 실제 효과가 있을 때 이를 발견할 확률입니다.
                    표본 크기가 클수록 검정력이 높아져서 실제 효과를 더 잘 검출할 수 있습니다.
                3. 유의미한 결과 도출:
                    충분한 표본 크기를 통해 연구 결과의 통계적 유의성을 확보할 수 있습니다.
                    유의수준(α)과 검정력(1-β)에 기반한 표본 크기 설정이 필요합니다.

            * 표본 크기 결정 요소
                표본 크기를 결정할 때 고려해야 할 주요 요소들은 다음과 같습니다:

                1.유의수준 (α):흔히 0.05로 설정되며, 귀무가설이 참일 때 이를 기각할 확률입니다.
                2.검정력 (1-β):일반적으로 0.8 또는 0.9로 설정되며, 실제 효과가 있을 때 이를 검출할 확률입니다.
                3.효과 크기 (Effect Size):연구에서 관찰하고자 하는 최소한의 효과 크기입니다.
                표본 크기는 검출하고자 하는 효과 크기에 비례합니다. 작은 효과를 검출하려면 더 큰 표본이 필요합니다.
                4.변동성 (Variance):모집단의 변동성(또는 표준편차)이 클수록 더 큰 표본이 필요합니다.

            *표본 크기 계산 방법
                표본 크기는 다양한 방법으로 계산될 수 있습니다. 여기 몇 가지 예시를 들어보겠습니다:
                    1.단순 비교:
                    두 집단 간의 평균 차이를 비교할 때 사용하는 방법입니다.
                    예: 두 약물의 효과를 비교하는 경우.
                    2.비율 비교:
                    두 집단 간의 비율 차이를 비교할 때 사용하는 방법입니다.
                    예: 두 그룹의 치료 성공률을 비교하는 경우.
        [[요약]]
            표본 크기는 연구의 정확성과 검정력을 보장하는 중요한 요소입니다.
            유의수준, 검정력, 효과 크기, 변동성을 고려하여 표본 크기를 결정해야 합니다.
            충분한 표본 크기는 신뢰할 수 있는 연구 결과를 얻는 데 필수적입니다.


CHAP 3장 전체 요약 - 

        1.머신러닝: 데이터에서 패턴을 학습하여 예측이나 의사 결정을 수행하는 인공 지능 분야입니다.
        2.지도학습: 입력 데이터와 그에 상응하는 정답(레이블)을 사용하여 모델을 학습시키는 머신러닝 방법입니다.
        3.2차원, 3차원, 4차원: 데이터의 특성을 나타내는 공간의 수를 의미하며, 2차원은 평면, 3차원은 공간, 4차원은 시간 등 추가 차원을 나타냅니다.
        4.유의성 검정: 데이터로부터 얻은 결과가 우연에 의한 것인지를 확인하는 통계적 방법입니다.
        5.A-B 검정: 두 가지 변형(A와 B)을 비교하여 어떤 변형이 더 우수한지를 판별하는 실험적 방법입니다.
        6.대조군: 실험에서 다른 처리를 받지 않는 기준 그룹으로, 결과 비교를 위해 사용됩니다.
        7.가설 검정: 주어진 데이터를 사용하여 특정 가설에 대한 적합성을 통계적으로 검정하는 과정입니다.
        8.대립가설, 일원/이원 가설검정: 실험에서 주장하려는 가설로, 일원은 한 변수, 이원은 두 변수를 검정합니다.
        9.귀무가설: 실험에서 특정 효과가 없다고 가정하는 가설입니다.
        10.재표본추출: 데이터로부터 복제된 여러 표본을 생성하여 통계적 추론을 수행하는 방법입니다.
        11.순열검정: 데이터를 무작위로 재배열하여 가설을 검정하는 비모수적 방법입니다.
        12.웹 점착성, 전체 및 부트스트랩 순열검정: 웹사이트의 사용자 경험을 측정하기 위한 방법으로, 부트스트랩 순열검정은 샘플링 점착성을 비교하는 통계적 방법입니다.
        13.통계적 유의성과 p값: 통계적 유의성은 특정 결과가 우연에 의한 것인지를 나타내며, p값은 귀무가설이 참일 때 해당 결과를 관찰할 확률을 나타냅니다.
        14.유의수준, 제1종과 제2종 오류: 유의수준은 귀무가설을 기각할 기준, 제1종 오류는 실제로는 참인데 거짓으로 기각하는 오류, 제2종 오류는 실제로 거짓인데 참으로 유지하는 오류를 의미합니다.
        15.t 검정: 두 집단의 평균을 비교하는 통계적 방법입니다.
        16.다중검정, 자유도, 분산분석: 다중검정은 여러 개의 가설을 동시에 검정하는 과정, 자유도는 독립적인 정보의 수, 분산분석은 세 개 이상의 집단 간의 평균 차이를 비교하는 통계적 방법입니다.
        17.투키의 HSD: 집단 간 평균의 유의미한 차이를 찾기 위한 사후 검정 방법입니다.
        18.F통계량: 분산분석에서 그룹 간 변동과 그룹 내 변동의 비율을 나타내는 통계량입니다.
        19.이원 분산분석: 두 개의 요인을 동시에 고려하여 그룹 간 평균 차이를 비교하는 통계적 방법입니다.
        20.카이제곱 검정: 관찰된 빈도와 기대 빈도 사이의 차이를 사용하여 범주형 변수 간의 관계를 평가하는 통계적 방법입니다.
        21.피셔의 정확검정: 작은 표본 크기나 기대 빈도가 작을 때 정확한 p-값을 계산하기 위해 사용하는 비모수적 검정 방법입니다.
        22.검정력과 표본 크기: 검정력은 실제 효과를 검출할 확률이고, 표본 크기는 데이터 수집 단위의 수로, 둘은 가설 검정의 유효성과 정확성에 큰 영향을 미칩니다.
        23.표본크기 : 표본 크기는 연구의 정확성과 검정력을 보장하는 중요한 요소입니다.


--------------------------------------------------------------------------------------
CHAP4 회귀와 예측 BEGIN

강사자님 말-- 책에 없는 내용 --
지도 학습 - 타겟이 있는 학습 네임이 붙어 있는학습
비지도 학습 - 타겟이 없고 데이터 알고리즘에 인해서 그룹화를 하는것 

    * 지도학습 (Supervised Learning):
        지도학습은 입력과 해당하는 출력 사이의 관계를 학습하는 기계학습의 분야입니다.
        모델은 레이블된 데이터(입력과 해당하는 출력 쌍)를 사용하여 학습됩니다.
        예시:
            분류(Classification): 이메일이 스팸인지 아닌지를 예측하는 것과 같이 미리 정의된 클래스 중 하나로 데이터 포인트를 분류합니다.
            회귀(Regression): 주택 가격이나 주식 가격과 같은 연속적인 값에 대한 예측을 수행합니다.

    * 비지도학습 (Unsupervised Learning):
        비지도학습은 레이블이 없는 데이터에서 패턴이나 구조를 발견하는 기계학습의 분야입니다.
        모델은 레이블이 없는 데이터만을 사용하여 학습됩니다.
        예시:
            군집화(Clustering): 비슷한 특성을 갖는 데이터 포인트들을 그룹으로 묶습니다.
            차원 축소(Dimensionality Reduction): 데이터의 특성을 보존하면서 고차원 데이터를 저차원으로 압축합니다.

    요약하면, 지도학습은 입력과 출력 간의 관계를 학습하고 예측하는 반면, 비지도학습은 데이터의 내재된 구조나 패턴을 발견하거나 요약하는 데 중점을 둔다.

    4.1 단순선형회귀
        - 단순선형회귀(Simple Linear Regression)는 하나의 독립 변수와 하나의 종속 변수 간의 선형 관계를 모델링하는 통계적 기법입니다. 이 모델은 독립 변수(x)와 종속 변수(y) 간의 선형 관계를 설명하는 직선을 생성합니다. 주로 독립 변수가 종속 변수에 미치는 영향을 분석하고 예측하는 데 사용됩니다. 회귀 분석에서 가장 간단한 형태로, 직선의 방정식을 통해 독립 변수와 종속 변수 간의 관계를 모델링합니다.

    **용어정리**
        -응답변수 (Response Variable 또는 Dependent Variable):분석하고자 하는 현상에 대한 관심 대상이며, 종속 변수로도 불립니다. 주로 예측하거나 설명하려는 변수입니다.
        -독립변수 (Predictor Variable 또는 Independent Variable):종속 변수에 영향을 주는 변수로, 설명 변수로도 불립니다. 예측에 사용되는 변수입니다.
        -레코드 (Record 또는 Observation):데이터 집합에서 한 행을 나타내며, 하나의 관측치를 포함합니다. 하나의 레코드는 독립 변수와 종속 변수의 값으로 구성됩니다.
        -절편 (Intercept):회귀선이 y축과 만나는 점을 나타내는 값으로, 독립 변수가 0일 때의 종속 변수의 예측값을 의미합니다.
        -회귀계수 (Regression Coefficient):독립 변수와 종속 변수 간의 관계를 설명하는 기울기를 나타내는 값으로, 회귀선의 기울기를 의미합니다.
        -적합값 (Fitted Value):회귀 모델을 사용하여 예측된 종속 변수의 값으로, 모델이 주어진 독립 변수에 대해 예측하는 값입니다.
        -잔차 (Residual):실제 관측값과 회귀 모델로 예측한 값 사이의 차이를 의미합니다. 잔차는 회귀 모델의 적합도를 평가하는 데 사용됩니다.
        -최소제곱 (Least Squares):회귀선과 실제 관측값 사이의 잔차 제곱의 합을 최소화하여 회귀 모델을 적합시키는 방법을 의미합니다. 이 방법은 회귀 모델의 파라미터를 추정하는 데 사용됩니다.

    4.1.1 회귀식
        회귀식은 회귀 분석을 통해 독립 변수와 종속 변수 간의 관계를 나타내는 수학적 모델을 의미합니다. 일반적으로 회귀식은 다음과 같은 형태를 가집니다:

        "y=β0​+β1​x"
        
        여기서:
        yy는 종속 변수(응답 변수)를 나타냅니다.
        xx는 독립 변수(설명 변수)를 나타냅니다.
        β0β0​는 절편을 나타내며, 회귀선이 y축과 만나는 점의 값입니다.
        β1β1​는 회귀계수를 나타내며, 독립 변수와 종속 변수 간의 관계를 설명하는 기울기입니다.
        또한, 회귀식을 사용하여 주어진 독립 변수 값에 대해 종속 변수의 값을 예측할 수 있습니다. 또한, 회귀식을 통해 독립 변수와 종속 변수 간의 관계를 이해하고 해석할 수 있습니다.

    4.1.2 적합값과 잔차

        1. 적합값(Fitted Value):
            *적합값은 회귀 모델을 사용하여 독립 변수의 값에 대한 종속 변수의 예측값을 나타냅니다.
            *회귀 모델이 주어진 독립 변수의 값에 대해 예측한 종속 변수의 값입니다.
            *회귀 모델을 통해 예측된 종속 변수 값으로, 모델이 데이터에 얼마나 잘 맞는지를 평가하는 데 사용됩니다.

        2. 잔차(Residual):
            *잔차는 실제 관측값과 회귀 모델로 예측한 값 사이의 차이를 나타냅니다.
            *즉, 잔차는 관측된 종속 변수 값과 회귀 모델로 예측된 값 간의 오차를 나타냅니다.
            *잔차는 회귀 모델의 예측 오차를 나타내며, 모델이 데이터를 얼마나 잘 설명하고 있는지를 평가하는 데 사용됩니다.
            *잔차가 작을수록 모델이 데이터를 더 잘 설명한다고 볼 수 있습니다.

        적합값은 회귀 모델이 예측한 종속 변수의 값으로, 잔차는 실제 관측값과 예측값 간의 차이로 모델의 정확성을 평가하는 데 사용됩니다. 함께 사용하여 회귀 모델의 적합도를 평가하고 모델의 예측력을 이해하는 데 도움이 됩니다.

    4.1.3 최소제곱(Least Squares):

        * 최소제곱은 회귀 모델을 적합시킬 때 사용되는 기법 중 하나입니다.
        * 회귀 모델은 종속 변수와 독립 변수 간의 관계를 설명하는 직선이나 곡선으로 나타낼 수 있습니다.
        * 최소제곱은 회귀선과 실제 데이터 포인트 간의 거리(잔차)를 최소화하는 방법입니다.
        * 즉, 회귀선으로부터 관측값까지의 거리를 제곱하여 모두 더한 값이 최소가 되도록 회귀 모델을 조정합니다.
        * 최소제곱 방법을 사용하여 회귀 모델을 적합시키면, 회귀 계수와 절편을 추정할 수 있습니다.

    4.1.4. 예측 대 설명(프로파일링):

        * 예측 대 설명은 회귀 모델을 통해 얻은 결과를 해석하고 이해하는 과정입니다.
        * 예측은 회귀 모델을 사용하여 새로운 독립 변수 값에 대한 종속 변수 값을 예측하는 것을 의미합니다.
        * 설명은 회귀 모델의 독립 변수들이 종속 변수에 미치는 영향을 이해하고 해석하는 것을 의미합니다.
        * 프로파일링 과정에서는 회귀 계수의 크기와 방향, 변수의 중요성 등을 분석하여 각 독립 변수가 종속 변수에 어떻게 영향을 미치는지 이해하고 설명합니다.
        * 이를 통해 회귀 모델이 데이터를 얼마나 잘 설명하고 있는지를 판단할 수 있습니다.

    4.2 다중선형회귀

        *다중선형회귀(Multiple Linear Regression)는 하나의 종속 변수와 둘 이상의 독립 변수 간의 선형 관계를 모델링하는 통계적 기법입니다. 다중선형회귀는 단순선형회귀와 달리, 둘 이상의 독립 변수가 종속 변수에 영향을 미치는 경우에 사용됩니다.

        " y=β0​+β1​x1​+β2​x2​+…+βn​xn​+ϵ "

        * yy는 종속 변수(응답 변수)를 나타냅니다.
        * x1,x2,…,xnx1​,x2​,…,xn​은 독립 변수(설명 변수)들을 나타냅니다.
        * β0,β1,β2,…,βnβ0​,β1​,β2​,…,βn​은 회귀계수를 나타내며, 각 독립 변수의 영향을 설명하는 계수입니다.
        * ϵ은 모델의 잔차를 나타냅니다.

        다중선형회귀 모델을 통해 여러 독립 변수들과 종속 변수 간의 관계를 모델링할 수 있으며, 이를 통해 독립 변수들이 종속 변수에 미치는 영향을 분석할 수 있습니다.

    **용어정리**
        제곱근평균제곱오차 (Root Mean Square Error, RMSE): 
            * 예측 값과 실제 값 간의 차이를 나타내는 오차의 제곱을 평균한 후 제곱근을 취한 값입니다.
            * RMSE가 작을수록 모델의 예측이 실제 값과 가깝다고 볼 수 있습니다.

        잔차표준오차 (Residual Standard Error, RSE):
            *회귀 분석에서 잔차의 표준 편차를 나타내는 지표입니다.
            *모델이 관찰값을 설명하는 정도를 측정하며, 작을수록 모델이 더 잘 설명한다고 할 수 있습니다.

        R제곱 (Coefficient of Determination, R-squared):
            *종속 변수의 총 변동 중 모델로 설명되는 변동의 비율을 나타내는 지표입니다.
            *0에서 1 사이의 값을 가지며, 1에 가까울수록 모델이 데이터를 잘 설명한다고 할 수 있습니다.

        t통계량 (t-Statistic):
            * 회귀 모델에서 각 회귀 계수의 유효성을 검정하는 데 사용되는 지표입니다.
            * t통계량은 회귀 계수를 표준 오차로 나눈 값으로, 회귀 계수가 유의한지를 판단하는 데 사용됩니다.
        가중회귀 (Weighted Regression):
            * 회귀 분석에서 각 데이터 포인트에 가중치를 부여하여 모델을 적합시키는 방법입니다.
            * 가중회귀는 일반 회귀 분석에서 모든 데이터 포인트를 동일하게 취급하는 한계를 극복하기 위해 사용됩니다.

    4.2.1 킹 카운티 주택 정보 예제티
    4.2.2 모형 평가
        제곱근 평균제곱오차

        잔차 표준오차
   
   
--------------------------------------------------------------------------------------
## 2024-05.31 [금]
--------------------------------------------------------------------------------------

Review]

빅데이터 -> IoT기기 센서 세팅 -> 저장 -> Data생성 -> file 메모리 
-> database[MySql] -> PC, Linux[Web] -> 분석 온도 

그래프 ()
numpy - 숫자타입을
pandas - serise가 모여서 DataFrame이 된다. 엑셀이라고 생각하면 됨 행과열

데이터의 분류 (수치, 범주) linear - 범주형을 수치형으로 바꿀 수 있다.

모집단, 샘플, 표본 || 회귀 chap4 - 머신러닝의 핵심 

그러면 회귀란 무엇이냐?
    - 2차원 데이터 표를 그리는 것, 다차원, 선형회귀.
    - 범주형 실제 데이터의 위치 [범주+분류]
    - 평가 -[MISS, RMSE, R2]
    - 계산이 이루어 지는 방식
    - 오차전파 back-prepagation
--------------------------------------------------------------------------------------

어제 이어서 CHAP4 회귀 그대로 진행 

    4.2.3 교차타당성검사 (Cross-Validation)

        교차타당성검사는 모델의 성능을 평가하기 위한 방법으로, 데이터의 과적합(overfitting)을 방지하고 일반화 성능을 향상시키기 위해 사용됩니다. 일반적인 방법은 다음과 같습니다:

    k-겹 교차타당성 (k-Fold Cross-Validation):
        데이터를 k개의 부분집합(fold)으로 나누고, k번의 반복된 학습 및 평가를 통해 모델 성능을 측정합니다.
        각 반복에서는 k개의 부분집합 중 하나를 검증 세트로 사용하고, 나머지 k-1개를 학습 세트로 사용합니다.
        모든 k번의 반복이 끝나면 각 반복에서 얻은 평가 점수의 평균을 내어 최종 성능을 평가합니다.

        이 방법의 장점은 데이터의 모든 부분이 검증 세트로 사용되기 때문에, 모델 성능 평가가 더욱 신뢰할 수 있다는 것입니다.

    4.2.4 모형선택 및 단계적회귀

        단계적 회귀 (Stepwise Regression)
        단계적 회귀는 회귀 분석에서 독립 변수를 선택하는 방법입니다. 모델의 복잡성을 줄이고 성능을 최적화하기 위해 불필요한 변수들을 제거하거나 중요한 변수를 추가하는 방식입니다. 두 가지 주요 접근법이 있습니다:

        1. 전진 선택법 (Forward Selection):
            시작할 때 모델에 변수가 없고, 하나씩 변수를 추가합니다.
            각 단계마다 가장 크게 성능이 향상되는 변수를 추가합니다.
            더 이상 성능이 유의미하게 향상되지 않을 때까지 반복합니다.
        2. 후진 제거법 (Backward Elimination):
            모든 변수를 포함한 모델로 시작합니다.
            각 단계마다 가장 유의미하지 않은 변수를 제거합니다.
            더 이상 제거할 변수가 없을 때까지 반복합니다.
        3. 단계적 방법 (Stepwise Method):
            전진 선택법과 후진 제거법을 결합한 방식으로, 변수를 추가할 때마다 기존 변수 중 유의미하지 않은 변수를 제거할 수도 있습니다.

    단계적 회귀는 변수 선택 과정을 자동화하여 모델의 예측 성능을 최적화하려고 합니다.

    4.2.5 가중 회귀 (Weighted Regression)

        가중 회귀는 데이터 포인트마다 다른 가중치를 부여하여 회귀 분석을 수행하는 방법입니다. 이 방법은 각 데이터 포인트의 중요도나 신뢰도에 따라 가중치를 다르게 적용합니다. 주요 용도는 다음과 같습니다:

            1. 이상치 처리: 이상치가 있는 데이터에서 이상치의 영향을 줄이기 위해 낮은 가중치를 부여할 수 있습니다.
            2. 데이터 신뢰도: 데이터 포인트마다 측정의 신뢰도가 다를 때, 신뢰도가 높은 데이터에 더 높은 가중치를 부여할 수 있습니다.
            3. 시간 가중: 시간에 따라 데이터의 중요도가 달라질 때, 최근 데이터에 더 높은 가중치를 부여할 수 있습니다.

            가중 회귀를 수행할 때, 회귀 모델은 가중치를 고려하여 잔차(residual)를 최소화하는 방향으로 최적화됩니다.

4.3 회귀를 이용한 예측

    **용어정리**
        -예측구간[prediction interval] : 개별예측값 주위의 불확실한 구간
        -외삽법[extrapolation] : 모델링에 사용된 데이터 범위를 벗어난 부분까지 모델을 확장하는 것

    4.3.1 외삽의 위험
    4.3.2 신뢰구간과 예측구간

4.4 회귀에서의 요인변수 [매우중요!!!]

    4.4.1 가변수 표현 (Dummy Variable Representation)

    가변수 표현은 범주형 변수를 수치형 변수로 변환하는 방법입니다. 이 방법은 범주형 변수의 각 수준(level)을 이진 변수(binary variable)로 변환하여 회귀 분석이나 다른 머신러닝 모델에 사용할 수 있도록 합니다.

    예를 들어, 범주형 변수 "색상"이 "빨강", "파랑", "초록" 세 가지 값을 가질 경우, 이를 가변수로 변환하면 다음과 같은 이진 변수로 표현할 수 있습니다:

                            빨강: [1, 0, 0]
                            파랑: [0, 1, 0]
                            초록: [0, 0, 1]

    여기서 각 가변수는 해당 범주에 속하면 1, 그렇지 않으면 0의 값을 가집니다.

    4.4.2 다수의 수준을 갖는 요인변수들 (Factors with Multiple Levels)

        다수의 수준을 갖는 요인변수는 여러 가지 범주(수준)를 갖는 범주형 변수를 의미합니다. 예를 들어, "색상"이라는 변수는 "빨강", "파랑", "초록" 등의 여러 수준을 가질 수 있습니다. 이러한 요인변수를 분석에 사용하기 위해 여러 가지 방법을 사용할 수 있습니다:

    1. 기준 부호화 (Reference Coding):
        기준 범주를 하나 설정하고 나머지 범주를 그 기준 범주와 비교합니다.
        예: "빨강"을 기준 범주로 설정하면, "파랑"과 "초록"에 대한 가변수는 [1, 0], [0, 1]로 표현되며, "빨강"은 [0, 0]으로 표현됩니다.

    2. 원-핫 인코딩 (One-Hot Encoding):
        각 범주를 독립된 이진 변수로 변환합니다.
        예: "빨강", "파랑", "초록" 각각을 독립된 변수로 표현하면 [1, 0, 0], [0, 1, 0], [0, 0, 1]로 나타냅니다.

    3. 편차 부호화 (Deviation Coding):
        각 범주가 평균 수준에서의 편차를 나타내도록 합니다.
        기준 범주(예: "빨강")를 제외한 나머지 범주를 -1, 0, 1 등의 값으로 표현하여 평균 대비 편차를 나타냅니다.

    4. 이진 부호화 (Binary Encoding):
        범주형 변수를 이진 숫자로 변환하고, 각 이진 자리수를 새로운 변수로 만듭니다.
        예: "빨강" = 1 (001), "파랑" = 2 (010), "초록" = 3 (011)으로 변환한 후, 각 자리수를 변수로 사용합니다.

        이 방법들은 모두 범주형 변수를 수치형 변수로 변환하여 분석에 사용할 수 있도록 도와줍니다. 각각의 방법은 특정 상황에 더 적합할 수 있으며, 선택하는 방법에 따라 모델의 성능과 해석이 달라질 수 있습니다.

    4.4.3 순서가 있는 요인변수 (Ordinal Factor Variable)

        "순서가 있는 요인 변수", "순서 요인 변수", "순서 범주형 변수"는 모두 순서가 있는 범주형 변수를 지칭하는 용어들입니다. 이 용어들은 데이터 과학, 통계학, 머신러닝에서 사용되는 용어로, 순서가 있는 범주형 데이터를 다루기 위해 사용됩니다.
        
        1. 순서가 있는 요인 변수 (Ordinal Factor)
            순서가 있는 요인 변수는 범주형 변수의 일종으로, 그 값들 사이에 순서가 존재합니다. 그러나 그 사이의 간격이 균등하지 않을 수 있습니다. 예를 들어, 교육 수준(초등학교, 중학교, 고등학교, 대학교)이나 고객 만족도(매우 불만족, 불만족, 보통, 만족, 매우 만족) 등이 이에 해당합니다. 이 변수들은 순서가 있으므로 순서 정보를 이용한 분석이 가능합니다.
            
        2.순서 요인 변수 (Ordinal Factor)
            순서 요인 변수는 순서가 있는 요인 변수와 같은 의미로 사용되며, 범주의 순서가 중요한 변수입니다. 순서 요인 변수는 요인 분석이나 다른 통계적 분석에서 주로 사용됩니다. 이 변수들은 범주의 순서가 관련된 데이터 분석 및 모델링에서 중요한 역할을 합니다.

        3.순서 범주형 변수 (Ordered Categorical Variable)
            순서 범주형 변수는 범주형 변수 중에서도 각 범주가 서로 비교 가능하고 순서가 있는 변수를 지칭합니다. 이러한 변수들은 수치형 변수와 유사한 성격을 가지고 있으며, 일반적으로는 수치형 변수로 취급될 수 있습니다. 예를 들어, "선호도"가 "매우 싫다", "싫다", "보통", "좋다", "매우 좋다"와 같이 순서가 있는 경우가 해당됩니다.

        이러한 용어들은 데이터 과학과 통계학에서 중요한 의미를 갖고 있으며, 순서가 있는 범주형 데이터를 이해하고 분석하는 데 사용됩니다.

    **요약**

        *순서가 있는 요인변수 (Ordered Factor Variable)
        데이터 분석에서 요인(Factor)은 범주형 변수를 의미합니다. 순서가 있는 요인변수는 이러한 범주형 변수 중에서도 순서가 있는 경우를 말합니다. 예를 들어, 교육 수준(고졸, 대졸, 석사, 박사)이나 리커트 척도(매우 동의, 동의, 보통, 반대, 매우 반대) 등이 이에 해당합니다.

        *순서요인변수 (Ordinal Factor Variable)
        이 용어는 순서가 있는 요인변수와 동일한 의미로 사용됩니다. 즉, 범주형 변수 중 순서나 등급이 있는 변수를 가리킵니다.

        *순서 범주형 변수 (Ordinal Categorical Variable)
        이 역시 순서가 있는 요인변수와 같은 의미입니다. 순서 범주형 변수는 범주 간의 순서가 중요하지만, 그 사이의 간격은 반드시 일정하지 않은 변수입니다. 예를 들어, '소득 수준'을 '낮음', '중간', '높음'으로 나눈 경우입니다.

        * 요인변수는 회귀를 위해 수치형 변수로 변환해야한다.
        * 요인변수를 p개의 개별값으로 인코딩하기 위한 가장 흔한 방법은 P-1개의 가변수를 만들어 사용하는 것이다.
        * 다수의 수준을 갖는요인 변수의 경우, 더 적은 수의 수준을 갖는 변수가 되도록 수준들을 통합해야한다.
        * 순서를갖는 요인변수의 경우, 수치형 변수로 변환하여 사용할수있다.

    4.5 회귀방정식 해석

        **용어정리**
        1.변수간 상관 (Correlation between variables):
            두 변수 간의 관련성 또는 상호 연관성을 나타냅니다. 변수간 상관은 일반적으로 피어슨 상관 계수(Pearson correlation coefficient)를 사용하여 측정됩니다. 이 값은 -1부터 1까지의 범위를 가지며, 양의 값은 양의 선형 상관 관계를, 음의 값은 음의 선형 상관 관계를 나타냅니다. 0은 상관 관계가 없음을 의미합니다.

        2.다중공선성 (Multicollinearity):
            다중공선성은 회귀 분석에서 독립 변수들 간에 높은 상관 관계가 있는 경우 발생합니다. 이는 회귀 모델에서 각 독립 변수의 영향을 정확하게 평가하는 데 어려움을 일으키며, 회귀 계수의 추정치가 불안정해지는 문제를 초래할 수 있습니다.

        3.교란변수 (Confounding variable):
            교란변수는 종속 변수와 독립 변수 간의 관계를 왜곡시키는 변수를 의미합니다. 종속 변수와 독립 변수 간의 관계를 분석할 때 통제되지 않은 교란변수가 있다면, 이것이 분석 결과에 영향을 미칠 수 있습니다.

        4.주효과 (Main effect):
            주효과는 통계적 실험에서 주로 사용되며, 각 독립 변수의 개별적인 영향을 나타냅니다. 즉, 모든 다른 변수가 고정된 상태에서 해당 독립 변수의 영향을 측정합니다.

        5.상호작용 (Interaction):
            상호작용은 두 변수 간의 관계가 다른 변수의 수준에 따라 달라지는 경우를 의미합니다. 이는 통계 모델에서 변수 간의 조합이 예측 변수의 영향을 설명하는 데 도움이 되는 경우에 중요합니다. 예를 들어, A와 B라는 두 독립 변수가 있을 때, A와 B의 상호작용은 A와 B가 함께 있을 때 종속 변수에 미치는 영향을 나타냅니다.

        4.5.1 예측변수 간 상관 (Correlation between predictor variables):
            회귀 분석에서 사용되는 예측 변수들 간의 상관 관계를 의미합니다. 이는 예측 변수들이 서로 연관되어 있을 때 발생합니다. 이러한 상관 관계는 다중공선성을 야기할 수 있으며, 모델의 해석을 어렵게 만들 수 있습니다.

        4.5.2 다중공선성 (Multicollinearity):
            다중공선성은 회귀 분석에서 예측 변수들 간에 높은 상관 관계가 있는 경우를 말합니다. 즉, 하나의 예측 변수가 다른 예측 변수들과 강한 선형 관계를 갖고 있을 때 발생합니다. 다중공선성은 회귀 분석의 결과를 왜곡시키고, 모델의 신뢰성을 저하시킬 수 있습니다.

        4.5.3 교란변수 (Confounding variable):
            교란변수는 종속 변수와 예측 변수 간의 관계를 왜곡시키는 변수를 의미합니다. 다른 변수들로 인해 예측 변수와 종속 변수 간의 관계가 오해되거나 왜곡되는 경우가 있습니다. 이를 통제하지 않으면 부정확한 결과를 얻을 수 있습니다.

        4.5.4 상호작용과 주효과 (Interaction and Main effect):
            상호작용은 두 예측 변수 간의 관계가 다른 변수의 수준에 따라 변하는 경우를 의미합니다. 즉, 한 변수의 효과가 다른 변수의 값을 고려하여 달라지는 경우입니다. 반면 주효과는 각 예측 변수의 개별 효과를 나타냅니다.
    
            예를 들어, A와 B라는 두 예측 변수가 있을 때, A와 B 사이의 상호작용이 있다면 A의 효과는 B의 값에 따라 변할 수 있습니다. 주효과는 A와 B의 효과가 각각 독립적으로 고려될 때를 의미합니다.

    4.6. 회귀진단
        회귀진단(Regression diagnostics)은 회귀 분석에서 모델의 적합성과 신뢰성을 평가하는 과정을 말합니다. 회귀 모델을 만들고 데이터에 적합시킨 후, 모델이 데이터를 잘 설명하고 적절히 작동하는지를 확인하기 위해 다양한 통계적 기법과 그래픽 방법을 사용합니다.

        **용어정리**
            1.표준화 잔차 (Standardized Residual): 잔차를 해당 잔차의 표준편차로 나눈 값으로, 이상치를 식별하거나 모델의 적합성을 비교하는 데 사용됩니다.
            2.특잇값 (Outlier):일반적인 데이터 패턴에서 벗어난 극단적인 값을 가진 관측치로, 회귀 분석 결과에 영향을 줄 수 있습니다.
            3.영향값 (Influence value):특정 관측치가 회귀 분석 결과에 미치는 영향의 정도를 나타내는 값으로, 해당 관측치를 제외했을 때 모델 결과가 어떻게 변하는지를 나타냅니다.
            4.지렛대(레버리지) (Leverage):특정 관측치가 회귀 모델에 미치는 영향력을 나타내며, 해당 관측치가 독립 변수들의 평균으로부터 얼마나 멀리 떨어져 있는지를 측정합니다.
            5.비정규 잔차 (Non-Normal Residual):회귀 분석의 잔차가 정규분포를 따르지 않는 경우를 의미합니다.
            6.이분산성 (Heteroscedasticity):잔차의 분산이 독립 변수들의 값에 따라서 변하는 경우를 의미합니다.
            7.편잔차 그림 (Residual Plot):회귀 모델의 적합성을 시각적으로 확인하기 위해 잔차를 가로축으로, 예측값(또는 독립 변수)을 세로축으로 하는 그래프를 말합니다.

        4.6.1 특잇값
        특잇값 (Singular Value)
            정의: 행렬의 특이값 분해(Singular Value Decomposition, SVD)에서 나오는 값들로, 행렬의 중요한 성질을 나타냄.
            용도: 데이터 압축, 차원 축소, 신호 처리 등에서 사용됨.

        극단값 (Outlier)
            정의: 데이터 분포에서 다른 관측값들과 현저히 다르게 떨어져 있는 값.
            특징: 극단값은 회귀 분석 등에서 모델의 결과에 큰 영향을 미칠 수 있으며, 종종 데이터 오류나 특이한 상황을 반영함.
            영향값: 극단값이 회귀 분석 결과에 미치는 영향을 평가한 값.

        4.6.2 영향값 (Influence Value)

            정의: 특정 데이터 포인트가 회귀 분석 결과에 미치는 영향을 측정한 값.
            특징: 높은 영향값은 해당 포인트가 모델에 큰 영향을 미친다는 것을 의미함.
        4.6.3 이분산성, 비정규성, 오차 간 상관
            
            *이분산성 (Heteroscedasticity)
            정의: 회귀 분석에서 오차의 분산이 일정하지 않고, 독립 변수의 값에 따라 달라지는 현상.
            특징: 이분산성이 존재하면 회귀 모델의 신뢰도가 떨어짐.

            *비정규성 (Non-normality)
            정의: 데이터의 분포가 정규분포를 따르지 않는 현상.
            특징: 회귀 분석 가정이 위배되어, 추정치와 통계적 검정이 부정확해질 수 있음.

            *오차간 상관 (Autocorrelation)
            정의: 회귀 분석에서 오차항들 간에 상관관계가 있는 현상.
            특징: 오차 간 상관이 있으면 모델의 신뢰성이 낮아지고, 예측력이 떨어짐.

        4.6.4 편잔차그림과 비선형성

        *편잔차 그림 (Partial Residual Plot)

            정의: 특정 독립 변수가 종속 변수에 미치는 효과를 시각적으로 나타낸 그래프.
            특징: 변수의 비선형성과 상호작용을 식별하는 데 유용함.

        *비선형성 (Non-linearity)

            정의: 독립 변수와 종속 변수 간의 관계가 선형이 아닌 현상.
            특징: 회귀 모델이 비선형 관계를 포착하지 못하면, 모델의 적합성이 떨어짐.

    4.7 다향회귀와 스플라인 회귀

        다항 회귀는 독립 변수와 종속 변수 간의 관계를 다항식으로 모델링합니다. 선형 회귀가 독립 변수와 종속 변수 사이의 선형 관계를 모델링하는 반면, 다항 회귀는 비선형 관계를 다항식으로 표현합니다.

        **용어정리**
            *다항 회귀 (Polynomial Regression): 독립 변수와 종속 변수 간의 비선형 관계를 다항식으로 모델링하는 기법.
            *스플라인 회귀 (Spline Regression): 데이터를 여러 구간으로 나누어 각 구간을 다항식으로 모델링하고 매끄럽게 연결하는 기법.
            *매듭 (Knot): 스플라인 회귀에서 데이터를 나누는 기준점.
            *일반화 가법 모형 (GAM): 예측 변수와 반응 변수 간의 관계를 비선형 함수의 합으로 모델링하는 유연한 통계 모형.
            *링크 함수 (Link Function): 반응 변수의 기대값과 선형 예측값을 연결하는 함수.

        * 다항 회귀 모델: 다항 회귀 모델은 위의 다항식을 사용하여 데이터를 적합시킵니다. 
        예를 들어, 2차 다항 회귀는 다음과 같습니다:
    
                            y=a0+a1x+a2x2
        4.7.1 다항식: 
        다항식은 여러 항의 합으로 구성된 수학적 표현입니다. 
        일반적인 n차 다항식은 다음과 같이 나타낼 수 있습니다:
                    
                     y=a0+a1x+a2x2+a3x3+...+anxn

        여기서 ai​는 다항식의 계수이고, xx는 독립 변수입니다.

        4.7.2 스플라인 (Spline)

        스플라인은 주어진 구간을 여러 부분으로 나누고 각 부분을 다항식으로 적합시킨 곡선입니다. 각 구간의 다항식은 매끄럽게 연결되어 전체 곡선이 부드러운 모양을 갖도록 합니다.

        * 구간 정의: 스플라인은 구간을 나누는 접점 (knots)을 기준으로 정의됩니다. 각 구간에서의 다항식은 그 구간에 있는 데이터 포인트를 적합시킵니다.
        * B-스플라인: B-스플라인은 기본 스플라인 함수로, 각 구간에서의 다항식을 표현하는 데 사용됩니다. B-스플라인은 높은 유연성과 매끄러움을 제공하며, 복잡한 데이터 패턴을 효과적으로 모델링할 수 있습니다.

        4.7.3 일반화가법모형


CHAP 5 분류 [시작!]  

    5.1 나이브 베이즈

    **용어정리**
        *조건부확률(conditional probability) : 어떤 사건 (Y=i)이 주어졌을때, 해당사건(X=i)를 관찰 할 확률 p(X1|Y1)
        *사후확률(posterior probabilty) : 예측정보를 통합한 후 결과의 확률(이와 달리, 사전확률에서는 예측변수에 대한 정보를 고려하지 않는다.)
        
        5.1.1 나이브하지 않은 베이지분류는 왜 현실성이 없을까?
        5.1.2 나이브한해법
        5.1.3 수치형 예측변수
        
    5.2 판별분석
        5.2.1 공분산행렬
        5.2.2 피셔의 선형판별

    5.3 로지스틱 회귀
        5.3.1 로지스틱 반응함수와 로짓
        5.3.2 로지스틱 회귀와 GLM
        5.3.3 일반화선형모형
        5.3.4 로지스틱 회귀의 예측값
        5.3.5 계수와 오즈비 해석하기
        5.3.6 선형회귀와 룆스틱 회귀: 유사점과 차이점

--------------------------------------------------------------------------------
## 2024-06.03 [월]
--------------------------------------------------------------------------------

    5.4 분류 모델 평가하기
        5.4.1 혼동행렬
        5.4.2 희귀 클래스 문제
        5.4.3 정밀도, 재현율, 특이도
        5.4.4 ROC곡선
        5.4.5 AUC
        5.4.6 리프트

    5.5 불균형 데이터 다루기
        5.5.1 과소표본추출
        5.5.2 과잉표본추출과 상향/하향 가중치
        5.5.3 데이터 생성
        5.5.4 비용 기반 분류
        5.5.5 예측결과 분석

chap6 시작 - 통계적 머신러닝

통계적 머신러닝, 앙상블 학습, 의사결정 트리는 모두 데이터 분석 및 예측을 위한 머신러닝 분야에서 중요한 개념들입니다. 각각에 대해 자세히 설명하겠습니다.

1. 통계적 머신러닝 (Statistical Machine Learning)

통계적 머신러닝은 통계학의 원리를 활용하여 데이터로부터 예측 모델을 구축하는 방법입니다. 이 접근법은 데이터의 확률 분포를 추정하거나 통계 모델을 통해 데이터의 패턴을 학습합니다. 주요 개념은 다음과 같습니다:

    * 회귀 분석 (Regression Analysis): 연속적인 값을 예측하기 위해 사용됩니다. 예를 들어, 주택 가격 예측, 주식 가격 예측 등이 있습니다.
    * 분류 (Classification): 데이터가 어떤 카테고리에 속하는지 예측합니다. 예를 들어, 이메일이 스팸인지 아닌지 분류하는 것 등이 있습니다.
    * 클러스터링 (Clustering): 데이터 포인트를 비슷한 특성을 가진 그룹으로 나눕니다. 예를 들어, 고객 세분화 등이 있습니다.

2. 앙상블 학습 (Ensemble Learning)

앙상블 학습은 여러 개의 모델을 결합하여 더 나은 성능을 얻는 방법입니다. 개별 모델의 예측을 종합하여 최종 예측을 도출하기 때문에 일반적으로 단일 모델보다 더 정확하고 견고한 결과를 제공합니다. 대표적인 앙상블 기법으로는 다음이 있습니다:

    * 배깅 (Bagging): 여러 모델을 독립적으로 학습시키고 이들의 예측을 평균내거나 다수결로 결합합니다. 예: 랜덤 포레스트(Random Forest).
    * 부스팅 (Boosting): 약한 모델을 순차적으로 학습시키며, 이전 모델이 잘못 예측한 부분에 가중치를 두어 점점 더 성능을 향상시킵니다. 예: 그라디언트 부스팅(Gradient Boosting), XGBoost.
    * 스태킹 (Stacking): 여러 다른 모델의 예측을 새로운 모델의 입력으로 사용하여 최종 예측을 만드는 기법입니다.

3. 의사결정 트리 (Decision Tree)

의사결정 트리는 데이터의 특성을 기반으로 의사결정을 트리 구조로 모델링한 것입니다. 트리의 각 노드는 특성에 대한 조건문을 나타내며, 가지(branch)는 조건에 따른 결과를 나타냅니다. 최종 리프 노드는 예측 결과를 나타냅니다. 주요 특징은 다음과 같습니다:

    *직관적 해석: 트리 구조로 인해 예측 과정이 매우 직관적이고 해석이 용이합니다.
    *비모수적 방법: 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
    *카테고리 및 연속형 데이터 처리: 의사결정 트리는 다양한 유형의 데이터를 처리할 수 있습니다.

의사결정 트리는 단독으로 사용될 수도 있지만, 배깅(예: 랜덤 포레스트)이나 부스팅(예: AdaBoost, Gradient Boosting)과 같은 앙상블 기법에서 기본 모델로 자주 사용됩니다.


    6.1 k-최근접이웃 (k-Nearest Neighbors, KNN)

        KNN의 기본 개념

        KNN은 새로운 데이터 포인트를 예측할 때, 기존 데이터 포인트 중 가장 가까운 k개의 이웃을 찾고, 이 이웃들의 정보를 기반으로 예측을 수행합니다. "가까움"은 일반적으로 유클리드 거리와 같은 거리 측정 방법을 통해 정의됩니다.

        KNN 알고리즘의 작동 방식

            1. 훈련 데이터 준비: 알고리즘이 학습할 수 있도록 레이블이 지정된 훈련 데이터를 준비합니다.
            2. 새로운 데이터 포인트: 예측하려는 새로운 데이터 포인트를 제공합니다.
            3. 거리 계산: 새로운 데이터 포인트와 모든 훈련 데이터 포인트 간의 거리를 계산합니다.
            4. k개의 이웃 선택: 계산된 거리 중 가장 가까운 k개의 이웃을 선택합니다.
            예측:
                분류: 선택된 k개의 이웃 중 가장 많이 나타나는 클래스를 예측 값으로 지정합니다 (다수결 투표).
                회귀: 선택된 k개의 이웃의 평균 값을 예측 값으로 지정합니다.

        KNN의 특징

            * 단순함: 구현이 매우 간단합니다.
            * 비매개변수적: 사전에 모델을 학습시키지 않으며, 예측 시에만 계산이 이루어집니다.
            * 유연성: 분류와 회귀 모두에 사용할 수 있습니다.
            * 특성 스케일링: 거리 기반 알고리즘이기 때문에 특성 스케일링(정규화)이 중요합니다.

        KNN의 장단점

        장점:
            직관적이고 이해하기 쉬움: 알고리즘의 작동 방식이 명확하고 단순합니다.
            적응성: 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
            다양한 문제에 적용 가능: 분류와 회귀 문제 모두에 사용할 수 있습니다.
        단점:
            계산 비용이 높음: 예측 시 모든 훈련 데이터 포인트와의 거리를 계산해야 하므로 데이터가 많을수록 계산 비용이 높아집니다.
            메모리 사용량이 큼: 모든 훈련 데이터를 저장하고 있어야 합니다.
            고차원 데이터에서 성능 저하: 고차원 데이터에서 거리 계산이 덜 효과적일 수 있습니다 (차원의 저주).

        KNN의 사용 사례
            이미지 인식: 비슷한 이미지들을 군집화하거나 분류하는 데 사용됩니다.
            추천 시스템: 사용자 선호도를 기반으로 비슷한 사용자들의 데이터를 참조하여 추천을 제공합니다.
            패턴 인식: 텍스트 분류, 음성 인식 등 다양한 패턴 인식 문제에 활용됩니다.

        KNN은 이해하기 쉽고 다양한 문제에 적용 가능한 유용한 알고리즘입니다. 다만, 대규모 데이터셋이나 고차원 데이터셋에서의 사용은 효율성을 고려해야 합니다.

        ** 용어정리 **
            *이웃(neighbor) : 예측변수에서 값들이 유사한 레코드
            *거리지표(distance metric) : 각 레코드사이가 얼마나 멀리 떨어져 있는지를 나타내는 단일값
            *표준화(standardization) : 평균을 뺀 후에 표준편차로 나누는 일(유의어 : 정규화)
            *z점수(z-score) : 표준화를 통해 얻은 값
            *K : 최근접 이웃을 계산하는데 사용되는 이웃의 개수

        6.1.1 예제 : 대출연체예측
        6.1.2 거리 지표
        6.1.3 원-핫 인코더
        6.1.4 표준화(정규화, z점수)
        6.1.5 선택하기
        6.1.6 KNN을 통한 피처 엔지니어링

    6.2 트리모델 

    트리 모델(Tree Model)은 머신러닝에서 주로 사용되는 예측 모델의 한 유형으로, 트리 구조를 이용하여 의사 결정을 모델링합니다. 트리 모델의 대표적인 종류에는 의사결정 트리(Decision Tree), 랜덤 포레스트(Random Forest), 그리고 그라디언트 부스팅 트리(Gradient Boosting Tree) 등이 있습니다. 트리 모델의 주요 특징과 각각의 트리 모델을 간단히 설명하겠습니다.

    트리 모델의 주요 특징

    1. 트리 구조: 트리 모델은 루트 노드(root node)에서 시작하여 리프 노드(leaf node)로 끝나는 트리 구조를 가집니다. 각 내부 노드는 하나의 특성(feature)을 기반으로 데이터를 분할하고, 리프 노드는 최종 예측값을 나타냅니다.
    2. 규칙 기반 학습: 트리 모델은 데이터를 분할하기 위한 일련의 규칙을 학습합니다. 각 분할(split)은 데이터의 특성과 값을 기반으로 결정됩니다.
    3. 비모수적: 트리 모델은 데이터의 분포에 대한 가정을 필요로 하지 않습니다.
    4. 해석 가능성: 트리 모델은 의사 결정 과정을 시각적으로 표현할 수 있어, 모델의 작동 방식을 쉽게 이해하고 설명할 수 있습니다.

    ** 용어정리 **
        * 재귀분할(recursive partitioning) : 마지막 분할 영역에 해당하는 출력이 최대한 비슷한(homogeneous)결과를 보이도록 데이터를 반복적으로 분할하는 것
        * 분할값(split value) : 분하값을 기준으로 예측변수를 그 값보다 작은 영역과 큰 영역으로 나눈다.
        *마디(노드)node: 의사결정 트리와 같은 가지치기 형태로 구성된 규칙들을 집합에서, 노드는 분할규칙의 시각적인 표시라고 할 수 있다.
        *잎(leaf):if-then 규칙의 가장 마지막부분, 혹은 트리의 마지막 가지branch 부분을 의미한다. 트리모델에서 잎 노드는 어떤레코드에 적용할 최종적인 분류규칙을 의미한다.
        *손실(loss):분류하는과정에서 발생하는 오분류의 수, 손실이 클수록 불순도가 높다고 하 수 있다.
        *불순도(impurity):데이터를 분할한 집합엣 서로 다른 클래스의 데이터가 얼마나 섞여있는지를 나타낸다. 더 많이 섞여 있을수록 불순도가 높다고 할 수 있다.(유의어:이질성heterogenity, 반의어:동질성homogeneity, 순도)
        *가지치기(pruning):학습이 끝난 트리모델에서 오버피팅을 줄이기 위해 가지들을 하나씩 잘라내는 과정

        6.2.1 간단한예제
        6.2.2 재귀분할 알고리즘
        6.2.3 동질성과 불순도 측정하기
            -지니계수
            지니불순도를 지니계수(Gini coefficient)와 혼동해서는 안된다. 둘다 모두 개념적으로 비슷하지만 지니계수는 이진분류문제로 한정되며, AUC지표와 관련이 있는 용어이다.
        6.2.4 트리 형성 중지하기
        6.2.5 연속값 예측하기
        6.2.6 트리 활용하기
--------------------------------------------------------------------------------
## 2024-06.04 [화]
--------------------------------------------------------------------------------

    6.3 배깅과 랜덤 포레스트

    **용어 정리**
        * 앙상블(ensmeble) : 여러 모델의 집합을 이요해서 하나의 예측을 이끌어내는 방식(유의어:모델평균화[model averaging])
        * 배깅(bagging) : 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법(유의어:부트스트랩 종합[vootstrap aggregation])
        * 랜덤 프레스트(random forest) : 의사 결정 트리 모델에 기반을 둔 배깅 추정모델(유의어:배깅 의사 결정 트리)
        * 변수 중요도(variable importance) : 모델 성능에 미치는 예측변수의 중요도

        6.3.1 배깅 (Bagging)
            배깅(Bootstrap Aggregating)은 여러 약한 학습기를 결합하여 예측 성능을 향상시키는 앙상블 기법입니다. 배깅의 기본 아이디어는 원래 데이터셋에서 여러 개의 부트스트랩 샘플(중복을 허용한 랜덤 샘플)을 생성하고, 각 샘플에 대해 모델을 학습한 후 그 예측 결과를 평균내거나 다수결로 결정하는 것입니다. 배깅은 모델의 분산을 줄여 과적합을 방지하는 데 효과적입니다.

        6.3.2 랜덤포레스트 (Random Forest)
            랜덤포레스트는 배깅의 확장판으로, 여러 결정 트리를 앙상블하여 예측 성능을 향상시키는 방법입니다. 랜덤포레스트는 배깅의 기본 개념에 더해, 각 결정 트리가 학습할 때 데이터의 무작위 하위 집합뿐만 아니라 특성(변수)의 무작위 하위 집합도 선택합니다. 이는 트리들 간의 상관성을 줄여 모델의 예측 성능을 향상시키고, 과적합을 방지합니다. 랜덤포레스트는 높은 예측 정확도와 변수 중요도 측정의 장점이 있습니다.

        6.3.3 변수 중요도 (Feature Importance)
            랜덤포레스트와 같은 앙상블 모델은 변수 중요도를 평가할 수 있습니다. 변수 중요도는 각 특성이 모델의 예측에 얼마나 기여하는지를 나타냅니다.

        6.3.4 하이퍼파라미터
            하이퍼파라미터는 모델 학습 과정에서 사용자가 직접 설정해야 하는 매개변수로, 모델의 성능에 큰 영향을 미칩니다.

    6.4 부스팅

    **용어 정리**
        *앙상블(ensmeble) : 여러모델들의 집합을 통해 예측결과를 만들어 내는것 (유의어 : 모델 평균화)
        *부스팅(boosting) : 연속된 라운드마다 잔차가 큰 레코드들에 가중치 높여 일련의 모델들을 생성하는 일반기법
        *에이다부스트(AdaBoost) : 잔차에 따라 데이터의 가중치를 조절하는 부스팅의 초기버전
        *그레이디언트 부스팅(gradient boosting) : 비용함수(cost function)를 최소화하는 방향으로 부스팅을 활용하는 좀 더 일반적인 형태
        *확률적 그레이디언트 부스팅(stochastic gradient boosting) :각 라운드마다 레코드와 열을 재표본추출하는것을 포함하는 부스팅의 가장 일반적인 형태
        *정규화(regularization) : 비용함수에 모델의 파라미터 개수에 해당하는 벌점항을 추가해 오버피팅을 피하는 방법
        *하이퍼파라미터(hyperparameter) : 알고리즘을 피팅하기 전에 미리 세팅해야하는 파라미터

        6.4.1 부스팅 알고리즘
            부스팅(Boosting)은 여러 약한 학습기(weak learners)를 결합하여 강한 학습기(strong learner)를 만드는 앙상블 기법입니다. 약한 학습기란 예측 성능이 조금 낮은 모델을 의미하며, 이를 반복적으로 학습하여 예측 성능을 향상시키는 것입니다. 대표적인 부스팅 알고리즘에는 AdaBoost, Gradient Boosting, 그리고 XGBoost가 있습니다. 부스팅은 주로 순차적으로 모델을 학습시키며, 이전 모델이 틀린 데이터에 대해 가중치를 높여 다음 모델이 이 오류를 줄이도록 합니다.

        6.4.2 XG부스트
            XGBoost(eXtreme Gradient Boosting)는 Gradient Boosting 알고리즘의 확장판으로, 효율성과 성능이 매우 뛰어납니다. XGBoost는 다양한 최적화 기법과 정규화 기법을 도입하여 과적합을 방지하고, 빠른 학습과 예측을 가능하게 합니다. 또한, 분산 컴퓨팅을 지원하여 대규모 데이터셋에도 효과적으로 사용할 수 있습니다.

        6.4.3 정규화 : 오버피팅 피하기
            정규화는 모델의 복잡성을 줄여 과적합(overfitting)을 방지하는 기술입니다. 과적합은 모델이 학습 데이터에 너무 맞춰져서 새로운 데이터에 대해 일반화 성능이 떨어지는 현상입니다. 정규화 기법에는 L1 정규화(Lasso), L2 정규화(Ridge), 그리고 Elastic Net 등이 있습니다.

            * L1 정규화 (Lasso): 가중치의 절대값 합계를 최소화합니다. 이는 일부 가중치를 0으로 만들어 변수 선택(variable selection) 효과가 있습니다.
            * L2 정규화 (Ridge): 가중치의 제곱 합계를 최소화합니다. 이는 모든 가중치를 작게 만들어 모델의 복잡성을 줄입니다.
            * Elastic Net: L1과 L2 정규화를 결합한 방식으로, 두 가지 기법의 장점을 모두 취합니다.

        6.4.4 하이퍼파라미터와 교차타당성검사
            하이퍼파라미터는 모델 학습 과정에서 사용자가 직접 설정해야 하는 매개변수로, 모델의 성능에 큰 영향을 미칩니다. 예를 들어, 결정 트리의 깊이, 학습률(learning rate), 정규화 파라미터 등이 하이퍼파라미터에 해당합니다. 하이퍼파라미터는 학습 데이터에서 직접 학습되지 않으며, 사용자가 사전에 설정해야 합니다.

            교차타당성검사는 모델의 일반화 성능을 평가하기 위한 방법입니다. 데이터를 여러 개의 폴드(fold)로 나누고, 각 폴드에 대해 학습과 검증을 반복하여 모델의 성능을 평가합니다. 가장 일반적인 방법은 K-폴드 교차타당성(K-Fold Cross-Validation)입니다.

            K-폴드 교차타당성: 데이터를 K개의 폴드로 나누고, 각 폴드마다 한 번씩 검증 데이터를 사용하여 K번 학습을 반복합니다. 최종 모델 성능은 K번의 검증 결과를 평균하여 얻습니다.


Chap 7 비지도 학습 (Unsupervised Learning)

    비지도 학습은 데이터에 레이블(정답)이 없는 상황에서 패턴이나 구조를 찾는 머신러닝 기법입니다. 비지도 학습의 목표는 데이터의 숨겨진 구조를 발견하는 것입니다. 주요 비지도 학습 방법에는 클러스터링과 차원 축소가 있습니다.
    비지도학습이라는 용어는 레이블이 달린 데이터를 이용해 모델을 학습하는 과정없이 데이터로부터 의미를 이끌어내는 통계적기법들을 의미한다.

    클러스터링 (Clustering)

    클러스터링은 데이터를 유사한 특성을 가진 그룹(클러스터)으로 나누는 작업입니다. 클러스터링의 목적은 같은 클러스터 내의 데이터 포인트들이 서로 비슷하고, 다른 클러스터의 데이터 포인트들과는 다르게 만드는 것입니다. 대표적인 클러스터링 알고리즘은 다음과 같습니다:

    1. K-평균 클러스터링 (K-means Clustering):
        데이터를 K개의 클러스터로 나누는 알고리즘입니다.
        각 클러스터는 클러스터의 중심(centroid)을 기준으로 정의되며, 데이터 포인트는 가장 가까운 중심에 할당됩니다.
        반복적인 과정(군집의 중심 업데이트)을 통해 중심이 더 이상 변하지 않을 때까지 수행합니다.

    2. 계층적 클러스터링 (Hierarchical Clustering):
        데이터 포인트들을 계층적인 구조로 클러스터링합니다.
        상향식 방법(agglomerative): 각 데이터 포인트를 개별 클러스터로 시작하여, 가까운 클러스터끼리 합치는 방식으로 진행합니다.
        하향식 방법(divisive): 모든 데이터를 하나의 클러스터로 시작하여, 점진적으로 분할하는 방식으로 진행합니다.

    3. DBSCAN (Density-Based Spatial Clustering of Applications with Noise):
        밀도 기반 클러스터링 알고리즘입니다.
        데이터 포인트의 밀도를 기준으로 클러스터를 형성하며, 밀도가 높은 영역을 클러스터로 정의합니다.
        클러스터의 형태에 민감하지 않고, 노이즈와 이상치를 효과적으로 처리할 수 있습니다.

    차원 축소 (Dimensionality Reduction)

        차원 축소는 고차원 데이터를 저차원 공간으로 변환하는 과정입니다. 이는 데이터의 중요한 정보를 유지하면서 차원을 줄여 데이터 시각화, 노이즈 제거, 계산 비용 감소 등의 목적을 달성할 수 있습니다. 주요 차원 축소 방법은 다음과 같습니다:

    주성분 분석 (PCA, Principal Component Analysis):
        데이터의 분산을 최대한 보존하는 방향으로 주성분을 찾습니다.
        주성분은 데이터의 분산이 가장 큰 방향을 나타내며, 상호 직교합니다.
        데이터의 차원을 줄이면서 중요한 정보를 유지할 수 있습니다.

    선형판별분석 (LDA, Linear Discriminant Analysis):
        클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 방향으로 변환합니다.
        주로 분류 문제에서 사용되며, 레이블이 있는 데이터에 적용됩니다.

    t-SNE (t-Distributed Stochastic Neighbor Embedding):
        고차원 데이터를 저차원으로 변환하여 시각화하는 데 주로 사용됩니다.
        데이터 포인트 간의 거리를 보존하면서 저차원 공간으로 매핑합니다.
        비선형 구조를 잘 보존하며, 데이터의 클러스터 구조를 시각화하는 데 효과적입니다.

7.1 주성분 분석 (PCA, Principal Component Analysis)

        주성분 분석은 고차원 데이터를 저차원으로 축소하는 차원 축소 기법입니다. 데이터의 분산을 최대한 보존하는 방향으로 주성분을 찾아 데이터의 중요한 정보를 유지합니다. PCA는 데이터의 상관관계를 고려하여 주성분을 생성하며, 주성분들은 상호 직교(orthogonal)합니다.

        **용어정리**
            *주성분(principal component): 예측변수들의선형결합
            *부하(loading) : 예측변수들을 성분으로 변형할떄 사용되는 기중치(유의어:가중치)
            *스크리그래프(screeplot) : 성분들의 변동을 표시한 그림, 설명된 분산 혹은설명된 분산의 비율을 이용하여 성분들의 상대적인 중요도를 보여준다.
            *비지도 학습: 레이블이 없는 데이터에서 패턴이나 구조를 찾는 학습 방법입니다.
            *클러스터링: 데이터를 유사한 특성을 가진 그룹으로 나누는 방법으로, K-평균, 계층적 클러스터링, DBSCAN 등이 있습니다.
            *차원 축소: 고차원 데이터를 저차원으로 변환하는 방법으로, PCA, LDA, t-SNE 등이 있습니다.

    7.1.2 주성분 계산 (Principal Component Calculation)
        주성분을 계산하는 과정은 다음과 같습니다:
            1.데이터 표준화:
                데이터의 각 변수에 대해 평균을 0, 분산을 1로 맞추는 표준화를 수행합니다. 이는 변수의 단위 차이를 제거하고, 모든 변수의 중요도를 동일하게 고려하게 합니다.
                표준화된 데이터 행렬 XX를 얻습니다.

            2.공분산 행렬 계산:
                표준화된 데이터 행렬 XX의 공분산 행렬 ΣΣ를 계산합니다.
                공분산 행렬 ΣΣ는 각 변수 간의 공변동(covariance)을 나타내며, 이는 Σ=1n−1XTXΣ=n−11​XTX로 계산됩니다.

            3.고유값 및 고유벡터 계산:
                공분산 행렬 ΣΣ의 고유값(eigenvalue)과 고유벡터(eigenvector)를 계산합니다.
                고유값은 공분산 행렬의 분산을 설명하는 비율을 나타내며, 고유벡터는 주성분 방향을 나타냅니다.

            4.주성분 선택:
                고유값을 내림차순으로 정렬하여 가장 큰 고유값에 대응하는 고유벡터를 선택합니다.
                선택된 고유벡터가 주성분을 형성합니다.

            5.주성분 변환:
                원래 데이터를 선택된 주성분으로 변환하여 새로운 저차원 공간에 투영합니다.
                이 새로운 좌표가 주성분 점수(principal component scores)입니다.

        공변동[covariation]
        공변동은 두 변수 간의 상관관계를 나타냅니다. 두 변수 XX와 YY의 공분산 Cov(X,Y)Cov(X,Y)는 다음과 같이 계산됩니다:

        Cov(X,Y)=1n−1∑i=1n(Xi−Xˉ)(Yi−Yˉ)Cov(X,Y)=n−11​∑i=1n​(Xi​−Xˉ)(Yi​−Yˉ)

        여기서 XˉXˉ와 YˉYˉ는 각각 변수 XX와 YY의 평균입니다. 공분산이 양수이면 두 변수는 양의 상관관계를 가지며, 음수이면 음의 상관관계를 가집니다.

        7.1.3 주성분 해석 (Principal Component Interpretation)

        주성분 해석은 각 주성분이 원래 변수들의 어떤 조합으로 구성되었는지를 이해하는 과정입니다. 각 주성분은 원래 변수들의 선형 결합으로 표현되며, 주성분의 방향은 고유벡터에 의해 결정됩니다. 주성분의 크기(즉, 주성분 점수)는 고유값에 의해 결정됩니다.

        주성분의 해석은 다음과 같은 단계로 이루어집니다:
            1.고유벡터: 각 고유벡터의 요소는 주성분이 원래 변수에 대한 기여도를 나타냅니다.
            2.주성분 점수: 주성분 점수는 주성분으로 변환된 데이터를 의미하며, 각 데이터 포인트가 주성분 공간에서의 위치를 나타냅니다.
            3.주성분의 분산: 각 주성분이 데이터의 전체 분산에서 차지하는 비율을 이해합니다. 이는 주성분이 데이터의 변동성을 얼마나 잘 설명하는지를 나타냅니다.
        
        스크리 그래프 (Scree Plot)

        스크리 그래프는 고유값의 크기를 내림차순으로 표시하여, 데이터의 중요한 주성분 개수를 시각적으로 선택하는 데 도움을 줍니다. 스크리 그래프에서 고유값의 급격한 감소가 나타나는 지점을 찾는 것이 중요합니다. 이 급격한 감소 이후의 고유값들은 데이터의 변동성을 잘 설명하지 못하므로, 해당 지점 이전의 주성분들을 선택하는 것이 일반적입니다.

        스크리 그래프를 그리는 방법:
            1.고유값을 내림차순으로 정렬합니다.
            2.고유값을 y축에, 주성분 번호를 x축에 표시합니다.
            3.고유값의 급격한 감소가 나타나는 지점을 찾습니다. 이 지점 이후의 주성분은 무시할 수 있습니다.
        
        7.1.4 대응분석
        대응분석의 개요
        대응분석은 주성분 분석(PCA)과 유사한 수학적 원리를 사용하여, 범주형 데이터의 행과 열을 각각 주성분으로 변환합니다. 이를 통해 복잡한 다차원 데이터를 시각적으로 이해할 수 있는 2차원 또는 3차원 공간으로 축소합니다. 대응분석은 특히 마케팅, 사회과학, 생물학 등 다양한 분야에서 사용됩니다.
        
        대응분석의 주요 단계

        1.교차표 구성:
            분석할 범주형 변수들로 구성된 교차표를 작성합니다. 교차표의 각 셀은 두 범주형 변수의 빈도 또는 비율을 나타냅니다.
        2.행과 열의 프로파일 계산:
            각 행의 프로파일은 행 합계를 기준으로 표준화된 확률 분포를 나타냅니다.
            각 열의 프로파일은 열 합계를 기준으로 표준화된 확률 분포를 나타냅니다.
        3.카이제곱 거리 계산:
            카이제곱 거리는 두 범주형 변수 간의 차이를 측정하는 데 사용됩니다. 이 거리는 각 셀의 관찰된 빈도와 기대 빈도 간의 차이의 제곱을 기대 빈도로 나누어 계산합니다.
        4.고유값과 고유벡터 계산:
            카이제곱 거리 행렬의 고유값과 고유벡터를 계산하여, 각 범주의 주성분을 구합니다. 이는 PCA에서 공분산 행렬을 사용하는 것과 유사합니다.
        5.차원 축소 및 시각화:
            계산된 고유값과 고유벡터를 사용하여 데이터를 저차원 공간에 투영합니다. 일반적으로 2차원 또는 3차원 공간으로 축소하여 시각화합니다.

    대응분석의 해석
        *행과 열의 좌표: 대응분석의 결과로 얻어진 행과 열의 좌표는 저차원 공간에서 각 범주형 변수의 관계를 나타냅니다. 가까운 거리에 위치한 행과 열은 높은 연관성을 가지며, 멀리 떨어진 행과 열은 낮은 연관성을 가집니다.
        *주성분의 중요도: 각 주성분의 중요도는 고유값의 크기로 결정되며, 이는 데이터의 분산을 얼마나 설명하는지를 나타냅니다. 중요한 주성분은 데이터의 구조를 잘 설명합니다.
        *그래프 해석: 대응분석 결과를 시각화한 그래프에서는 각 점이 범주형 변수의 범주를 나타내며, 점들 간의 거리는 범주들 간의 관계를 나타냅니다. 이러한 시각화는 데이터의 구조적 패턴을 이해하는 데 도움을 줍니다.

    대응분석과 주성분 분석의 비교
        *데이터 유형: PCA는 연속형 데이터를, 대응분석은 범주형 데이터를 분석합니다.
        *수학적 원리: 두 기법 모두 고유값 분해(eigen decomposition)를 사용하지만, PCA는 공분산 행렬을, 대응분석은 카이제곱 거리 행렬을 사용합니다.
        *목표: PCA는 데이터의 분산을 최대한 보존하면서 차원을 축소하는 데 초점을 맞추고, 대응분석은 범주형 변수 간의 관계를 시각화하는 데 중점을 둡니다.

    대응분석 요약
        대응분석은 범주형 데이터의 구조적 관계를 분석하고 시각화하는 강력한 도구입니다. 주성분 분석과 유사한 수학적 원리를 사용하지만, 범주형 변수에 특화되어 있습니다. 대응분석을 통해 범주형 데이터의 숨겨진 패턴과 관계를 쉽게 이해할 수 있습니다.

7-2 k-평균 클러스터링
        클러스터링은 데이터를 서로 다른 그룹으로 분류하는 기술을 말한다.
        클러스터링의 목적은 데이터로부터 유의미한 그룹들을 구하는것이다.
        
        **용어정리**
            클러스터(cluster):서로 유사한 레코드들의 집합
            클러스터 평균(cluster mean) : 한 클러스터안에 속한 레코드들의 평균 벡터변수
            k : 클러스터의 개수

        7.2.1 간단한 예제
        7.2.2 k-평균 알고리즘
        7.2.3 클러스터 해석
--------------------------------------------------------------------------------
## 2024-06.05 [수]
--------------------------------------------------------------------------------
7.3 계층적 클러스터링
    
    계층적 클러스터링(hierarchical clustering)은 데이터 포인트를 그룹화하는 방법 중 하나로, 두 가지 주요 접근 방식이 있습니다: 병합적(agglomerative) 방법과 분할적(divisive) 방법입니다.

    **용어정리**
    덴드로그램[dendrogram] : 레코드들, 그리고 레코드들이 속한 계층적 클러스터를 시각적으로 표현
    거리[distance] : 한 레코드가 다른 레코드들과 얼마나 가까운지를 보여주는 측정지표
    비유사도[dissimlarity] : 한 클러스터가 다른 클러스터들과 얼마나 가까운지를 보여주는 측정지표

    7.3.1 간단한 예제
    7.3.2 덴드로그램 (Dendrogram)

        *계층적 클러스터링의 결과를 시각적으로 표현하는 도구로, 클러스터링 과정에서 클러스터 간의 결합 순서를 나무 형태로 나타냅니다.
        *덴드로그램의 높이는 클러스터 간의 거리 또는 유사성을 나타내며, 클러스터의 결합 또는 분할을 이해하는 데 유용합니다.

        장점과 단점
        장점:

            클러스터 수를 미리 지정할 필요가 없습니다.
            클러스터 구조를 덴드로그램으로 시각화하여 이해하기 쉽습니다.
            여러 클러스터링의 가능성을 탐색할 수 있습니다.

        단점:
            계산 비용이 높습니다. (병합적 방법의 경우 O(n^3))
            큰 데이터셋에는 부적합할 수 있습니다.
            초기 데이터 포인트 간의 거리 측정에 민감합니다.

    7.3.3 병합 알고리즘

        병합적 계층적 클러스터링의 병합 과정은 다음과 같은 단계를 거칩니다

        초기화:
            * 각 데이터 포인트를 하나의 클러스터로 간주하여 시작합니다.

        반복 병합:
            * 모든 클러스터 간의 비유사도를 계산합니다.
            * 가장 비유사도가 작은 두 클러스터를 합칩니다.
            * 새로운 클러스터 간의 비유사도를 업데이트합니다.
            * 모든 데이터가 하나의 클러스터가 될 때까지 이 과정을 반복합니다.

    7.3.4 비유사도 측정

        비유사도(Dissimilarity)는 계층적 클러스터링에서 두 클러스터 간의 거리를 측정하는 방법입니다. 각 비유사도 측정 방법은 클러스터를 병합하는 방식에 영향을 미치며, 클러스터의 형태와 구조에 큰 영향을 줍니다. 여기서는 완전연결, 단일연결, 평균연결, 최소분산 방법에 대해 자세히 설명하겠습니다.

        비유사도 측정 방법 요약

            *단일연결: 가장 가까운 두 점의 거리 사용, 긴 클러스터 형성 가능.
            *완전연결: 가장 먼 두 점의 거리 사용, 더 구형 클러스터 형성.
            *평균연결: 모든 점 쌍의 평균 거리 사용, 중립적인 클러스터 형성.
            *워드기법 (Ward's Method): 클러스터 내 분산 최소화, 구형 클러스터 형성

7.4 모델 기반 클러스터링
    모델 기반 클러스터링(Model-based Clustering)은 데이터가 특정 통계적 분포를 따른다고 가정하고 클러스터를 찾는 방법입니다. 여기서 주로 사용되는 모델 중 하나가 정규혼합 모델(Gaussian Mixture Model, GMM)입니다. 이 모델은 각 클러스터가 다변량 정규 분포를 따른다고 가정합니다.

    *정규혼합 모델 (Gaussian Mixture Model, GMM)
        GMM은 데이터가 여러 개의 다변량 정규 분포(가우시안 분포)의 혼합으로 구성되어 있다고 가정합니다. 각 가우시안 분포는 하나의 클러스터를 나타냅니다. GMM은 클러스터의 평균과 공분산을 통해 각 클러스터를 모델링합니다.

    7.4.1 다변량정규분포
        다변량 정규 분포(Multivariate Normal Distribution)는 여러 변수들이 상호 종속적으로 정규 분포를 따르는 분포입니다. 이는 단변량 정규 분포의 확장으로, 다차원 공간에서 정규 분포를 모델링합니다.

    7.4.2 정규혼합
    7.4.3 클러스터 개수 결정하기
        정규혼합 모델(Gaussian Mixture Model, GMM)에서 클러스터 개수를 결정하는 방법 중 하나로 베이지안 정보 기준(Bayesian Information Criterion, BIC)을 사용할 수 있습니다. BIC는 모델의 적합도와 복잡성 사이의 균형을 고려하여 최적의 모델을 선택하는 데 도움을 줍니다.
    
    베이즈 정보 기준 (BIC)

        *BIC는 모델의 로그 가능도(log-likelihood)에 모델의 복잡성을 고려하는 페널티(term)를 추가하여 계산됩니다. BIC 값이 낮을수록 모델이 더 적합하다고 간주합니다.

    주요개념
        *클러스터들이 각자 서로 다른 확률분포로부터 발생한 것으로 가정한다.
        *분포(일반적으로 정규분포) 개수에 대한 가정에 따라 서로 다른 적합한 모델이 있다.
        *이 방법은 너무 많은 파라미터(오버피팅의 원인이 될 수 있다)를 사용하지않으면서도 데이터에 적합한 모델을 선택한다.

7.5 스케일링과 범주형 변수

        **용어정리**
        *스케일링(scaling): 데이터의 범위를 늘리거나 줄이는 방식으로 여러 변수들이 같은 스케일에 오도록 하는 것
        *정규화(normalization): 원래 변수값에서 평균을 뺀 후에 표준편차로 나누는 방법으로, 스케일링의 일종이다.
        *고워거리(Gower's distance): 수치형과 범주형 데이터가 섞여있는 경우에 모든 변수가 0~1 사이로 오도록 하는 스케일링 방법.

    7.5.1 변수 스케일링 (Feature Scaling)

    변수 스케일링은 데이터의 특성(feature) 간에 존재하는 값의 범위 차이를 조정하는 과정입니다. 일반적으로 스케일링을 수행하는 이유는 다음과 같습니다:

        1.모델 성능 향상: 일부 머신러닝 알고리즘은 데이터의 스케일에 민감합니다. 스케일이 큰 특성은 모델 학습에 더 많은 영향을 미칠 수 있습니다.
        2.수렴 속도 향상: 경사 하강법과 같은 최적화 알고리즘은 스케일에 민감하여 스케일링을 통해 수렴 속도를 높일 수 있습니다.

    주요 스케일링 기법은 표준화(Standardization), 정규화(Normalization), 최소-최대 스케일링(Min-Max Scaling) 등이 있습니다.

    7.5.2 지배변수 (Dominant Variable)

    지배변수는 클러스터링 분석에서 가장 중요한 역할을 하는 변수를 가리킵니다. 지배변수를 식별하는 것은 클러스터링 결과를 해석하는 데 도움이 됩니다. 주로 주성분 분석(PCA)을 사용하여 지배변수를 식별하거나, 도메인 지식을 활용하여 중요한 변수를 선정합니다.

    7.5.3 범주형 데이터와 고워 거리
        범주형 데이터와 거리 측정

    범주형 데이터는 이산적인 값을 가지는 변수로, 거리 기반의 클러스터링 알고리즘에서는 직접적으로 사용할 수 없습니다. 이를 해결하기 위해 범주형 데이터를 숫자로 변환하는 인코딩 방법이 필요합니다. 일반적으로 원-핫 인코딩(One-Hot Encoding)을 사용하여 범주형 데이터를 이진 벡터로 변환합니다. 이를 통해 범주 간의 거리를 측정할 수 있습니다.

    7.5.4 혼합데이터의 클러스터링 문제

    혼합데이터는 수치형 데이터와 범주형 데이터가 혼합된 데이터를 의미합니다. 이러한 데이터를 클러스터링하는 것은 일반적인 클러스터링 문제보다 더 복잡합니다. 범주형 데이터를 어떻게 수치형 데이터와 함께 처리할지가 중요한 문제입니다. 일반적으로는 수치형 데이터는 변수 스케일링을 적용하고, 범주형 데이터는 원-핫 인코딩을 적용하여 데이터를 전처리한 후 클러스터링을 수행합니다.

    이러한 혼합데이터의 클러스터링 문제를 해결하기 위해 클러스터링 알고리즘을 수정하여 범주형 데이터를 처리할 수 있는 기능을 추가하는 경우도 있습니다. 예를 들어, k-평균 알고리즘의 확장인 k-평균 알고리즘 with Gower distance는 범주형 데이터를 고려한 거리 측정 방법을 사용하여 혼합데이터를 클러스터링할 수 있습니다.


    주요개념
        * 스케일이 서로 다른 변수들을 스케일이 비슷하도록 변환하여, 스케일이 알고리즘에 큰 영향을 미치지않도록한다.
        * 일반적인 스케일링 방법은 각 변수에서 평균을 빼고 표준편차로 나눠주는 정규화(표준화) 방법이다.
        * 또 다른 방법은 고워 거리를 사용하는 것이다. 이 방법은 모든변수를 0~1 범위로 스케일링한다.(수치형과 범주형데이터가 서로혼합된 경우에 많이 사용된다)
        * 변수 스케일링은 데이터 전처리의 중요한 단계이며, 클러스터링 분석에서 지배변수는 해석력을 높이는 데 도움이 됩니다. 범주형 데이터와 거리 측정 문제는 적절한 인코딩 방법을 선택하여 해결할 수 있으며, 혼합데이터의 클러스터링 문제는 데이터 전처리와 알고리즘 수정을 통해 처리할 수 있습니다.

--------------------------------------------------------------------------------
## 2024-06.07 [금]
--------------------------------------------------------------------------------

- Kaggle - Kaggle 정의+설명 후, 문제 풀기

- Colab을 사용하여 오버피팅, 변수종요성, 타이타닉, 하우스 프라이스 문제 풀어보기 실습.
- overfitting.ipyb
- xgboost_py.ipyb
- kaggle.Titanic1.ipyb
- kaggle.Titanic2.ipyb

- 오후 위니드소프트 [회사소개 - 대표]
    바이오클러스터 = 구축을 위해 사업진행하는 회사
    SI/SW 사람중심으로 가는, 직원이 중심인 회사 - 가치관 : 휴머니즘
    개발족은 모든 연구비가 대부분 인건비로 쓰이기에, 
    인건비를 줄이는 AI가 출시될경우 시장이 많이 줄것이다.

    회사 소개 결론 : 산출물을 항상 작성하라.
    회사 선배를 누구 만나느냐에 따라서 변할 수 있다.
    내가 검증가능한 기록문서를 꼭 남겨라 [ 산출물 ]
    내가 개발자로 되기로 했었던 동기를 잃지마라.
    개개인의 역랴은 회사가 절대 키워주지못한다,
    스스로 따로 시간을 내어서 꾸준히 개개인의 역량을 키워야 발전을 할것이다.

--------------------------------------------------------------------------------
## 2024-06.10 [월]
--------------------------------------------------------------------------------
지난주 수업에 이어서, 주택가격맞추기 문제 실습.
    - house_price1.ipyb
    - house_price2.ipyb

파이썬 머신러닝 시작 - 데어터과학

Chap01 - 머신러닝의 기초
    01 머신러닝이란?
        - 머신러닝이란 기계가 패턴을 학습하여 자동화하는 알고리즘
        - 알고리즘이란 어떠한 문제를 해결하기 위한 일련의 절차나 방법
    02 머신러닝의 학습 프로세스와 종류
    03 머신러닝 연대기
    04 머신러닝 환경 구축하기
    05 머신러닝 모델링
        -파이썬으로 머신러닝 환경구축을 하는 이유?
            스크립트언어, 쉬운언어, 머신러닝과 딥러닝의 표준언어
        - 인터프리터
            아나콘다, 미니콘다 
        - 코드 편집기
            주피터, VS코드
        - 통계분석 및 전처리
            넘파이, 판다스, 사이파이
        - 시각화 도구
            맷플롯립, 시본, 플롯리
        -머신러닝 프레임워크
            사이킷런
---------------------------------------------------------------------------------------------------------------------------------------------
Chap02 - 데이터의 이해
    01 피쳐란?
        - 피쳐란 = 데이터의 이해

            특성(Feature)은 머신러닝에서 데이터의 중요한 속성이나 측정값을 나타내는 변수입니다. 각 특성은 데이터셋의 특정 측면을 나타내며, 모델이 예측하거나 분류할 때 사용됩니다.

            ## 특성의 중요성

            특성은 머신러닝 모델의 성능에 큰 영향을 미칩니다. 적절한 특성을 선택하고 변환하는 과정은 모델의 예측 성능을 높이는 데 중요한 역할을 합니다.

            1. **특성 선택 (Feature Selection)**: 데이터셋에서 유의미한 특성을 선택하고 불필요한 특성을 제거하는 과정입니다. 이는 모델의 복잡도를 줄이고, 과적합을 방지하며, 모델의 해석력을 높입니다.
            2. **특성 변환 (Feature Transformation)**: 특성을 변환하여 모델의 성능을 개선하는 과정입니다. 예를 들어, 로그 변환, 표준화, 정규화 등이 있습니다.

            ## 특성 선택 방법

            특성 선택은 모델의 성능을 향상시키고 과적합을 방지하는 데 중요한 과정입니다. 다음은 몇 가지 주요 방법입니다:

            ### 필터 방법 (Filter Method)

            통계적 방법을 사용하여 중요하지 않은 특성을 제거합니다. 예를 들어, 분산이 낮은 특성을 제거하거나, 상관관계가 낮은 특성을 선택하는 방법이 있습니다.

            ### 랩퍼 방법 (Wrapper Method)

            모델 성능을 기준으로 특성을 선택합니다. 반복적으로 모델을 학습하고 평가하여 최적의 특성 집합을 찾습니다. 예를 들어, 전진 선택, 후진 제거, 단계적 선택 방법이 있습니다.

            ### 임베디드 방법 (Embedded Method)

            모델 훈련 과정에서 특성 선택을 수행합니다. 예를 들어, 결정 트리 기반 모델은 훈련 과정에서 특성의 중요도를 평가하고 선택합니다.

            ## 특성 변환 방법

            특성 변환은 모델의 성능을 개선하고 데이터의 분포를 조정하는 데 중요한 과정입니다. 다음은 몇 가지 주요 방법입니다:

            ### 표준화 (Standardization)

            특성의 평균을 0으로, 분산을 1로 조정합니다. 이는 서로 다른 스케일을 가진 특성들을 동일한 스케일로 맞추는 데 유용합니다.

            ```python
            from sklearn.preprocessing import StandardScaler

            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)


        - 피쳐의표기법 = 데이터 테이블 [DT, DF]
        - 차원의 저주 (Curse of Dimensionality)
            차원의 저주는 머신러닝과 데이터 분석에서 고차원 데이터가 가지는 문제들을 설명하는 용어입니다. 데이터의 차원이 증가할수록 발생하는 다양한 문제들을 다룹니다.

        ## 차원의 저주란?

        차원의 저주는 고차원 공간에서 데이터 분석 및 모델링이 어려워지는 현상을 의미합니다. 주요 문제는 다음과 같습니다:

            1. **데이터 희소성**: 차원이 증가할수록 데이터 포인트들이 고차원 공간에서 매우 희소하게 분포하게 됩니다. 이는 데이터 포인트들 간의 거리가 멀어지고, 밀집된 구조를 찾기 어렵게 만듭니다.
            2. **모델 과적합**: 고차원 데이터는 모델이 데이터의 노이즈까지 학습하게 되어 과적합(overfitting)될 가능성이 높습니다. 이는 모델이 훈련 데이터에서는 높은 성능을 보이지만, 새로운 데이터에서는 성능이 저하되는 문제를 일으킵니다.
            3. **계산 복잡도 증가**: 차원이 증가하면 계산 복잡도가 기하급수적으로 증가합니다. 이는 모델 훈련과 예측에 많은 시간과 자원을 소모하게 만듭니다.

        ## 차원의 저주 해결 방법

        차원의 저주를 해결하거나 완화하기 위해 여러 가지 방법을 사용할 수 있습니다:

        ### 1. 차원 축소 (Dimensionality Reduction)

        차원 축소 기법을 사용하여 고차원 데이터를 저차원으로 변환할 수 있습니다.

            - **주성분 분석 (PCA)**: 데이터의 분산을 최대한 보존하면서 저차원으로 변환합니다.
            - **선형판별분석 (LDA)**: 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 방향으로 데이터를 변환합니다.
            - **t-SNE, UMAP**: 고차원 데이터를 시각화 목적으로 저차원으로 변환하는 비선형 방법들입니다.

        ### 2. 특성 선택 (Feature Selection)

        모델 성능에 크게 기여하지 않는 불필요한 특성을 제거합니다.

        - **필터 방법**: 통계적 방법을 사용하여 중요하지 않은 특성을 제거합니다.
        - **랩퍼 방법**: 모델 성능을 기준으로 특성을 선택합니다.
        - **임베디드 방법**: 모델 훈련 과정에서 특성 선택을 수행합니다.

        ### 3. 정규화 및 규제 (Normalization and Regularization)

        모델의 복잡도를 줄여 과적합을 방지합니다.

        - **L1, L2 정규화**: 모델이 과적합되지 않도록 패널티를 추가하여 학습합니다.
        - **드롭아웃**: 뉴럴 네트워크에서 과적합을 방지하기 위해 일부 노드를 임의로 학습에서 제외합니다.

        ## 예시: PCA를 사용한 차원 축소

        다음은 Python을 사용하여 주성분 분석(PCA)을 통해 차원 축소를 수행하는 예제입니다:

            ```python
            import numpy as np
            import matplotlib.pyplot as plt
            from sklearn.decomposition import PCA
            from sklearn.datasets import load_digits

            # 데이터 로드
            digits = load_digits()
            X = digits.data
            y = digits.target

            # PCA를 사용한 차원 축소
            pca = PCA(n_components=2)
            X_reduced = pca.fit_transform(X)

            # 차원 축소된 데이터 시각화
            plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.title('PCA of Digits Dataset')
            plt.colorbar()
            plt.show()

    02 피쳐의 종류
        연속형 데이터
        이산형 데이터

        # 데이터 유형 (Data Types)

        데이터는 그 특성과 측정 방식에 따라 연속형 데이터와 이산형 데이터로 분류할 수 있습니다.

        ## 연속형 데이터 (Continuous Data)

        연속형 데이터는 특정 범위 내에서 연속적으로 측정될 수 있는 값을 가지는 데이터입니다. 이는 실수(real number)로 나타낼 수 있으며, 측정 단위가 무한히 작아질 수 있어 더 정밀하게 측정될 수 있습니다.

        ### 특징
        - 값들이 연속적으로 분포하며, 중간값이 존재할 수 있습니다.
        - 일반적으로 측정 단위가 있어 더 정밀하게 측정 가능합니다.
        - 예시: 23.4도(온도), 5.67km(거리), 12.8kg(무게)

        ## 이산형 데이터 (Discrete Data)

        이산형 데이터는 특정 값들만을 가질 수 있는 데이터로, 정수(integer)로 나타낼 수 있습니다. 이는 값들 사이에 간격이 있으며, 중간값이 존재하지 않습니다.

        ### 특징
        - 값들이 불연속적으로 분포하며, 중간값이 존재하지 않습니다.
        - 일반적으로 정수로 표현됩니다.
        - 예시: 3명(사람 수), 5개(개수), 6번(주사위 눈금)

        ## 예시

        아래는 연속형 데이터와 이산형 데이터를 Python 코드로 생성하고 시각화하는 예제입니다:

        ```python
        import numpy as np
        import matplotlib.pyplot as plt

        # 연속형 데이터 생성
        continuous_data = np.random.normal(loc=0, scale=1, size=1000)

        # 이산형 데이터 생성
        discrete_data = np.random.randint(low=0, high=10, size=1000)

        # 연속형 데이터 시각화
        plt.figure(figsize=(12, 6))

        plt.subplot(1, 2, 1)
        plt.hist(continuous_data, bins=30, color='blue', alpha=0.7)
        plt.title('Continuous Data Distribution')
        plt.xlabel('Value')
        plt.ylabel('Frequency')

        # 이산형 데이터 시각화
        plt.subplot(1, 2, 2)
        plt.hist(discrete_data, bins=10, color='green', alpha=0.7, rwidth=0.8)
        plt.title('Discrete Data Distribution')
        plt.xlabel('Value')
        plt.ylabel('Frequency')

        plt.tight_layout()      
        plt.show()

    03 데이터를 모델에 대입하기
        - 판다스
        # Pandas를 사용한 데이터 전처리 및 모델에 대입하기

        Pandas는 Python에서 데이터 분석 및 조작을 위한 강력한 라이브러리입니다. 이 가이드에서는 Pandas를 사용하여 데이터를 로드하고 전처리한 후, 머신러닝 모델에 대입하는 방법을 설명합니다.

        ## 요구 사항
            - Python 3.x
            - Pandas
            - Scikit-learn

            필요한 라이브러리를 설치하려면 다음 명령을 사용하십시오:

            ```bash
            pip install pandas scikit-learn

---------------------------------------------------------------------------------------------------------------------------------------------
Chap06 데이터 전처리

    데이터 전처리의 개념
        - 머신러닝 모델에 훈련 데이터를 입력하기 전에 가공하는 단계를 말한다.

    1. 데이터 정리(Cleaning): 결측값, 이상치(outlier) 및 중복 데이터를 처리합니다.
    2. 데이터 변환(Transformation): 데이터를 적절한 형식으로 변환합니다. 예를 들어, 범주형 데이터를 수치형으로 변환하거나, 로그 변환 등을 수행합니다.
    3. 데이터 표준화(Standardization) 및 정규화(Normalization): 데이터를 일정한 범위로 조정하여 모델이 균일하게 데이터를 처리할 수 있도록 합니다.
    4. 특성 선택(Feature Selection) 및 추출(Feature Extraction): 모델의 성능을 향상시키기 위해 중요한 특성을 선택하거나 새로운 특성을 생성합니다.

    데이터 품질 문제
        -데이터 분포의 지나친 차이, 기수형 데이터와 서수형 데이터, 결측치, 이상치
    데이터 전처리의 기초
        1.결측값 처리: 결측값을 제거하거나 대체합니다.
        2.이상치 처리: 이상치를 탐지하고 제거하거나 수정합니다.
        3.데이터 인코딩: 범주형 데이터를 수치형으로 변환합니다.
        4.데이터 스케일링: 데이터를 표준화 또는 정규화합니다.
    01 데이터 전처리의 기초
        기수형 데이터(Nominal Data)
            -기수형 데이터는 범주형 데이터의 한 종류로, 명목적 범주를 나타내며 순서가 없는 데이터입니다. 예를 들어, 색상(빨강, 파랑, 녹색), 성별(남성, 여성) 등이 기수형 데이터에 해당합니다.
        
        서수형 데이터(Ordinal Data)
            -서수형 데이터는 범주형 데이터의 한 종류로, 순서가 있는 데이터입니다. 하지만 이 순서 사이의 간격은 의미가 없거나 일정하지 않을 수 있습니다. 예를 들어, 만족도 조사(매우 불만족, 불만족, 보통, 만족, 매우 만족), 학점(A, B, C, D, F) 등이 서수형 데이터에 해당합니다.
        
        결측치(Missing Values)
            - 결측치는 데이터셋에서 누락된 값을 의미합니다. 결측치는 다양한 원인으로 발생할 수 있으며, 이를 처리하지 않으면 모델의 성능이 저하될 수 있습니다.
        
        이상치(Outliers)
            - 이상치는 데이터에서 다른 값들과 크게 벗어난 값들을 의미합니다. 이상치는 모델 성능에 영향을 미칠 수 있으므로 적절히 처리해야 합니다.
            
    02 데이터 전처리의 전략
        1.결측치 처리하기 : 드롭과 채우기
            1.1 드롭[drop]
            1.2 채우기[fillna]
        2. 범주형 데이터 처리하기 : 원핫인코딩
            - [get_dummies]
        3. 범주형 데이터 변환하여 처리하기 : 바인딩
            - [binding]
        4. 데이터의 크기 맞추기 : 피쳐 스케일링
            - [feature scaling]

    03 데이터 전처리의 실습
        데이터 전처리 : 데이터 전처리는 머신러닝 프로젝트의 핵심 단계 중 하나로, 데이터를 모델 학습에 적합한 형태로 준비하는 과정을 의미합니다. 이 단계에서는 원시 데이터를 정제하고 변환하여, 모델이 보다 효과적으로 학습할 수 있도록 합니다. 데이터 전처리는 데이터의 품질과 모델의 성능에 큰 영향을 미치기 때문에 매우 중요합니다.
        * 데이터 전처리의 중요성
            1. 모델 성능 향상: 적절한 전처리를 통해 모델의 성능을 향상시킬 수 있습니다.
            2. 데이터의 일관성 유지: 결측치, 이상치, 스케일 차이 등을 처리하여 데이터의 일관성을 유지합니다.
            3. 모델의 안정성 강화: 데이터 전처리를 통해 모델의 과적합(overfitting)을 방지할 수 있습니다.

        이와 같은 데이터 전처리 단계를 통해 데이터를 최적화하고, 머신러닝 모델의 학습 효율성과 예측 정확도를 높일 수 있습니다. 각 단계에서 적절한 방법을 선택하고, 필요한 경우 추가적인 데이터 분석과 변환을 수행하는 것이 중요합니다.

        1. 머신러닝 프로세스와 데이터 전처리
            머신러닝 프로세스는 데이터를 준비하고 모델을 학습하여 예측 또는 분류 문제를 해결하는 일련의 단계로 구성됩니다. 
            
            * 머신러닝 프로세스
                            
                이 문서는 머신러닝 프로젝트를 수행하는 데 필요한 주요 단계와 그 내용을 표로 정리한 것입니다. 각 단계에서 수행해야 할 작업들을 쉽게 확인할 수 있습니다.

                ## 머신러닝 프로세스 개요
                | 단계                | 설명                                                                  |
                |---------------------|----------------------------------------------------------------------|
                | **1. 문제 정의 및 목표 설정** | 문제를 정의하고 해결하고자 하는 목표를 명확히 설정합니다. |
                | **2. 데이터 수집**       | 필요한 데이터를 수집합니다. 데이터 소스는 CSV, Excel, 데이터베이스, 웹 API 등 다양합니다. |
                | **3. 데이터 전처리**     | 데이터를 정제하고 변환하여 모델 학습에 적합한 형태로 준비합니다. |
                | **4. 탐색적 데이터 분석 (EDA)** | 데이터를 시각화하고 통계적으로 분석하여 데이터의 특성을 이해합니다. |
                | **5. 특성 선택 및 엔지니어링** | 모델 학습에 사용할 중요한 특성을 선택하거나 새로운 특성을 생성합니다. |
                | **6. 모델 선택 및 학습**  | 문제에 적합한 머신러닝 알고리즘을 선택하고, 데이터를 사용하여 모델을 학습시킵니다. |
                | **7. 모델 평가**         | 학습된 모델을 검증 데이터 또는 테스트 데이터를 사용하여 평가합니다. 다양한 성능 지표를 사용하여 모델의 성능을 측정합니다. |
                | **8. 모델 튜닝 및 개선**  | 하이퍼파라미터 튜닝, 특성 엔지니어링, 데이터 증강 등을 통해 모델의 성능을 개선합니다. |
                | **9. 모델 배포**         | 최종 모델을 프로덕션 환경에 배포하여 실제 데이터에 대해 예측을 수행합니다. |
                | **10. 모니터링 및 유지보수** | 배포된 모델의 성능을 지속적으로 모니터링하고, 데이터의 변화나 모델의 성능 저하에 따라 모델을 재훈련하거나 업데이트합니다. |

                ## 데이터 전처리 단계
                | 단계                    | 설명                                                                  |
                |-------------------------|----------------------------------------------------------------------|
                | **1. 데이터 로드**        | 데이터를 불러옵니다. CSV 파일, Excel 파일, 데이터베이스 등을 사용할 수 있습니다. |
                | **2. 기초 정보 확인**     | 데이터의 크기, 변수의 데이터 타입, 결측치 여부 등을 확인합니다. |
                | **3. 결측치 처리**        | 결측치를 확인하고, 삭제하거나 평균값 등으로 대체합니다. |
                | **4. 이상치 처리**        | 이상치를 탐지하고, 제거하거나 대체합니다. 박스플롯, IQR 방법 등을 사용할 수 있습니다. |
                | **5. 데이터 변환**        | 범주형 데이터를 인코딩하거나, 수치형 데이터를 스케일링합니다. |
                | **6. 특성 엔지니어링**    | 새로운 특성을 생성하거나 기존 특성을 변환하여 모델의 성능을 향상시킵니다. |
                | **7. 데이터 분할**        | 학습용과 테스트용 데이터를 분할합니다. |

                ### 데이터 전처리 코드 예시
                ```python
                import pandas as pd
                from sklearn.model_selection import train_test_split
                from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder

                # 데이터 로드
                df = pd.read_csv('data.csv')

                # 결측치 처리
                df.fillna(df.mean(), inplace=True)

                # 이상치 처리 (IQR 방법)
                Q1 = df['column_name'].quantile(0.25)
                Q3 = df['column_name'].quantile(0.75)
                IQR = Q3 - Q1
                filter = (df['column_name'] >= Q1 - 1.5 * IQR) & (df['column_name'] <= Q3 + 1.5 * IQR)
                df = df.loc[filter]

                # 범주형 데이터 인코딩
                label_encoder = LabelEncoder()
                df['categorical_column'] = label_encoder.fit_transform(df['categorical_column'])
                df = pd.get_dummies(df, columns=['categorical_column'])

                # 데이터 스케일링
                scaler = StandardScaler()
                df[['numeric_column']] = scaler.fit_transform(df[['numeric_column']])

                normalizer = MinMaxScaler()
                df[['numeric_column']] = normalizer.fit_transform(df[['numeric_column']])

                # 데이터 분할
                X = df.drop('target', axis=1)
                y = df['target']
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


        2. 데이터 전처리 실습하기 : 타이타닉 생존자 예측하기
            01.실제데이터확보하기
            02.연습용 데이터 확인하기
            03.데이터 열 확인하기
            04.데이터 노트 작성하기
            05.결측치 확인하기
            06.범주형 데이터 처리하기
            07.데이터 시각화 진행하기

---------------------------------------------------------------------------------------------------------------------------------------------
머신러닝 회귀기법[매우중요!!]

chap07 선형회귀의 기초
    # 선형회귀 (Linear Regression)

    선형회귀는 가장 기본적이고 널리 사용되는 회귀 분석 기법으로, 종속 변수와 독립 변수 간의 선형 관계를 모델링합니다. 이 문서에서는 선형회귀의 기본 개념과 수식, 구현 방법 등을 간단히 설명합니다.

    ## 1. 선형회귀의 정의

    선형회귀는 종속 변수 y와 독립 변수 x 간의 관계를 다음과 같은 직선 방정식으로 모델링합니다:

    y = β0 + β1 x + ε

    여기서:
    - y : 종속 변수 (예측하려는 값)
    - x : 독립 변수 (예측에 사용되는 변수)
    - β0 : 절편 (y절편)
    - β1 : 기울기 (회귀 계수)
    - ε : 오차 항 (잔차)

    ## 2. 목적

    선형회귀 모델의 목적은 주어진 독립 변수 x에 대해 종속 변수 y를 가장 잘 예측할 수 있는 β0와 β1 값을 찾는 것입니다.

    ## 3. 비용 함수 (손실 함수)

    가장 일반적인 비용 함수는 평균 제곱 오차 (Mean Squared Error, MSE)입니다:

    MSE = (1/n) ∑ (yi - (β0 + β1 xi))^2

    ## 4. 최소제곱법 (Least Squares Method)

    최소제곱법을 사용하여 비용 함수를 최소화하는 β0와 β1 값을 찾습니다.

    β1 = ∑ ((xi - x̄)(yi - ȳ)) / ∑ ((xi - x̄)^2)

    β0 = ȳ - β1 · x̄

    ## 5. 가설 검정

    회귀 계수 β1이 0이 아닌지 검정하는 가설 검정을 수행합니다.

    - 귀무가설 (H0): β1 = 0
    - 대립가설 (H1): β1 ≠ 0

    ## 6. 결정계수 (R-squared)

    모델의 설명력을 평가하는 지표로 결정계수 R^2를 사용합니다:

    R^2 = 1 - (∑ (yi - (β0 + β1 xi))^2) / (∑ (yi - ȳ)^2)

    ## 7. 선형회귀의 가정

    - 선형성: 독립 변수와 종속 변수 사이에 선형 관계가 존재해야 합니다.
    - 독립성: 오차 항 ε이 독립적이어야 합니다.
    - 등분산성 (Homoscedasticity): 오차 항의 분산이 일정해야 합니다.
    - 정규성: 오차 항이 정규분포를 따라야 합니다.

    ## 8. 예제 코드 (Python)

    ```python
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    import matplotlib.pyplot as plt

    # 데이터 생성
    np.random.seed(0)
    X = 2 * np.random.rand(100, 1)
    y = 4 + 3 * X + np.random.randn(100, 1)

    # 데이터 분할
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 선형회귀 모델 생성
    model = LinearRegression()
    model.fit(X_train, y_train)

    # 예측
    y_pred = model.predict(X_test)

    # 결과 출력
    print(f"기울기 (beta1): {model.coef_[0][0]}")
    print(f"절편 (beta0): {model.intercept_[0]}")
    print(f"평균제곱오차 (MSE): {mean_squared_error(y_test, y_pred)}")
    print(f"결정계수 (R^2): {r2_score(y_test, y_pred)}")

    # 시각화
    plt.scatter(X_test, y_test, color='blue', label='Actual data')
    plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regression line')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('선형회귀')
    plt.legend()
    plt.show()

    01 선형회귀의 이해
    02 선형회귀의 기초 수식
    03 최소자승법으로 선형회귀 풀기

    04 경사하강법으로 선형회귀 풀기
        경사하강법(Gradient Descent)은 함수의 최솟값을 찾기 위한 반복적인 최적화 알고리즘입니다. 머신러닝과 딥러닝에서 매우 중요한 역할을 하며, 특히 손실 함수(loss function)의 최소값을 찾기 위해 사용됩니다. 
        경사하강법은 함수의 기울기(gradient)를 이용하여 파라미터(가중치)를 업데이트하며, 최적화 과정을 통해 모델의 성능을 개선합니다.
    
    경사하강법의 원리
        목표 함수: 최적화하려는 함수 J(θ)J(θ), 예를 들어 평균 제곱 오차(MSE)와 같은 손실 함수입니다.
        기울기 계산: 함수의 기울기를 계산합니다. 기울기는 함수의 변화를 나타내는 벡터로, 각 파라미터의 변화율을 제공합니다. 함수 J(θ)J(θ)의 기울기는 다음과 같이 계산됩니다:
                        ∇J(θ)=[∂J(θ)∂θ1,∂J(θ)∂θ2,…,∂J(θ)∂θn]
                        ∇J(θ)=[∂θ1​∂J(θ)​,∂θ2​∂J(θ)​,…,∂θn​∂J(θ)​]
        파라미터 업데이트: 기울기를 사용하여 파라미터를 업데이트합니다. 파라미터 θθ는 다음과 같이 업데이트됩니다:
                                    θ:=θ−α∇J(θ)   
        여기서 αα는 학습률(learning rate)로, 파라미터 업데이트의 크기를 조절합니다. 학습률이 너무 크면 발산할 수 있고, 너무 작으면 수렴 속도가 느려질 수 있습니다.
        경사하강법의 단계
            1.초기화: 파라미터 θθ를 초기값으로 설정합니다.
            2.기울기 계산: 현재 파라미터 θθ에서 함수의 기울기를 계산합니다.
            3.파라미터 업데이트: 기울기에 학습률을 곱한 값을 빼줍니다.
            4.반복: 반복적으로 2와 3 단계를 수행하여 함수의 최솟값을 찾습니다. 이 과정을 수렴할 때까지 계속합니다.

    요약
        * 경사하강법은 함수의 최솟값을 찾기 위한 반복적인 최적화 알고리즘입니다.
        * 기울기를 계산하여 파라미터를 업데이트합니다.
        * 학습률(alpha)은 파라미터 업데이트의 크기를 조절하는 중요한 하이퍼파라미터입니다.
        * 배치 경사하강법, 확률적 경사하강법, 미니배치 경사하강법 등 다양한 변형이 존재합니다.

    05 선형회귀 성능 측정하기
    06 코드로 선형회귀 구현하기

    **요약**
    1.선형회귀 : 종속변수 y와 한개 이상의 독립변수 x와의 선형상관관계를 모델링하는 회귀분석 기법이다.
    2.비용함수 : 머신러닝에서 최소화해야 할 예측값과 실제값의 차이로 다음과 같이 표현한다.
    3.최소자승법 : 선형대수 표기법을 사용하여 방정석으로 선형회귀 문제를 푸는방법
    경사하강법 : 경사를 하강하면서 수식을 최소화하는 매겨변수의 값을 찾아내는 방법
    4.홀드아웃 메서드 : 일반적인 데이터 분할 기법으로, 모델생성을 위해 전체 데이터에서 랜덤하게 학습 데이터셋과 테스트 데이터셋으로 나눈다. 데이터를 나누는 비율은 데이터의 개수마다 다른데, 일반적으로 7:3 또는 8:2정도의 비율로 나눈다.
    선형회귀 성능측정지표 
        *MAE(Mean Absolute Error): 평균 절대 단차로, 예측값과 실제값의 차이를절댓값으로 표현하는 지표이다.
        *RMSE(Root Mean Squared Error): 평균제곱근 오차로, 오차에 대해 제곱한 다음 모든값을 더하여 평균을 낸 후 제곱근을 구하는 방식이다.
        *결정계수(R-squared): 두 개의 값의 증감이 얼마나 일관성을 가지는지 나타내는 지표이다.
