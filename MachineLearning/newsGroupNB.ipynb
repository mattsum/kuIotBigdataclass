{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups(subset=\"all\")\n",
    "print(news.keys())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news  target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...      10\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...       3\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...      17\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...       3\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...       4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({\"news\": news.data, \"target\": news.target})\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Mamatha Devineni Ratnam &lt;mr47+@andrew.cm...</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: mblawson@midway.ecn.uoknor.edu (Matthew ...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Alexander Samuel McDiarmid &lt;am2o+@andrew...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news                    target\n",
       "0  From: Mamatha Devineni Ratnam <mr47+@andrew.cm...          rec.sport.hockey\n",
       "1  From: mblawson@midway.ecn.uoknor.edu (Matthew ...  comp.sys.ibm.pc.hardware\n",
       "2  From: hilmi-er@dsv.su.se (Hilmi Eren)\\nSubject...     talk.politics.mideast\n",
       "3  From: guyd@austin.ibm.com (Guy Dawson)\\nSubjec...  comp.sys.ibm.pc.hardware\n",
       "4  From: Alexander Samuel McDiarmid <am2o+@andrew...     comp.sys.mac.hardware"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dict = {idx: name for idx, name in enumerate(news.target_names)}\n",
    "news_df[\"target\"] = news_df[\"target\"].replace(target_dict)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansign(df):\n",
    "    delete_email = re.sub(r\"\\b[\\w\\+]+@[\\w]+.[\\w]+.[\\w]+.[\\w]+\\b\", \" \", df)\n",
    "    delete_number = re.sub(r\"\\b|\\d+|\\b\", \" \", delete_email)\n",
    "    delete_non_word = re.sub(r\"\\b[\\W]+\\b\", \" \", delete_number)\n",
    "    cleaning_result = \" \".join(delete_non_word.split())\n",
    "    return cleaning_result\n",
    "\n",
    "\n",
    "news_df.loc[:, \"news\"] = news_df.loc[:, \"news\"].apply(data_cleansign)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson)\n",
      "Subject: Which high-performance VLB video card?\n",
      "Summary: Seek recommendations for VLB video card\n",
      "Nntp-Posting-Host: midway.ecn.uoknor.edu\n",
      "Organization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\n",
      "Keywords: orchid, stealth, vlb\n",
      "Lines: 21\n",
      "\n",
      "  My brother is in the market for a high-performance video card that supports\n",
      "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
      "\n",
      "  - Diamond Stealth Pro Local Bus\n",
      "\n",
      "  - Orchid Farenheit 1280\n",
      "\n",
      "  - ATI Graphics Ultra Pro\n",
      "\n",
      "  - Any other high-performance VLB card\n",
      "\n",
      "\n",
      "Please post or email.  Thank you!\n",
      "\n",
      "  - Matt\n",
      "\n",
      "-- \n",
      "    |  Matthew B. Lawson <------------> (mblawson@essex.ecn.uoknor.edu)  |   \n",
      "  --+-- \"Now I, Nebuchadnezzar, praise and exalt and glorify the King  --+-- \n",
      "    |   of heaven, because everything he does is right and all his ways  |   \n",
      "    |   are just.\" - Nebuchadnezzar, king of Babylon, 562 B.C.           |   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'From Matthew B Lawson Subject Which high performance VLB video card Summary Seek recommendations for VLB video card Nntp Posting Host midway ecn uoknor edu Organization Engineering Computer Network University of Oklahoma Norman OK USA Keywords orchid stealth vlb Lines My brother is in the market for a high performance video card that supports VESA local bus with MB RAM Does anyone have suggestions ideas on Diamond Stealth Pro Local Bus Orchid Farenheit ATI Graphics Ultra Pro Any other high performance VLB card Please post or email Thank you Matt Matthew B Lawson Now I Nebuchadnezzar praise and exalt and glorify the King of heaven because everything he does is right and all his ways are just Nebuchadnezzar king of Babylon B C . |'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(news.data[1])\n",
    "news_df[\"news\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look', 'look', 'look']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import stem\n",
    "\n",
    "stmmer = stem.SnowballStemmer(\"english\")\n",
    "sentence = \"looking looks looked\"\n",
    "\n",
    "[stmmer.stem(word) for word in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('imag', 'imag', 'imagin')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmmer.stem(\"images\"), stmmer.stem(\"imaging\"), stmmer.stem(\"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "vectorizer = [CountVectorizer(), TfidfVectorizer(), StemmedCountVectorizer(), StemmedTfidfVectorizer()]\n",
    "algorithms = [MultinomialNB(), LogisticRegression(), BernoulliNB()]\n",
    "pipelines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "for case in list(itertools.product(vectorizer, algorithms)):\n",
    "    pipelines.append(make_pipeline(*case))\n",
    "len(pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CountVectorizer': {'CountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'CountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'CountVectorizer__min_df': array([0.]),\n",
       "  'CountVectorizer__lowercase': [True, False],\n",
       "  'CountVectorizer__stop_words': ['english']},\n",
       " 'TfidfVectorizer': {'TfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'TfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'TfidfVectorizer__min_df': array([0.]),\n",
       "  'TfidfVectorizer__lowercase': [True, False],\n",
       "  'TfidfVectorizer__stop_words': ['english']},\n",
       " 'StemmedCountVectorizer': {'StemmedCountVectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'StemmedCountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'StemmedCountVectorizer__min_df': array([0.]),\n",
       "  'StemmedCountVectorizer__lowercase': [True, False],\n",
       "  'StemmedCountVectorizer__stop_words': ['english']},\n",
       " 'StemmedTfidfVectorizer': {'StemmedTfidfVectorizer__ngram_range': [(1, 1),\n",
       "   (1, 3)],\n",
       "  'StemmedTfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'StemmedTfidfVectorizer__min_df': array([0.]),\n",
       "  'StemmedTfidfVectorizer__lowercase': [True, False],\n",
       "  'StemmedTfidfVectorizer__stop_words': ['english']}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_params = [(1, 1), (1, 3)]\n",
    "stopwords_params = [\"english\"]\n",
    "lowercase_params = [True, False]\n",
    "max_df_params = np.linspace(0.4, 0.6, num=6)\n",
    "min_df_params = np.linspace(0.0, 0.0, num=1)\n",
    "\n",
    "attributes = {\"ngram_range\":ngrams_params,\"max_df\":max_df_params,\"min_df\":min_df_params,\n",
    "              \"lowercase\":lowercase_params,\"stop_words\":stopwords_params}\n",
    "vectorizer_names = [\"CountVectorizer\", \"TfidfVectorizer\", \"StemmedCountVectorizer\", \"StemmedTfidfVectorizer\"]\n",
    "vectorizer_params_dict={}\n",
    "\n",
    "for vect_name in vectorizer_names:\n",
    "    vectorizer_params_dict[vect_name]= {}\n",
    "    for key, value in attributes.items():\n",
    "        param_name = vect_name + \"__\" + key\n",
    "        vectorizer_params_dict[vect_name][param_name] = value\n",
    "\n",
    "vectorizer_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MultinomialNB': {'MultinomialNB__alpha': array([1.])},\n",
       " 'LogisticRegression': [{'LogisticRegression__multi_class': ['multinomial'],\n",
       "   'LogisticRegression__solver': ['saga'],\n",
       "   'LogisticRegression__penalty': ['l1'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithms_names = [\"MultinomialNB\", \"LogisticRegression\"]\n",
    "\n",
    "algorithms_params_dict = {}\n",
    "alpha_params = np.linspace(1.0,1.0,num=1)\n",
    "for i in range(1):\n",
    "    algorithms_params_dict[algorithms_names[i]] = {algorithms_names[i]+ \"__alpha\":alpha_params}\n",
    "c_params = [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]\n",
    "\n",
    "algorithms_params_dict[algorithms_names[1]] = [{\n",
    "    \"LogisticRegression__multi_class\": [\"multinomial\"],\n",
    "    \"LogisticRegression__solver\" : [\"saga\"],\n",
    "    \"LogisticRegression__penalty\" : [\"l1\"],\n",
    "    \"LogisticRegression__C\" : c_params},{\n",
    "    \"LogisticRegression__multi_class\": [\"ovr\"],\n",
    "    \"LogisticRegression__solver\" : [\"liblinear\"],\n",
    "    \"LogisticRegression__penalty\" : [\"l2\"],\n",
    "    \"LogisticRegression__C\" : c_params\n",
    "    }]\n",
    "\n",
    "algorithms_params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'CountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'CountVectorizer__min_df': array([0.]),\n",
       "  'CountVectorizer__lowercase': [True, False],\n",
       "  'CountVectorizer__stop_words': ['english'],\n",
       "  'MultinomialNB__alpha': array([1.])},\n",
       " [{'CountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'CountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'CountVectorizer__min_df': array([0.]),\n",
       "   'CountVectorizer__lowercase': [True, False],\n",
       "   'CountVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'CountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'CountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'CountVectorizer__min_df': array([0.]),\n",
       "   'CountVectorizer__lowercase': [True, False],\n",
       "   'CountVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'TfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'TfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'TfidfVectorizer__min_df': array([0.]),\n",
       "  'TfidfVectorizer__lowercase': [True, False],\n",
       "  'TfidfVectorizer__stop_words': ['english'],\n",
       "  'MultinomialNB__alpha': array([1.])},\n",
       " [{'TfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'TfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'TfidfVectorizer__min_df': array([0.]),\n",
       "   'TfidfVectorizer__lowercase': [True, False],\n",
       "   'TfidfVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'TfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'TfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'TfidfVectorizer__min_df': array([0.]),\n",
       "   'TfidfVectorizer__lowercase': [True, False],\n",
       "   'TfidfVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'StemmedCountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'StemmedCountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'StemmedCountVectorizer__min_df': array([0.]),\n",
       "  'StemmedCountVectorizer__lowercase': [True, False],\n",
       "  'StemmedCountVectorizer__stop_words': ['english'],\n",
       "  'MultinomialNB__alpha': array([1.])},\n",
       " [{'StemmedCountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'StemmedCountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'StemmedCountVectorizer__min_df': array([0.]),\n",
       "   'StemmedCountVectorizer__lowercase': [True, False],\n",
       "   'StemmedCountVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'StemmedCountVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'StemmedCountVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'StemmedCountVectorizer__min_df': array([0.]),\n",
       "   'StemmedCountVectorizer__lowercase': [True, False],\n",
       "   'StemmedCountVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],\n",
       " {'StemmedTfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "  'StemmedTfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "  'StemmedTfidfVectorizer__min_df': array([0.]),\n",
       "  'StemmedTfidfVectorizer__lowercase': [True, False],\n",
       "  'StemmedTfidfVectorizer__stop_words': ['english'],\n",
       "  'MultinomialNB__alpha': array([1.])},\n",
       " [{'StemmedTfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'StemmedTfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'StemmedTfidfVectorizer__min_df': array([0.]),\n",
       "   'StemmedTfidfVectorizer__lowercase': [True, False],\n",
       "   'StemmedTfidfVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},\n",
       "  {'StemmedTfidfVectorizer__ngram_range': [(1, 1), (1, 3)],\n",
       "   'StemmedTfidfVectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),\n",
       "   'StemmedTfidfVectorizer__min_df': array([0.]),\n",
       "   'StemmedTfidfVectorizer__lowercase': [True, False],\n",
       "   'StemmedTfidfVectorizer__stop_words': ['english'],\n",
       "   'LogisticRegression__multi_class': ['ovr'],\n",
       "   'LogisticRegression__solver': ['liblinear'],\n",
       "   'LogisticRegression__penalty': ['l2'],\n",
       "   'LogisticRegression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines_params = []\n",
    "for case in list(itertools.product(vectorizer_names, algorithms_names)):\n",
    "    vect_params = vectorizer_params_dict[case[0]].copy()\n",
    "    algo_params = algorithms_params_dict[case[1]]\n",
    "\n",
    "    if isinstance(algo_params, dict):\n",
    "        vect_params.update(algo_params)\n",
    "        pipelines_params.append(vect_params)\n",
    "    else:\n",
    "        temp = []\n",
    "        for param in algo_params:\n",
    "            vect_params.update(param)\n",
    "            temp.append(vect_params)\n",
    "        pipelines_params.append(temp)\n",
    "\n",
    "pipelines_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
